# Data Parallelism
Ïù¥Î≤à ÏÑ∏ÏÖòÏóêÎäî Îç∞Ïù¥ÌÑ∞ Î≥ëÎ†¨Ìôî Í∏∞Î≤ïÏóê ÎåÄÌï¥ ÏïåÏïÑÎ≥¥Í≤†ÏäµÎãàÎã§.
    
## 1. `torch.nn.DataParallel`
Í∞ÄÏû• Î®ºÏ†Ä Ïö∞Î¶¨ÏóêÍ≤å ÏπúÏàôÌïú `torch.nn.DataParallel`Ïùò ÎèôÏûë Î∞©ÏãùÏóê ÎåÄÌï¥ ÏïåÏïÑÎ¥ÖÏãúÎã§. `torch.nn.DataParallel`ÏùÄ single-node & multi-GPUÏóêÏÑú ÎèôÏûëÌïòÎäî multi-thread Î™®ÎìàÏûÖÎãàÎã§.

### 1) Forward Pass
1. ÏûÖÎ†•Îêú mini-batchÎ•º **Scatter**ÌïòÏó¨ Í∞Å ÎîîÎ∞îÏù¥Ïä§Î°ú Ï†ÑÏÜ°.
2. GPU-1Ïóê Ïò¨ÎùºÏôÄ ÏûàÎäî Î™®Îç∏Ïùò ÌååÎùºÎØ∏ÌÑ∞Î•º GPU-2,3,4Î°ú **Broadcast**.
3. Í∞Å ÎîîÎ∞îÏù¥Ïä§Î°ú Î≥µÏ†úÎêú Î™®Îç∏Î°ú **Forward**ÌïòÏó¨ LogitsÏùÑ Í≥ÑÏÇ∞ Ìï®.
4. Í≥ÑÏÇ∞Îêú LogitsÏùÑ **Gather**ÌïòÏó¨ GPU-1Ïóê Î™®Ïùå.
5. LogitsÏúºÎ°úÎ∂ÄÌÑ∞ **Loss**Î•º Í≥ÑÏÇ∞Ìï®. (with loss reduction)
   
![](../images/dp_forward.png)
    
ÏΩîÎìúÎ°ú ÎÇòÌÉÄÎÇ¥Î©¥ ÏïÑÎûòÏôÄ Í∞ôÏäµÎãàÎã§.
```
import torch.nn as nn


def data_parallel(module, inputs, labels, device_ids, output_device):
    inputs = nn.parallel.scatter(inputs, device_ids)
    # ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞Î•º device_idsÎì§Ïóê ScatterÌï®

    replicas = nn.parallel.replicate(module, device_ids)
    # Î™®Îç∏ÏùÑ device_idsÎì§Ïóê Î≥µÏ†úÌï®.
   
    logit = nn.parallel.parallel_apply(replicas, inputs)
    # Í∞Å deviceÏóê Î≥µÏ†úÎêú Î™®Îç∏Ïù¥ Í∞Å deviceÏùò Îç∞Ïù¥ÌÑ∞Î•º ForwardÌï®.

    logits = nn.parallel.gather(outputs, output_device)
    # Î™®Îç∏Ïùò logitÏùÑ output_device(ÌïòÎÇòÏùò device)Î°ú Î™®Ïùå
    
    return logits
```

### 2) Backward Pass
1. Í≥ÑÏÇ∞Îêú LossÎ•º Í∞Å ÎîîÎ∞îÏù¥Ïä§Ïóê **Scatter**Ìï®.
2. Ï†ÑÎã¨Î∞õÏùÄ LossÎ•º Ïù¥Ïö©Ìï¥ÏÑú Í∞Å ÎîîÎ∞îÏù¥Ïä§ÏóêÏÑú **Backward**Î•º ÏàòÌñâÌïòÏó¨ Gradients Í≥ÑÏÇ∞.
3. Í≥ÑÏÇ∞Îêú Î™®Îì† GradientÎ•º GPU-1Î°ú **Reduce**ÌïòÏó¨ GPU-1Ïóê Ï†ÑÎ∂Ä ÎçîÌï®.
4. ÎçîÌï¥ÏßÑ GradientsÎ•º Ïù¥Ïö©ÌïòÏó¨ GPU-1Ïóê ÏûàÎäî Î™®Îç∏ÏùÑ ÏóÖÎç∞Ïù¥Ìä∏.
![](../images/dp_backward.png)
   
#### ÌòπÏãúÎÇò Î™®Î•¥ÏãúÎäî Î∂ÑÎì§ÏùÑ ÏúÑÌï¥...
- `loss.backward()`: Í∏∞Ïö∏Í∏∞Î•º ÎØ∏Î∂ÑÌï¥ÏÑú GradientÎ•º Í≥ÑÏÇ∞
- `optimizer.step()`: Í≥ÑÏÇ∞Îêú GradientÎ•º Ïù¥Ïö©Ìï¥ÏÑú ÌååÎùºÎØ∏ÌÑ∞Î•º ÏóÖÎç∞Ïù¥Ìä∏
- Computation costÎäî `backward()` > `step()`.

![](../images/backward_step.png)
```
"""
src/data_parallel.py
"""

from torch import nn
from torch.optim import Adam
from torch.utils.data import DataLoader
from transformers import BertForSequenceClassification, BertTokenizer
from datasets import load_dataset

# 1. create dataset
datasets = load_dataset("multi_nli").data["train"]
datasets = [
    {
        "premise": str(p),
        "hypothesis": str(h),
        "labels": l.as_py(),
    }
    for p, h, l in zip(datasets[2], datasets[5], datasets[9])
]
data_loader = DataLoader(datasets, batch_size=128, num_workers=4)

# 2. create model and tokenizer
model_name = "bert-base-cased"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3).cuda()

# 3. make data parallel module
# device_ids: ÏÇ¨Ïö©Ìï† ÎîîÎ∞îÏù¥Ïä§ Î¶¨Ïä§Ìä∏ / output_device: Ï∂úÎ†•Í∞íÏùÑ Î™®ÏùÑ ÎîîÎ∞îÏù¥Ïä§
model = nn.DataParallel(model, device_ids=[0, 1, 2, 3], output_device=0)

# 4. create optimizer and loss fn
optimizer = Adam(model.parameters(), lr=3e-5)
loss_fn = nn.CrossEntropyLoss(reduction="mean")

# 5. start training
for i, data in enumerate(data_loader):
    optimizer.zero_grad()
    tokens = tokenizer(
        data["premise"],
        data["hypothesis"],
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt",
    )

    logits = model(
        input_ids=tokens.input_ids.cuda(),
        attention_mask=tokens.attention_mask.cuda(),
        return_dict=False,
    )[0]

    loss = loss_fn(logits, data["labels"].cuda())
    loss.backward()
    optimizer.step()

    if i % 10 == 0:
        print(f"step:{i}, loss:{loss}")

    if i == 300:
        break
```

```
[glogin01]$ python ../src/data_parallel.py
Using custom data configuration default
Reusing dataset multi_nli
(/home/ubuntu/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 58.31it/s]
Some weights of the model checkpoint at bert-base-cased were not used when initializing
BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias',
'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight',
'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight',
'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on
another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a
BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that
you expect to be exactly identical (initializing a BertForSequenceClassification model frm
BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased
and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
step:0, loss:1.1612184047698975
step:10, loss:1.1026676893234253
step:20, loss:1.0577733516693115
step:30, loss:0.9685771465301514
step:40, loss:0.8478926420211792
step:50, loss:0.8693557977676392
step:60, loss:0.7827763557434082
step:70, loss:0.7895966172218323
step:80, loss:0.7631332278251648
step:90, loss:0.6766361594200134
step:100, loss:0.6931278109550476
step:110, loss:0.7477961778640747
step:120, loss:0.7386300563812256
step:130, loss:0.7414667010307312
step:140, loss:0.7170238494873047
step:150, loss:0.7286601066589355
step:160, loss:0.7063153982162476
step:170, loss:0.6415464282035828
step:180, loss:0.7068504095077515
step:190, loss:0.593433678150177
step:200, loss:0.6224725246429443
step:210, loss:0.7025654315948486
step:220, loss:0.5605336427688599
step:230, loss:0.578403890132904
step:240, loss:0.7344318628311157
step:250, loss:0.5977576971054077
step:260, loss:0.6717301607131958
step:270, loss:0.7103744745254517
step:280, loss:0.6679482460021973
step:290, loss:0.635512113571167
step:300, loss:0.45178914070129395
```
![](../images/dp_training.png)
Multi-GPUÏóêÏÑú ÌïôÏäµÏù¥ Ïûò ÎêòÎäîÍµ∞Ïöî. Í∑∏Îü∞Îç∞ Î¨∏Ï†úÎäî 0Î≤à GPUÏóê LogitsÏù¥ Ïè†Î¶¨Îã§Î≥¥Îãà GPU Î©îÎ™®Î¶¨ Î∂àÍ∑†Ìòï Î¨∏Ï†úÍ∞Ä ÏùºÏñ¥ÎÇ©ÎãàÎã§. Ïù¥Îü¨Ìïú Î¨∏Ï†úÎäî 0Î≤à deviceÎ°ú LogitsÏù¥ ÏïÑÎãå LossÎ•º GatherÌïòÎäî Î∞©ÏãùÏúºÎ°ú Î≥ÄÍ≤ΩÌïòÎ©¥ Ïñ¥ÎäêÏ†ïÎèÑ ÏôÑÌôîÏãúÌÇ¨ Ïàò ÏûàÏäµÎãàÎã§. LogitsÏóê ÎπÑÌï¥ LossÎäî ScalarÏù¥Í∏∞ ÎïåÎ¨∏Ïóê ÌÅ¨Í∏∞Í∞Ä Ìõ®Ïî¨ ÏûëÍ∏∞ ÎïåÎ¨∏Ïù¥Ï£†. Ïù¥ ÏûëÏóÖÏùÄ [ÎãπÍ∑ºÎßàÏºì Î∏îÎ°úÍ∑∏](https://medium.com/daangn/pytorch-multi-gpu-%ED%95%99%EC%8A%B5-%EC%A0%9C%EB%8C%80%EB%A1%9C-%ED%95%98%EA%B8%B0-27270617936b)Ïóê ÏÜåÍ∞úÎêòÏóàÎçò [PyTorch-Encoding](https://github.com/zhanghang1989/PyTorch-Encoding)Ïùò `DataParallelCriterion`Í≥º ÎèôÏùºÌï©ÎãàÎã§. Î∏îÎ°úÍ∑∏Ïóê ÍΩ§ÎÇò Î≥µÏû°ÌïòÍ≤å ÏÑ§Î™ÖÎêòÏñ¥ ÏûàÎäîÎç∞, Î≥µÏû°Ìïú Î∞©Î≤ï ÎåÄÏã† Í∞ÑÎã®ÌïòÍ≤å **forward Ìï®ÏàòÎ•º Ïò§Î≤ÑÎùºÏù¥Îìú ÌïòÎäî Í≤É** ÎßåÏúºÎ°ú ÎèôÏùº Í∏∞Îä•ÏùÑ ÏâΩÍ≤å Íµ¨ÌòÑ Ìï† Ïàò ÏûàÏäµÎãàÎã§.
    
![](../images/dp_forward_2.png)
    
ÌïµÏã¨ÏùÄ Loss ComputationÍ≥º LossÍ∞Ä reductionÏùÑ multi-thread ÏïàÏóêÏÑú ÏûëÎèô ÏãúÌÇ§Îäî Í≤ÉÏûÖÎãàÎã§. Î™®Îç∏Ïùò forward Ìï®ÏàòÎäî multi-threadÏóêÏÑú ÏûëÎèôÎêòÍ≥† ÏûàÍ∏∞ ÎïåÎ¨∏Ïóê Loss Computation Î∂ÄÎ∂ÑÏùÑ forward Ìï®Ïàò ÏïàÏóê ÎÑ£ÏúºÎ©¥ Îß§Ïö∞ ÏâΩÍ≤å Íµ¨ÌòÑÌï† Ïàò ÏûàÍ≤†Ï£†.
    
ÌïúÍ∞ÄÏßÄ ÌäπÏù¥Ìïú Ï†êÏùÄ Ïù¥Î†áÍ≤å Íµ¨ÌòÑÌïòÎ©¥ LossÏùò reductionÏù¥ 2Î≤à ÏùºÏñ¥ÎÇòÍ≤å ÎêòÎäîÎç∞Ïöî. multi-threadÏóêÏÑú batch_size//4Í∞úÏóêÏÑú 4Í∞úÎ°ú reduction ÎêòÎäî Í≥ºÏ†ï(Í∑∏Î¶ºÏóêÏÑú 4Î≤à)Ïù¥ ÌïúÎ≤à ÏùºÏñ¥ÎÇòÍ≥†, Í∞Å ÎîîÎ∞îÏù¥Ïä§ÏóêÏÑú Ï∂úÎ†•Îêú 4Í∞úÏùò LossÎ•º 1Í∞úÎ°ú Reduction ÌïòÎäî Í≥ºÏ†ï(Í∑∏Î¶ºÏóêÏÑú 5Î≤à)Ïù¥ Îã§Ïãú ÏùºÏñ¥ÎÇòÍ≤å Îê©ÎãàÎã§. Í∑∏Î†áÎã§Í≥† ÌïòÎçîÎùºÎèÑ Loss computation Î∂ÄÎ∂ÑÏùÑ Î≥ëÎ†¨Ìôî ÏãúÌÇ¨ Ïàò ÏûàÍ≥†, 0Î≤à GPUÏóê Í∞ÄÌï¥ÏßÄÎäî Î©îÎ™®Î¶¨ Î∂ÄÎã¥Ïù¥ Ï†ÅÍ∏∞ ÎïåÎ¨∏Ïóê Ìõ®Ïî¨ Ìö®Ïú®Ï†ÅÏù¥Ï£†.
```
"""
src/custom_data_parallel.py
"""

from torch import nn


# logitsÏùÑ Ï∂úÎ†•ÌïòÎäî ÏùºÎ∞òÏ†ÅÏù∏ Î™®Îç∏
class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(768, 3)

    def forward(self, inputs):
        outputs = self.linear(inputs)
        return outputs


# forward passÏóêÏÑú lossÎ•º Ï∂úÎ†•ÌïòÎäî parallel Î™®Îç∏
class ParallelLossModel(Model):
    def __init__(self):
        super().__init__()

    def forward(self, inputs, labels):
        logits = super(ParallelLossModel, self).forward(inputs)
        loss = nn.CrossEntropyLoss(reduction="mean")(logits, labels)
        return loss
```
Ïö¥Ïù¥ Ï¢ãÍ≤åÎèÑ Ïö∞Î¶¨Í∞Ä ÏûêÏ£º ÏÇ¨Ïö©ÌïòÎäî Huggingface Transformers Î™®Îç∏Îì§ÏùÄ forward passÏóêÏÑú Í≥ß Î∞îÎ°ú LossÎ•º Íµ¨ÌïòÎäî Í∏∞Îä•ÏùÑ ÎÇ¥Ïû•ÌïòÍ≥† ÏûàÏäµÎãàÎã§. Îî∞ÎùºÏÑú Ïù¥Îü¨Ìïú Í≥ºÏ†ï ÏóÜÏù¥ transformersÏùò Í∏∞Îä•ÏùÑ Ïù¥Ïö©ÌïòÏó¨ ÏßÑÌñâÌïòÍ≤†ÏäµÎãàÎã§. ÏïÑÎûòÏùò ÏΩîÎìúÎäî Transformers Î™®Îç∏Ïùò `labels`Ïù∏ÏûêÏóê ÎùºÎ≤®ÏùÑ ÏûÖÎ†•ÌïòÏó¨ LossÎ•º Î∞îÎ°ú Ï∂úÎ†•Ìï©ÎãàÎã§.
```
"""
src/efficient_data_parallel.py
"""

# 1 ~ 4ÍπåÏßÄ ÏÉùÎûµ...

# 5. start training
for i, data in enumerate(data_loader):
    optimizer.zero_grad()
    tokens = tokenizer(
        data["premise"],
        data["hypothesis"],
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt",
    )

    loss = model(
        input_ids=tokens.input_ids.cuda(),
        attention_mask=tokens.attention_mask.cuda(),
        labels=data["labels"],
    ).loss
    
    loss = loss.mean()
    # (4,) -> (1,)
    loss.backward()
    optimizer.step()

    if i % 10 == 0:
        print(f"step:{i}, loss:{loss}")

    if i == 300:
        break
```

```
[glogin01]$ python ../src/efficient_data_parallel.py
Using custom data configuration default
Reusing dataset multi_nli
(/home/ubuntu/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 199.34it/s]
Some weights of the model checkpoint at bert-base-cased were not used when initializing
BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight',
'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias',
'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on
another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a
BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that
you expect to be exactly identical (initializing a BertForSequenceClassification model from a
BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased
and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/ubuntu/kevin/kevin_env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was
asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
step:0, loss:1.186471700668335
step:10, loss:1.1163532733917236
step:20, loss:1.091385841369629
step:30, loss:1.0980195999145508
step:40, loss:1.0779412984848022
step:50, loss:1.053116798400879
step:60, loss:0.9878815412521362
step:70, loss:0.9763977527618408
step:80, loss:0.8458528518676758
step:90, loss:0.8098542094230652
step:100, loss:0.7924742698669434
step:110, loss:0.8259536027908325
step:120, loss:0.8083906173706055
step:130, loss:0.7789419889450073
step:140, loss:0.7848180532455444
step:150, loss:0.7716841697692871
step:160, loss:0.7316021919250488
step:170, loss:0.6465802192687988
step:180, loss:0.7471408843994141
step:190, loss:0.5954912900924683
step:200, loss:0.6941753029823303
step:210, loss:0.7786209583282471
step:220, loss:0.6332131028175354
step:230, loss:0.6579948663711548
step:240, loss:0.7271711230278015
step:250, loss:0.5837332010269165
step:260, loss:0.6737046241760254
step:270, loss:0.6502429246902466
step:280, loss:0.6647026538848877
step:290, loss:0.6707975268363953
step:300, loss:0.47382402420043945
```

## 2. `torch.nn.DataParallel`Ïùò Î¨∏Ï†úÏ†ê
### 1) Î©ÄÌã∞Ïì∞Î†àÎìú Î™®ÎìàÏù¥Í∏∞ ÎïåÎ¨∏Ïóê PythonÏóêÏÑú ÎπÑÌö®Ïú®Ï†ÅÏûÑ.
PythonÏùÄ GIL (Global Interpreter Lock)Ïóê ÏùòÌï¥ ÌïòÎÇòÏùò ÌîÑÎ°úÏÑ∏Ïä§ÏóêÏÑú ÎèôÏãúÏóê Ïó¨Îü¨Í∞úÏùò Ïì∞Î†àÎìúÍ∞Ä ÏûëÎèô Ìï† Ïàò ÏóÜÏäµÎãàÎã§. Îî∞ÎùºÏÑú Í∑ºÎ≥∏Ï†ÅÏúºÎ°ú Î©ÄÌã∞ Ïì∞Î†àÎìúÍ∞Ä ÏïÑÎãå **Î©ÄÌã∞ ÌîÑÎ°úÏÑ∏Ïä§ ÌîÑÎ°úÍ∑∏Îû®**ÏúºÎ°ú ÎßåÎì§Ïñ¥ÏÑú Ïó¨Îü¨Í∞úÏùò ÌîÑÎ°úÏÑ∏Ïä§Î•º ÎèôÏãúÏóê Ïã§ÌñâÌïòÍ≤å Ìï¥ÏïºÌï©ÎãàÎã§.
 
### 2) ÌïòÎÇòÏùò Î™®Îç∏ÏóêÏÑú ÏóÖÎç∞Ïù¥Ìä∏ Îêú Î™®Îç∏Ïù¥ Îã§Î•∏ deviceÎ°ú Îß§ Ïä§ÌÖùÎßàÎã§ Î≥µÏ†úÎêòÏñ¥Ïïº Ìï®.
ÌòÑÏû¨Ïùò Î∞©ÏãùÏùÄ Í∞Å ÎîîÎ∞îÏù¥Ïä§ÏóêÏÑú Í≥ÑÏÇ∞Îêú GradientÎ•º ÌïòÎÇòÏùò ÎîîÎ∞îÏù¥Ïä§Î°ú Î™®ÏïÑÏÑú(Gather) ÏóÖÎç∞Ïù¥Ìä∏ ÌïòÎäî Î∞©ÏãùÏù¥Í∏∞ ÎïåÎ¨∏Ïóê ÏóÖÎç∞Ïù¥Ìä∏Îêú Î™®Îç∏ÏùÑ Îß§Î≤à Îã§Î•∏ ÎîîÎ∞îÏù¥Ïä§Îì§Î°ú Î≥µÏ†ú(Broadcast)Ìï¥Ïïº ÌïòÎäîÎç∞, Ïù¥ Í≥ºÏ†ïÏù¥ ÍΩ§ÎÇò ÎπÑÏåâÎãàÎã§. Í∑∏Îü¨ÎÇò GradientÎ•º GatherÌïòÏßÄ ÏïäÍ≥† Í∞Å ÎîîÎ∞îÏù¥Ïä§ÏóêÏÑú ÏûêÏ≤¥Ï†ÅÏúºÎ°ú `step()`ÏùÑ ÏàòÌñâÌïúÎã§Î©¥ Î™®Îç∏ÏùÑ Îß§Î≤à Î≥µÏ†úÌïòÏßÄ ÏïäÏïÑÎèÑ ÎêòÍ≤†Ï£†. Ïñ¥ÎñªÍ≤å Ïù¥ Í≤ÉÏùÑ Íµ¨ÌòÑ Ìï† Ïàò ÏûàÏùÑÍπåÏöî?

### Solution? ‚ûù All-reduce!! ü§î
![](../images/allreduce.png)

Ï†ïÎãµÏùÄ ÏïûÏÑú Î∞∞Ïõ†Îçò All-reduce Ïó∞ÏÇ∞ÏûÖÎãàÎã§. Í∞Å ÎîîÎ∞îÏù¥Ïä§ÏóêÏÑú Í≥ÑÏÇ∞Îêú GradientsÎ•º Î™®Îëê ÎçîÌï¥ÏÑú Î™®Îì† ÎîîÎ∞îÏù¥Ïä§Ïóê Í∑†ÏùºÌïòÍ≤å ÎøåÎ†§Ï§ÄÎã§Î©¥ Í∞Å ÎîîÎ∞îÏù¥Ïä§ÏóêÏÑú ÏûêÏ≤¥Ï†ÅÏúºÎ°ú `step()`ÏùÑ ÏàòÌñâ Ìï† Ïàò ÏûàÏäµÎãàÎã§. Í∑∏Îü¨Î©¥ Îß§Î≤à Î™®Îç∏ÏùÑ ÌäπÏ†ï ÎîîÎ∞îÏù¥Ïä§Î°úÎ∂ÄÌÑ∞ Î≥µÏ†úÌï¥ Ïò¨ ÌïÑÏöîÍ∞Ä ÏóÜÍ≤†Ï£†. Îî∞ÎùºÏÑú All-reduceÎ•º ÌôúÏö©ÌïòÎäî Î∞©ÏãùÏúºÎ°ú Í∏∞Ï°¥ Î∞©ÏãùÏùÑ Í∞úÏÑ†Ìï¥Ïïº Ìï©ÎãàÎã§.

### Í∑∏Îü¨ÎÇò...  ü§î
Í∑∏Îü¨ÎÇò All-reduceÎäî Îß§Ïö∞ ÎπÑÏö©Ïù¥ ÎÜíÏùÄ Ïó∞ÏÇ∞Ïóê ÏÜçÌï©ÎãàÎã§. Ïôú Í∑∏Îü¥ÍπåÏöî? All-reduceÏùò ÏÑ∏Î∂Ä Íµ¨ÌòÑÏùÑ ÏÇ¥Ìé¥Î¥ÖÏãúÎã§.
    
### Reduce + Broadcast Íµ¨ÌòÑ Î∞©Ïãù
![](../images/allreduce_1.png)


### All to All Íµ¨ÌòÑ Î∞©Ïãù
![](../images/allreduce_2.png)
 
## 3. `torch.nn.parallel.DistributedDataParallel` (Ïù¥Ìïò DDP)
### Ring All-reduce üíç
Ring All-reduceÎäî 2017ÎÖÑÏóê Î∞îÏù¥ÎëêÏùò Ïó∞Íµ¨ÏßÑÏù¥ Í∞úÎ∞úÌïú ÏÉàÎ°úÏö¥ Ïó∞ÏÇ∞ÏûÖÎãàÎã§. Í∏∞Ï°¥Ïùò Î∞©ÏãùÎì§Ïóê ÎπÑÌï¥ ÏõîÎì±Ìûà Ìö®Ïú®Ï†ÅÏù∏ ÏÑ±Îä•ÏùÑ Î≥¥Ïó¨Ï§¨Í∏∞ ÎïåÎ¨∏Ïóê DDP Í∞úÎ∞úÏùò ÌïµÏã¨Ïù¥ ÎêòÏóàÏ£†.
- https://github.com/baidu-research/baidu-allreduce
![](../images/ring_allreduce.gif)
![](../images/ring_allreduce.png)

### DDPÎûÄ?
DDPÎäî Í∏∞Ï°¥ DataParallelÏùò Î¨∏Ï†úÎ•º Í∞úÏÑ†ÌïòÍ∏∞ ÏúÑÌï¥ Îì±Ïû•Ìïú Îç∞Ïù¥ÌÑ∞ Î≥ëÎ†¨Ï≤òÎ¶¨ Î™®ÎìàÏù¥Î©∞ single/multi-node & multi-GPUÏóêÏÑú ÎèôÏûëÌïòÎäî multi-process Î™®ÎìàÏûÖÎãàÎã§. All-reduceÎ•º ÌôúÏö©ÌïòÍ≤å ÎêòÎ©¥ÏÑú ÎßàÏä§ÌÑ∞ ÌîÑÎ°úÏÑ∏Ïä§Ïùò Í∞úÎÖêÏù¥ ÏóÜÏñ¥Ï°åÍ∏∞ ÎïåÎ¨∏Ïóê ÌïôÏäµ Í≥ºÏ†ïÏù¥ Îß§Ïö∞ Ïã¨ÌîåÌïòÍ≤å Î≥ÄÌï©ÎãàÎã§.
  
![](../images/ddp.png)

```
"""
src/ddp.py
"""

import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel
from torch.optim import Adam
from torch.utils.data import DataLoader, DistributedSampler
from transformers import BertForSequenceClassification, BertTokenizer
from datasets import load_dataset

# 1. initialize process group
dist.init_process_group("nccl")
rank = dist.get_rank()
world_size = dist.get_world_size()
torch.cuda.set_device(rank)
device = torch.cuda.current_device()

# 2. create dataset
datasets = load_dataset("multi_nli").data["train"]
datasets = [
    {
        "premise": str(p),
        "hypothesis": str(h),
        "labels": l.as_py(),
    }
    for p, h, l in zip(datasets[2], datasets[5], datasets[9])
]

# 3. create DistributedSampler
# DistributedSamplerÎäî Îç∞Ïù¥ÌÑ∞Î•º Ï™ºÍ∞úÏÑú Îã§Î•∏ ÌîÑÎ°úÏÑ∏Ïä§Î°ú Ï†ÑÏÜ°ÌïòÍ∏∞ ÏúÑÌïú Î™®ÎìàÏûÖÎãàÎã§.
sampler = DistributedSampler(
    datasets,
    num_replicas=world_size,
    rank=rank,
    shuffle=True,
)
data_loader = DataLoader(
    datasets,
    batch_size=32,
    num_workers=4,
    sampler=sampler,
    shuffle=False,
    pin_memory=True,
)


# 4. create model and tokenizer
model_name = "bert-base-cased"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3).cuda()
# 5. make distributed data parallel module
model = DistributedDataParallel(model, device_ids=[device], output_device=device)

# 5. create optimizer
optimizer = Adam(model.parameters(), lr=3e-5)

# 6. start training
for i, data in enumerate(data_loader):
    optimizer.zero_grad()
    tokens = tokenizer(
        data["premise"],
        data["hypothesis"],
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt",
    )

    loss = model(
        input_ids=tokens.input_ids.cuda(),
        attention_mask=tokens.attention_mask.cuda(),
        labels=data["labels"],
    ).loss

    loss.backward()
    optimizer.step()

    if i % 10 == 0 and rank == 0:
        print(f"step:{i}, loss:{loss}")

    if i == 300:
        break
```

Î©ÄÌã∞ÌîÑÎ°úÏÑ∏Ïä§ Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏù¥Í∏∞ ÎïåÎ¨∏Ïóê `torch.distributed.launch`Î•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.
```
[glogin01]$ python -m  torch.distributed.launch --nproc_per_node=4 ../src/ddp.py
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Using custom data configuration default
Using custom data configuration default
Reusing dataset multi_nli (/home/ubuntu/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)
Reusing dataset multi_nli (/home/ubuntu/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 181.01it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 149.46it/s]
Using custom data configuration default
Reusing dataset multi_nli (/home/ubuntu/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 229.28it/s]
Using custom data configuration default
Reusing dataset multi_nli (/home/ubuntu/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 361.84it/s]
Some weights of the model checkpoint at bert-base-cased were not used when initializing
BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight',
'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight',
'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on
another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a
BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that
you expect to be exactly identical (initializing a BertForSequenceClassification model from a
BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased
and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-cased were not used when initializing
BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias',
'cls.seq_relationship.bias',
'cls.predictions.transform.LayerNorm.bias','cls.predictions.transform.LayerNorm.weight',
'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on
another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a
BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that
you expect to be exactly identical (initializing a BertForSequenceClassification model from a
BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased
and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-cased were not used when initializing
BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias',
'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias',
'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', '
cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on
another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a
BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that
you expect to be exactly identical (initializing a BertForSequenceClassification model from a
BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased
and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-cased were not used when initializing
BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias',
'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias',
'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on
another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a
BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that
you expect to be exactly identical (initializing a BertForSequenceClassification model from a
BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased
and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
step:0, loss:1.1451387405395508
step:10, loss:1.0912988185882568
step:20, loss:1.0485237836837769
step:30, loss:0.9971571564674377
step:40, loss:0.9472718238830566
step:50, loss:1.0532103776931763
step:60, loss:0.6478840112686157
step:70, loss:0.9035330414772034
step:80, loss:0.8176743388175964
step:90, loss:1.058182716369629
step:100, loss:0.7739772796630859
step:110, loss:0.6652507185935974
step:120, loss:0.7778272032737732
step:130, loss:0.827933669090271
step:140, loss:0.6303764581680298
step:150, loss:0.5062040090560913
step:160, loss:0.8570529222488403
step:170, loss:0.6550942063331604
step:180, loss:0.6157522797584534
step:190, loss:0.7612558007240295
step:200, loss:0.7380551099777222
step:210, loss:0.7818665504455566
step:220, loss:0.9607051610946655
step:230, loss:0.8241059184074402
step:240, loss:0.5454672574996948
step:250, loss:0.4731343686580658
step:260, loss:0.8883727788925171
step:270, loss:0.4605785310268402
step:280, loss:0.7553415298461914
step:290, loss:0.8398311138153076
step:300, loss:0.45668572187423706
```

### Í∑∏Îü∞Îç∞ Ïû†Íπê, All-reduceÎ•º Ïñ∏Ï†ú ÏàòÌñâÌïòÎäîÍ≤å Ï¢ãÏùÑÍπåÏöî?
- All-reduceÎ•º `backward()`Ïó∞ÏÇ∞Í≥º Ìï®Íªò ÌïòÎäîÍ≤å Ï¢ãÏùÑÍπåÏöî?
- ÏïÑÎãàÎ©¥ `backward()`Í∞Ä Î™®Îëê ÎÅùÎÇòÍ≥† `step()` ÏãúÏûë Ï†ÑÏóê ÌïòÎäîÍ≤å Ï¢ãÏùÑÍπåÏöî?
   
![](../images/ddp_analysis_1.png)
 
### Í≤∞Í≥ºÏ†ÅÏúºÎ°ú `backward()`ÏôÄ `all-reduce`Î•º Ï§ëÏ≤©ÏãúÌÇ§Îäî Í≤ÉÏù¥ Ï¢ãÏäµÎãàÎã§.
Í≤∞Í≥ºÏ†ÅÏúºÎ°ú `backward()`ÏôÄ `all-reduce`Î•º Ï§ëÏ≤©ÏãúÌÇ§Îäî Í≤ÉÏù¥ Í∞ÄÏû• Ìö®Ïú®Ï†ÅÏù∏ Î∞©ÏãùÏûÖÎãàÎã§. `all_reduce`Îäî ÎÑ§Ìä∏ÏõåÌÅ¨ ÌÜµÏã†, `backward()`, `step()` Îì±ÏùÄ GPU Ïó∞ÏÇ∞Ïù¥Í∏∞ ÎïåÎ¨∏Ïóê ÎèôÏãúÏóê Ï≤òÎ¶¨Ìï† Ïàò ÏûàÏ£†. Ïù¥Îì§ÏùÑ Ï§ëÏ≤©ÏãúÌÇ§Î©¥ Ï¶â, computationÍ≥º communicationÏù¥ ÏµúÎåÄÌïúÏúºÎ°ú overlap ÎêòÍ∏∞ ÎïåÎ¨∏Ïóê Ïó∞ÏÇ∞ Ìö®Ïú®Ïù¥ ÌÅ¨Í≤å Ï¶ùÍ∞ÄÌï©ÎãàÎã§.

![](../images/ddp_analysis_2.png)
   
Î∂ÑÏÑù Í≤∞Í≥º `backward()`ÏôÄ `step()`ÏùÑ ÎπÑÍµêÌï¥Î≥¥Î©¥ `backward()`Í∞Ä Ìõ®Ïî¨ Î¨¥Í±∞Ïö¥ Ïó∞ÏÇ∞Ïù¥ÏòÄÏäµÎãàÎã§.
![](../images/ddp_analysis_3.png)
ÎãπÏó∞Ìûà Îçî Î¨¥Í±∞Ïö¥ Ïó∞ÏÇ∞ÏùÑ Ï§ëÏ≤©ÏãúÌÇ¨ ÏàòÎ°ù Ï†ÑÏ≤¥ ÌïôÏäµ Í≥ºÏ†ïÏùÑ ÏàòÌñâÌïòÎäî ÏãúÍ∞ÑÏù¥ ÏßßÏïÑÏßëÎãàÎã§. Î∂ÑÏÑù Í≤∞Í≥º `backward()`Í∞Ä ÎÅùÎÇ†Îïå ÍπåÏßÄ Í∏∞Îã§Î¶¨Îäî Í≤É Î≥¥Îã§ `all-reduce`Î•º Ìï®Íªò ÏàòÌñâÌïòÎäî Í≤ÉÏù¥ Ìõ®Ïî¨ Îπ®ÎûêÏäµÎãàÎã§.\n",
    
![](../images/ddp_analysis_4.png)
  
### Ïù¥ Îïå, ÏÉùÍ∏∏ Ïàò ÏûàÎäî Í∂ÅÍ∏àÏ¶ùÎì§...
- Q1: `backward()` Ïó∞ÏÇ∞ Ï§ëÏóê GradientÍ∞Ä Î™®Îëê Í≥ÑÏÇ∞ÎêòÏßÄ ÏïäÏïòÎäîÎç∞ Ïñ¥ÎñªÍ≤å `all-reduce`Î•º ÏàòÌñâÌï©ÎãàÍπå?
  - A1: `backward()`Îäî Îí§Ï™Ω Î†àÏù¥Ïñ¥Î∂ÄÌÑ∞ ÏàúÏ∞®Ï†ÅÏúºÎ°ú Ïù¥Î£®Ïñ¥ÏßÄÍ∏∞ ÎïåÎ¨∏Ïóê Í≥ÑÏÇ∞Ïù¥ ÎÅùÎÇú Î†àÏù¥Ïñ¥ Î®ºÏ†Ä Ï†ÑÏÜ°ÌïòÎ©¥ Îê©ÎãàÎã§.
    
- Q2: Í∑∏Î†áÎã§Î©¥ Ïñ∏Ï†úÎßàÎã§ `all-reduce`Î•º ÏàòÌñâÌïòÎÇòÏöî? Î†àÏù¥Ïñ¥ÎßàÎã§ Ïù¥Î£®Ïñ¥ÏßÄÎÇòÏöî?
  - A2: ÏïÑÎãôÎãàÎã§. Gradient BucketingÏùÑ ÏàòÌñâÌï©ÎãàÎã§. BucketÏù¥ Í∞ÄÎìùÏ∞®Î©¥ All-reduceÎ•º ÏàòÌñâÌï©ÎãàÎã§.
   
### Gradient Bucekting
Gradient BucektingÎäî GradientÎ•º ÏùºÏ†ïÌïú ÏÇ¨Ïù¥Ï¶àÏùò bucketÏóê Ï†ÄÏû•Ìï¥ÎëêÍ≥† Í∞ÄÎìùÏ∞®Î©¥ Îã§Î•∏ ÌîÑÎ°úÏÑ∏Ïä§Î°ú Ï†ÑÏÜ°ÌïòÎäî Î∞©ÏãùÏûÖÎãàÎã§. Í∞ÄÏû• Î®ºÏ†Ä `backward()` Ïó∞ÏÇ∞ ÎèÑÏ§ë Îí§Ï™ΩÎ∂ÄÌÑ∞ Í≥ÑÏÇ∞Îêú GradientÎì§ÏùÑ Ï∞®Î°ÄÎåÄÎ°ú bucketÏóê Ï†ÄÏû•ÌïòÎã§Í∞Ä bucketÏùò Ïö©ÎüâÏù¥ Í∞ÄÎìùÏ∞®Î©¥ All-reduceÎ•º ÏàòÌñâÌï¥ÏÑú Í∞Å deviceÏóê GradientÏùò Ìï©ÏùÑ Ï†ÑÎã¨Ìï©ÎãàÎã§. Í∑∏Î¶º ÎïåÎ¨∏Ïóê Ìó∑Í∞àÎ¶¥ ÏàòÎèÑ ÏûàÎäîÎç∞, bucketÏóê Ï†ÄÏû•ÎêòÎäî Í≤ÉÏùÄ Î™®Îç∏Ïùò ÌååÎùºÎØ∏ÌÑ∞Í∞Ä ÏïÑÎãå Ìï¥Îãπ Î†àÏù¥Ïñ¥ÏóêÏÑú Ï∂úÎ†•Îêú GradientÏûÖÎãàÎã§. Î™®Îì† bucketÏùÄ ÏùºÏ†ïÌïú ÏÇ¨Ïù¥Ï¶àÎ•º Í∞ÄÏßÄÍ≥† ÏûàÏúºÎ©∞ `bucket_size_mb` Ïù∏ÏûêÎ•º ÌÜµÌï¥ mega-byte Îã®ÏúÑÎ°ú Ïö©ÎüâÏùÑ ÏÑ§Ï†ï Ìï† Ïàò ÏûàÏäµÎãàÎã§.
   
![](../images/ddp_analysis_5.png)
