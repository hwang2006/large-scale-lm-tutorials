# Data Parallelism
ì´ë²ˆ ì„¸ì…˜ì—ì„œëŠ” ë‹¤ì–‘í•œ ë°ì´í„° ë³‘ë ¬í™” ê¸°ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

**Contents**
* [DP in PyTorch (torch.nn.DataParallel)](#data-paralell-in-pytorch)
* [DDP in PyTorch (torch.nn.parallel.distributeddataparallel)](#distributed-data-parallel-in-pytorch)
* [Distributed Training with Horovod](#distributed-training-on-a-supercomputer-with-horovod)
* [Distributed Training with Pytorch Lightning](#distributed-training-on-a-supercomputer-with-pytorch-lightning)
    
## Data Paralell in PyTorch
ê°€ì¥ ë¨¼ì € ìš°ë¦¬ì—ê²Œ ì¹œìˆ™í•œ `torch.nn.DataParallel`ì˜ ë™ì‘ ë°©ì‹ì— ëŒ€í•´ ì•Œì•„ë´…ì‹œë‹¤. `torch.nn.DataParallel`ì€ single-node & multi-GPUì—ì„œ ë™ì‘í•˜ëŠ” multi-thread ëª¨ë“ˆì…ë‹ˆë‹¤.

### 1) Forward Pass
1. ì…ë ¥ëœ mini-batchë¥¼ **Scatter**í•˜ì—¬ ê° ë””ë°”ì´ìŠ¤ë¡œ ì „ì†¡.
2. GPU-1ì— ì˜¬ë¼ì™€ ìˆëŠ” ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ GPU-2,3,4ë¡œ **Broadcast**.
3. ê° ë””ë°”ì´ìŠ¤ë¡œ ë³µì œëœ ëª¨ë¸ë¡œ **Forward**í•˜ì—¬ Logitsì„ ê³„ì‚° í•¨.
4. ê³„ì‚°ëœ Logitsì„ **Gather**í•˜ì—¬ GPU-1ì— ëª¨ìŒ.
5. Logitsìœ¼ë¡œë¶€í„° **Loss**ë¥¼ ê³„ì‚°í•¨. (with loss reduction)
   
![](../images/dp_forward.png)
    
ì½”ë“œë¡œ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.
```
import torch.nn as nn


def data_parallel(module, inputs, labels, device_ids, output_device):
    inputs = nn.parallel.scatter(inputs, device_ids)
    # ì…ë ¥ ë°ì´í„°ë¥¼ device_idsë“¤ì— Scatterí•¨

    replicas = nn.parallel.replicate(module, device_ids)
    # ëª¨ë¸ì„ device_idsë“¤ì— ë³µì œí•¨.
   
    logit = nn.parallel.parallel_apply(replicas, inputs)
    # ê° deviceì— ë³µì œëœ ëª¨ë¸ì´ ê° deviceì˜ ë°ì´í„°ë¥¼ Forwardí•¨.

    logits = nn.parallel.gather(outputs, output_device)
    # ëª¨ë¸ì˜ logitì„ output_device(í•˜ë‚˜ì˜ device)ë¡œ ëª¨ìŒ
    
    return logits
```

### 2) Backward Pass
1. ê³„ì‚°ëœ Lossë¥¼ ê° ë””ë°”ì´ìŠ¤ì— **Scatter**í•¨.
2. ì „ë‹¬ë°›ì€ Lossë¥¼ ì´ìš©í•´ì„œ ê° ë””ë°”ì´ìŠ¤ì—ì„œ **Backward**ë¥¼ ìˆ˜í–‰í•˜ì—¬ Gradients ê³„ì‚°.
3. ê³„ì‚°ëœ ëª¨ë“  Gradientë¥¼ GPU-1ë¡œ **Reduce**í•˜ì—¬ GPU-1ì— ì „ë¶€ ë”í•¨.
4. ë”í•´ì§„ Gradientsë¥¼ ì´ìš©í•˜ì—¬ GPU-1ì— ìˆëŠ” ëª¨ë¸ì„ ì—…ë°ì´íŠ¸.
![](../images/dp_backward.png)
   
#### í˜¹ì‹œë‚˜ ëª¨ë¥´ì‹œëŠ” ë¶„ë“¤ì„ ìœ„í•´...
- `loss.backward()`: ê¸°ìš¸ê¸°ë¥¼ ë¯¸ë¶„í•´ì„œ Gradientë¥¼ ê³„ì‚°
- `optimizer.step()`: ê³„ì‚°ëœ Gradientë¥¼ ì´ìš©í•´ì„œ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸
- Computation costëŠ” `backward()` > `step()`.

![](../images/backward_step.png)
```
"""
src/ch4/data_parallel.org.py
"""

from torch import nn
from torch.optim import Adam
from torch.utils.data import DataLoader
from transformers import BertForSequenceClassification, BertTokenizer
from datasets import load_dataset

# 1. create dataset
datasets = load_dataset("multi_nli").data["train"]
datasets = [
    {
        "premise": str(p),
        "hypothesis": str(h),
        "labels": l.as_py(),
    }
    for p, h, l in zip(datasets[2], datasets[5], datasets[9])
]
data_loader = DataLoader(datasets, batch_size=128, num_workers=4)

# 2. create model and tokenizer
model_name = "bert-base-cased"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3).cuda()

# 3. make data parallel module
# device_ids: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸ / output_device: ì¶œë ¥ê°’ì„ ëª¨ì„ ë””ë°”ì´ìŠ¤
model = nn.DataParallel(model, device_ids=[0, 1, 2, 3], output_device=0)

# 4. create optimizer and loss fn
optimizer = Adam(model.parameters(), lr=3e-5)
loss_fn = nn.CrossEntropyLoss(reduction="mean")

# 5. start training
for i, data in enumerate(data_loader):
    optimizer.zero_grad()
    tokens = tokenizer(
        data["premise"],
        data["hypothesis"],
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt",
    )

    logits = model(
        input_ids=tokens.input_ids.cuda(),
        attention_mask=tokens.attention_mask.cuda(),
        return_dict=False,
    )[0]

    loss = loss_fn(logits, data["labels"].cuda())
    loss.backward()
    optimizer.step()

    if i % 10 == 0:
        print(f"step:{i}, loss:{loss}")

    if i == 300:
        break
```

Data Parallel ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•´ì„œ ë‰´ë¡  ì‹œìŠ¤í…œì—ì„œ ë…¸ë“œ 1ê°œì™€ GPU 4ê°œë¥¼ í• ë‹¹ë°›ëŠ”ë‹¤. `src/ch4` ë””ë ‰í† ë¦¬ë¡œ ì´ë™í•˜ê³  ëª¨ë“ˆì„ ë¡œë“œí•œë‹¤.
```
[glogin01]$ salloc --partition=cas_v100nv_8 -J debug --nodes=1 --time=8:00:00 --gres=gpu:4 --comment pytorch
...
salloc: Nodes gpu[05] are ready for job
[gpu05] $ pwd
/scratch/qualis/git-projects/large-scale-lm-tutorials/src/ch4
[gpu05]$ module load gcc/10.2.0 cmake/3.26.2 cuda/12.1
```
```
[gup05]$ conda activate large-scale-lm
(large-scale-lm) [gpu05]$ pip install transformers datasets evaluate scikit-learn
(large-scale-lm) [gpu05]$ python data_parallel.org.py
Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.89k/8.89k [00:00<00:00, 21.9kB/s]
Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 214M/214M [00:22<00:00, 9.63MB/s]
Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.94M/4.94M [00:09<00:00, 522kB/s]
Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.10M/5.10M [00:02<00:00, 1.95MB/s]
Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 392702/392702 [00:04<00:00, 94180.67 examples/s]
Generating validation_matched split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9815/9815 [00:00<00:00, 113395.79 examples/s]
Generating validation_mismatched split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9832/9832 [00:00<00:00, 11346.93 examples/s]
/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436M/436M [00:07<00:00, 62.1MB/s]
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
step:0, loss:1.1370294094085693
step:10, loss:1.1108639240264893
step:20, loss:1.0596747398376465
step:30, loss:0.9757345914840698
step:40, loss:0.8766409158706665
step:50, loss:0.8808916211128235
step:60, loss:0.8087615966796875
step:70, loss:0.8037520051002502
step:80, loss:0.7576047778129578
step:90, loss:0.6615070104598999
step:100, loss:0.7510014176368713
step:110, loss:0.7559493780136108
step:120, loss:0.7757337689399719
step:130, loss:0.7405450940132141
step:140, loss:0.7066846489906311
step:150, loss:0.716975212097168
step:160, loss:0.6581017971038818
step:170, loss:0.649760901927948
step:180, loss:0.7065140008926392
step:190, loss:0.5565707683563232
step:200, loss:0.6459232568740845
step:210, loss:0.689096212387085
step:220, loss:0.5366467833518982
step:230, loss:0.6110734939575195
step:240, loss:0.7274714112281799
step:250, loss:0.5718880891799927
step:260, loss:0.7024426460266113
step:270, loss:0.6618905663490295
step:280, loss:0.6490603685379028
step:290, loss:0.6166819334030151
step:300, loss:0.44881874322891235
```

```
"""
src/ch4/data_parallel.py
"""
import torch
from torch import nn
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

from torch.utils.data import DataLoader

from transformers import AutoModelForSequenceClassification
from transformers import AdamW


from transformers import get_scheduler

from tqdm.auto import tqdm

import evaluate

batch_size = 32
device_count = torch.cuda.device_count() if torch.cuda.is_available() else 1

raw_datasets = load_dataset("multi_nli", split="train")

train_test_datasets = raw_datasets.train_test_split(test_size=0.1, seed=42)

checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    return tokenizer(example["premise"], example["hypothesis"], truncation=True)

tokenized_datasets = train_test_datasets.map(tokenize_function, batched=True)
tokenized_datasets = tokenized_datasets.remove_columns(['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre'])

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)


#train_datasets = tokenized_datasets["train"].shuffle(seed=12).select(range(20000))
train_datasets = tokenized_datasets["train"].shuffle(seed=12).select(range(353431))
#valid_datasets = tokenized_datasets["test"].shuffle(seed=12).select(range(2000))
valid_datasets = tokenized_datasets["test"].shuffle(seed=12).select(range(39271))



train_dataloader = DataLoader(
    #tokenized_datasets["train"], shuffle=True, batch_size=32, =data_collator
    train_datasets, shuffle=True, batch_size=batch_size*device_count, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    #tokenized_datasets["test"], batch_size=64, collate_fn=data_collator
    valid_datasets, batch_size=batch_size*device_count, collate_fn=data_collator
)

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)

optimizer = AdamW(model.parameters(), lr=5e-5)

num_epochs = 3
#num_epochs = 1
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

# data parallel
#model = nn.DataParallel(model, device_ids=[0, 1, 2, 3], output_device=0)
model = nn.DataParallel(model)

progress_bar = tqdm(range(num_training_steps))

loss_fn = nn.CrossEntropyLoss(reduction="mean")

model.train()
for epoch in range(num_epochs):
    for i, batch in enumerate(train_dataloader):
        batch = batch.to(device)
        outputs = model(**batch)
        #loss = outputs.loss
        # print(f"loss: {loss}") #loss: tensor([1.1256, 1.0732, 1.2382, 1.2967], device='cuda:0',grad_fn=<GatherBackward>)
        #loss = torch.mean(loss)
        #print(f"after torch mean loss: {loss}") # loss: 1.1834330558776855
        logits = outputs.logits
        loss = loss_fn(logits, batch["labels"])


        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

        #steps = (epoch+1)*i
        #if steps % 100 == 0:
        #   print(f"step:{steps}, loss:{loss}")



metric = evaluate.load("accuracy")

num_eval_steps = len(eval_dataloader)
progress_bar = tqdm(range(num_eval_steps))

model.eval()

for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])
    progress_bar.update(1)

evaluation = metric.compute()
print(f"\nMetric: {evaluation}")

```
```
(large-scale-lm) [gpu05]$ python data_parallel.py
/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|                                                                        | 0/2762 [00:00<?, ?it/s]/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2762/2762 [11:50<00:00,  3.89it/s]
  0%|                                                                         | 0/307 [00:00<?, ?it/s]
Metric: {'accuracy': 0.8307657049731354}â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [00:37<00:00,  9.14it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [00:37<00:00,  8.15it/s]
```
![](../images/dp_training.png)
Multi-GPUì—ì„œ í•™ìŠµì´ ì˜ ë˜ëŠ”êµ°ìš”. ê·¸ëŸ°ë° ë¬¸ì œëŠ” 0ë²ˆ GPUì— Logitsì´ ì ë¦¬ë‹¤ë³´ë‹ˆ GPU ë©”ëª¨ë¦¬ ë¶ˆê· í˜• ë¬¸ì œê°€ ì¼ì–´ë‚©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œëŠ” 0ë²ˆ deviceë¡œ Logitsì´ ì•„ë‹Œ Lossë¥¼ Gatherí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½í•˜ë©´ ì–´ëŠì •ë„ ì™„í™”ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Logitsì— ë¹„í•´ LossëŠ” Scalarì´ê¸° ë•Œë¬¸ì— í¬ê¸°ê°€ í›¨ì”¬ ì‘ê¸° ë•Œë¬¸ì´ì£ . ì´ ì‘ì—…ì€ [ë‹¹ê·¼ë§ˆì¼“ ë¸”ë¡œê·¸](https://medium.com/daangn/pytorch-multi-gpu-%ED%95%99%EC%8A%B5-%EC%A0%9C%EB%8C%80%EB%A1%9C-%ED%95%98%EA%B8%B0-27270617936b)ì— ì†Œê°œë˜ì—ˆë˜ [PyTorch-Encoding](https://github.com/zhanghang1989/PyTorch-Encoding)ì˜ `DataParallelCriterion`ê³¼ ë™ì¼í•©ë‹ˆë‹¤. ë¸”ë¡œê·¸ì— ê½¤ë‚˜ ë³µì¡í•˜ê²Œ ì„¤ëª…ë˜ì–´ ìˆëŠ”ë°, ë³µì¡í•œ ë°©ë²• ëŒ€ì‹  ê°„ë‹¨í•˜ê²Œ **forward í•¨ìˆ˜ë¥¼ ì˜¤ë²„ë¼ì´ë“œ í•˜ëŠ” ê²ƒ** ë§Œìœ¼ë¡œ ë™ì¼ ê¸°ëŠ¥ì„ ì‰½ê²Œ êµ¬í˜„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
![](../images/dp_forward_2.png)
    
í•µì‹¬ì€ Loss Computationê³¼ Lossê°€ reductionì„ multi-thread ì•ˆì—ì„œ ì‘ë™ ì‹œí‚¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ëª¨ë¸ì˜ forward í•¨ìˆ˜ëŠ” multi-threadì—ì„œ ì‘ë™ë˜ê³  ìˆê¸° ë•Œë¬¸ì— Loss Computation ë¶€ë¶„ì„ forward í•¨ìˆ˜ ì•ˆì— ë„£ìœ¼ë©´ ë§¤ìš° ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆê² ì£ .
    
í•œê°€ì§€ íŠ¹ì´í•œ ì ì€ ì´ë ‡ê²Œ êµ¬í˜„í•˜ë©´ Lossì˜ reductionì´ 2ë²ˆ ì¼ì–´ë‚˜ê²Œ ë˜ëŠ”ë°ìš”. multi-threadì—ì„œ batch_size//4ê°œì—ì„œ 4ê°œë¡œ reduction ë˜ëŠ” ê³¼ì •(ê·¸ë¦¼ì—ì„œ 4ë²ˆ)ì´ í•œë²ˆ ì¼ì–´ë‚˜ê³ , ê° ë””ë°”ì´ìŠ¤ì—ì„œ ì¶œë ¥ëœ 4ê°œì˜ Lossë¥¼ 1ê°œë¡œ Reduction í•˜ëŠ” ê³¼ì •(ê·¸ë¦¼ì—ì„œ 5ë²ˆ)ì´ ë‹¤ì‹œ ì¼ì–´ë‚˜ê²Œ ë©ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ê³  í•˜ë”ë¼ë„ Loss computation ë¶€ë¶„ì„ ë³‘ë ¬í™” ì‹œí‚¬ ìˆ˜ ìˆê³ , 0ë²ˆ GPUì— ê°€í•´ì§€ëŠ” ë©”ëª¨ë¦¬ ë¶€ë‹´ì´ ì ê¸° ë•Œë¬¸ì— í›¨ì”¬ íš¨ìœ¨ì ì´ì£ .
```
"""
src/ch4/custom_data_parallel.py
"""

from torch import nn


# logitsì„ ì¶œë ¥í•˜ëŠ” ì¼ë°˜ì ì¸ ëª¨ë¸
class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(768, 3)

    def forward(self, inputs):
        outputs = self.linear(inputs)
        return outputs


# forward passì—ì„œ lossë¥¼ ì¶œë ¥í•˜ëŠ” parallel ëª¨ë¸
class ParallelLossModel(Model):
    def __init__(self):
        super().__init__()

    def forward(self, inputs, labels):
        logits = super(ParallelLossModel, self).forward(inputs)
        loss = nn.CrossEntropyLoss(reduction="mean")(logits, labels)
        return loss
```
ìš´ì´ ì¢‹ê²Œë„ ìš°ë¦¬ê°€ ìì£¼ ì‚¬ìš©í•˜ëŠ” Huggingface Transformers ëª¨ë¸ë“¤ì€ forward passì—ì„œ ê³§ ë°”ë¡œ Lossë¥¼ êµ¬í•˜ëŠ” ê¸°ëŠ¥ì„ ë‚´ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ëŸ¬í•œ ê³¼ì • ì—†ì´ transformersì˜ ê¸°ëŠ¥ì„ ì´ìš©í•˜ì—¬ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤. ì•„ë˜ì˜ ì½”ë“œëŠ” Transformers ëª¨ë¸ì˜ `labels`ì¸ìì— ë¼ë²¨ì„ ì…ë ¥í•˜ì—¬ Lossë¥¼ ë°”ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.
```
"""
src/efficient_data_parallel.org.py
"""

# 1 ~ 4ê¹Œì§€ ìƒëµ...

# 5. start training
for i, data in enumerate(data_loader):
    optimizer.zero_grad()
    tokens = tokenizer(
        data["premise"],
        data["hypothesis"],
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt",
    )

    loss = model(
        input_ids=tokens.input_ids.cuda(),
        attention_mask=tokens.attention_mask.cuda(),
        labels=data["labels"],
    ).loss
    
    loss = loss.mean()
    # (4,) -> (1,)
    loss.backward()
    optimizer.step()

    if i % 10 == 0:
        print(f"step:{i}, loss:{loss}")

    if i == 300:
        break
```

```
(large-scale-lm) [gpu05]$ python efficient_data_parallel.org.py
/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
step:0, loss:1.1640931367874146
step:10, loss:1.1262718439102173
step:20, loss:1.105604887008667
step:30, loss:1.0968422889709473
step:40, loss:1.0840587615966797
step:50, loss:1.0564978122711182
step:60, loss:1.0217713117599487
step:70, loss:0.9702370166778564
step:80, loss:0.8745643496513367
step:90, loss:0.7839782238006592
step:100, loss:0.7462543845176697
step:110, loss:0.9073987007141113
step:120, loss:0.8226974010467529
step:130, loss:0.8410908579826355
step:140, loss:0.8973464965820312
step:150, loss:0.7872884273529053
step:160, loss:0.8032517433166504
step:170, loss:0.7142447233200073
step:180, loss:0.8116591572761536
step:190, loss:0.6684067249298096
step:200, loss:0.7727738618850708
step:210, loss:0.866787314414978
step:220, loss:0.6384953856468201
step:230, loss:0.7246848344802856
step:240, loss:0.8128511309623718
step:250, loss:0.5956273078918457
step:260, loss:0.7926182746887207
step:270, loss:0.7693862318992615
step:280, loss:0.7714772820472717
step:290, loss:0.7309739589691162
step:300, loss:0.5764233469963074
```

## `torch.nn.DataParallel`ì˜ ë¬¸ì œì 
### 1) ë©€í‹°ì“°ë ˆë“œ ëª¨ë“ˆì´ê¸° ë•Œë¬¸ì— Pythonì—ì„œ ë¹„íš¨ìœ¨ì ì„.
Pythonì€ GIL (Global Interpreter Lock)ì— ì˜í•´ í•˜ë‚˜ì˜ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë™ì‹œì— ì—¬ëŸ¬ê°œì˜ ì“°ë ˆë“œê°€ ì‘ë™ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ê·¼ë³¸ì ìœ¼ë¡œ ë©€í‹° ì“°ë ˆë“œê°€ ì•„ë‹Œ **ë©€í‹° í”„ë¡œì„¸ìŠ¤ í”„ë¡œê·¸ë¨**ìœ¼ë¡œ ë§Œë“¤ì–´ì„œ ì—¬ëŸ¬ê°œì˜ í”„ë¡œì„¸ìŠ¤ë¥¼ ë™ì‹œì— ì‹¤í–‰í•˜ê²Œ í•´ì•¼í•©ë‹ˆë‹¤.
 
### 2) í•˜ë‚˜ì˜ ëª¨ë¸ì—ì„œ ì—…ë°ì´íŠ¸ ëœ ëª¨ë¸ì´ ë‹¤ë¥¸ deviceë¡œ ë§¤ ìŠ¤í…ë§ˆë‹¤ ë³µì œë˜ì–´ì•¼ í•¨.
í˜„ì¬ì˜ ë°©ì‹ì€ ê° ë””ë°”ì´ìŠ¤ì—ì„œ ê³„ì‚°ëœ Gradientë¥¼ í•˜ë‚˜ì˜ ë””ë°”ì´ìŠ¤ë¡œ ëª¨ì•„ì„œ(Gather) ì—…ë°ì´íŠ¸ í•˜ëŠ” ë°©ì‹ì´ê¸° ë•Œë¬¸ì— ì—…ë°ì´íŠ¸ëœ ëª¨ë¸ì„ ë§¤ë²ˆ ë‹¤ë¥¸ ë””ë°”ì´ìŠ¤ë“¤ë¡œ ë³µì œ(Broadcast)í•´ì•¼ í•˜ëŠ”ë°, ì´ ê³¼ì •ì´ ê½¤ë‚˜ ë¹„ìŒ‰ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ Gradientë¥¼ Gatherí•˜ì§€ ì•Šê³  ê° ë””ë°”ì´ìŠ¤ì—ì„œ ìì²´ì ìœ¼ë¡œ `step()`ì„ ìˆ˜í–‰í•œë‹¤ë©´ ëª¨ë¸ì„ ë§¤ë²ˆ ë³µì œí•˜ì§€ ì•Šì•„ë„ ë˜ê² ì£ . ì–´ë–»ê²Œ ì´ ê²ƒì„ êµ¬í˜„ í•  ìˆ˜ ìˆì„ê¹Œìš”?

### Solution? â All-reduce!! ğŸ¤”
![](../images/allreduce.png)

ì •ë‹µì€ ì•ì„œ ë°°ì› ë˜ All-reduce ì—°ì‚°ì…ë‹ˆë‹¤. ê° ë””ë°”ì´ìŠ¤ì—ì„œ ê³„ì‚°ëœ Gradientsë¥¼ ëª¨ë‘ ë”í•´ì„œ ëª¨ë“  ë””ë°”ì´ìŠ¤ì— ê· ì¼í•˜ê²Œ ë¿Œë ¤ì¤€ë‹¤ë©´ ê° ë””ë°”ì´ìŠ¤ì—ì„œ ìì²´ì ìœ¼ë¡œ `step()`ì„ ìˆ˜í–‰ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë©´ ë§¤ë²ˆ ëª¨ë¸ì„ íŠ¹ì • ë””ë°”ì´ìŠ¤ë¡œë¶€í„° ë³µì œí•´ ì˜¬ í•„ìš”ê°€ ì—†ê² ì£ . ë”°ë¼ì„œ All-reduceë¥¼ í™œìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê¸°ì¡´ ë°©ì‹ì„ ê°œì„ í•´ì•¼ í•©ë‹ˆë‹¤.

### ê·¸ëŸ¬ë‚˜...  ğŸ¤”
ê·¸ëŸ¬ë‚˜ All-reduceëŠ” ë§¤ìš° ë¹„ìš©ì´ ë†’ì€ ì—°ì‚°ì— ì†í•©ë‹ˆë‹¤. ì™œ ê·¸ëŸ´ê¹Œìš”? All-reduceì˜ ì„¸ë¶€ êµ¬í˜„ì„ ì‚´í´ë´…ì‹œë‹¤.
    
### Reduce + Broadcast êµ¬í˜„ ë°©ì‹
![](../images/allreduce_1.png)


### All to All êµ¬í˜„ ë°©ì‹
![](../images/allreduce_2.png)
 
## Distributed Data Parallel in PyTorch
### Ring All-reduce ğŸ’
Ring All-reduceëŠ” 2017ë…„ì— ë°”ì´ë‘ì˜ ì—°êµ¬ì§„ì´ ê°œë°œí•œ ìƒˆë¡œìš´ ì—°ì‚°ì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë°©ì‹ë“¤ì— ë¹„í•´ ì›”ë“±íˆ íš¨ìœ¨ì ì¸ ì„±ëŠ¥ì„ ë³´ì—¬ì¤¬ê¸° ë•Œë¬¸ì— DDP ê°œë°œì˜ í•µì‹¬ì´ ë˜ì—ˆì£ .
- https://github.com/baidu-research/baidu-allreduce
![](../images/ring_allreduce.gif)
![](../images/ring_allreduce.png)

### DDPë€?
DDPëŠ” ê¸°ì¡´ DataParallelì˜ ë¬¸ì œë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ ë“±ì¥í•œ ë°ì´í„° ë³‘ë ¬ì²˜ë¦¬ ëª¨ë“ˆì´ë©° single/multi-node & multi-GPUì—ì„œ ë™ì‘í•˜ëŠ” multi-process ëª¨ë“ˆì…ë‹ˆë‹¤. All-reduceë¥¼ í™œìš©í•˜ê²Œ ë˜ë©´ì„œ ë§ˆìŠ¤í„° í”„ë¡œì„¸ìŠ¤ì˜ ê°œë…ì´ ì—†ì–´ì¡Œê¸° ë•Œë¬¸ì— í•™ìŠµ ê³¼ì •ì´ ë§¤ìš° ì‹¬í”Œí•˜ê²Œ ë³€í•©ë‹ˆë‹¤.
  
![](../images/ddp.png)

```
"""
src/ch4/ddp.org.py
"""

import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel
from torch.optim import Adam
from torch.utils.data import DataLoader, DistributedSampler
from transformers import BertForSequenceClassification, BertTokenizer
from datasets import load_dataset

# 1. initialize process group
dist.init_process_group("nccl")
rank = dist.get_rank()
world_size = dist.get_world_size()
torch.cuda.set_device(rank)
device = torch.cuda.current_device()

# 2. create dataset
datasets = load_dataset("multi_nli").data["train"]
datasets = [
    {
        "premise": str(p),
        "hypothesis": str(h),
        "labels": l.as_py(),
    }
    for p, h, l in zip(datasets[2], datasets[5], datasets[9])
]

# 3. create DistributedSampler
# DistributedSamplerëŠ” ë°ì´í„°ë¥¼ ìª¼ê°œì„œ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ë¡œ ì „ì†¡í•˜ê¸° ìœ„í•œ ëª¨ë“ˆì…ë‹ˆë‹¤.
sampler = DistributedSampler(
    datasets,
    num_replicas=world_size,
    rank=rank,
    shuffle=True,
)
data_loader = DataLoader(
    datasets,
    batch_size=32,
    num_workers=4,
    sampler=sampler,
    shuffle=False,
    pin_memory=True,
)


# 4. create model and tokenizer
model_name = "bert-base-cased"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3).cuda()
# 5. make distributed data parallel module
model = DistributedDataParallel(model, device_ids=[device], output_device=device)

# 5. create optimizer
optimizer = Adam(model.parameters(), lr=3e-5)

# 6. start training
for i, data in enumerate(data_loader):
    optimizer.zero_grad()
    tokens = tokenizer(
        data["premise"],
        data["hypothesis"],
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt",
    )

    loss = model(
        input_ids=tokens.input_ids.cuda(),
        attention_mask=tokens.attention_mask.cuda(),
        labels=data["labels"],
    ).loss

    loss.backward()
    optimizer.step()

    if i % 10 == 0 and rank == 0:
        print(f"step:{i}, loss:{loss}")

    if i == 300:
        break
```

ë©€í‹°í”„ë¡œì„¸ìŠ¤ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ê¸° ë•Œë¬¸ì— `torch.distributed.launch`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
```
(large-scale-lm) [gpu05]$ python -m  torch.distributed.launch --nproc_per_node=4 ddp.org.py
(large-scale-lm) [gpu05]$ torchrun --nproc_per_node=4 ddp.org.py
(large-scale-lm) [gpu05]$ srun torchrun --nproc_per_node=4 ddp.org.py
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being
overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Using custom data configuration default
Using custom data configuration default
Reusing dataset multi_nli (/home/ubuntu/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)
Reusing dataset multi_nli (/home/ubuntu/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 181.01it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 149.46it/s]
Using custom data configuration default
Reusing dataset multi_nli (/home/ubuntu/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 229.28it/s]
Using custom data configuration default
Reusing dataset multi_nli (/home/ubuntu/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 361.84it/s]
Some weights of the model checkpoint at bert-base-cased were not used when initializing
BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight',
'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight',
'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on
another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a
BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that
you expect to be exactly identical (initializing a BertForSequenceClassification model from a
BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased
and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-cased were not used when initializing
BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias',
'cls.seq_relationship.bias',
'cls.predictions.transform.LayerNorm.bias','cls.predictions.transform.LayerNorm.weight',
'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on
another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a
BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that
you expect to be exactly identical (initializing a BertForSequenceClassification model from a
BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased
and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-cased were not used when initializing
BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias',
'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias',
'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', '
cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on
another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a
BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that
you expect to be exactly identical (initializing a BertForSequenceClassification model from a
BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased
and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-cased were not used when initializing
BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias',
'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias',
'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on
another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a
BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that
you expect to be exactly identical (initializing a BertForSequenceClassification model from a
BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased
and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
step:0, loss:1.1451387405395508
step:10, loss:1.0912988185882568
step:20, loss:1.0485237836837769
step:30, loss:0.9971571564674377
step:40, loss:0.9472718238830566
step:50, loss:1.0532103776931763
step:60, loss:0.6478840112686157
step:70, loss:0.9035330414772034
step:80, loss:0.8176743388175964
step:90, loss:1.058182716369629
step:100, loss:0.7739772796630859
step:110, loss:0.6652507185935974
step:120, loss:0.7778272032737732
step:130, loss:0.827933669090271
step:140, loss:0.6303764581680298
step:150, loss:0.5062040090560913
step:160, loss:0.8570529222488403
step:170, loss:0.6550942063331604
step:180, loss:0.6157522797584534
step:190, loss:0.7612558007240295
step:200, loss:0.7380551099777222
step:210, loss:0.7818665504455566
step:220, loss:0.9607051610946655
step:230, loss:0.8241059184074402
step:240, loss:0.5454672574996948
step:250, loss:0.4731343686580658
step:260, loss:0.8883727788925171
step:270, loss:0.4605785310268402
step:280, loss:0.7553415298461914
step:290, loss:0.8398311138153076
step:300, loss:0.45668572187423706
```

```
"""
src/ch4/ddp.py
"""

import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel

from torch import nn
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

from torch.utils.data import DataLoader, DistributedSampler

from transformers import AutoModelForSequenceClassification
from transformers import AdamW


from transformers import get_scheduler

from tqdm.auto import tqdm

import evaluate


# initialize process group
dist.init_process_group("nccl")
rank = dist.get_rank()
world_size = dist.get_world_size()
torch.cuda.set_device(rank)
device = torch.cuda.current_device()


batch_size = 32
device_count = torch.cuda.device_count() if torch.cuda.is_available() else 1

raw_datasets = load_dataset("multi_nli", split="train")

train_test_datasets = raw_datasets.train_test_split(test_size=0.1, seed=42)

checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    return tokenizer(example["premise"], example["hypothesis"], truncation=True)

tokenized_datasets = train_test_datasets.map(tokenize_function, batched=True)
tokenized_datasets = tokenized_datasets.remove_columns(['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre'])

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)


#train_datasets = tokenized_datasets["train"].shuffle(seed=12).select(range(20000))
train_datasets = tokenized_datasets["train"].shuffle(seed=12).select(range(353431))
#valid_datasets = tokenized_datasets["test"].shuffle(seed=12).select(range(2000))
valid_datasets = tokenized_datasets["test"].shuffle(seed=12).select(range(39271))

train_dataloader = DataLoader(
    #tokenized_datasets["train"], shuffle=True, batch_size=32, =data_collator
    train_datasets, shuffle=True, batch_size=batch_size*device_count, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    #tokenized_datasets["test"], batch_size=64, collate_fn=data_collator
    valid_datasets, batch_size=batch_size*device_count, collate_fn=data_collator
)

# create DistributedSampler
train_sampler = DistributedSampler(
    #datasets,
    train_datasets,
    num_replicas=world_size,
    rank=rank,
    shuffle=True,
)
train_dataloader = DataLoader(
    #datasets,
        train_datasets,
    batch_size=32,
    num_workers=8,
    sampler=train_sampler,
    shuffle=False,
    pin_memory=True,
    collate_fn=data_collator,
)

eval_sampler = DistributedSampler(
    #datasets,
    valid_datasets,
    num_replicas=world_size,
    rank=rank,
    shuffle=False,
)
eval_dataloader = DataLoader(
    #datasets,
    valid_datasets,
    batch_size=32,
    num_workers=8,
    sampler=eval_sampler,
    shuffle=False,
    pin_memory=True,
    collate_fn=data_collator,
)


model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)

optimizer = AdamW(model.parameters(), lr=5e-5)

#num_epochs = 5
num_epochs = 1
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

# distributed data parallel
model = model.to(device)
#model = DistributedDataParallel(model, device_ids=[device])
model = DistributedDataParallel(model)

progress_bar = tqdm(range(num_training_steps))

loss_fn = nn.CrossEntropyLoss(reduction="mean")

model.train()
for epoch in range(num_epochs):
    for i, batch in enumerate(train_dataloader):
        batch = batch.to(device)
        outputs = model(**batch)
        loss = outputs.loss
        # print(f"loss: {loss}") #loss: tensor([1.1256, 1.0732, 1.2382, 1.2967], device='cuda:0',grad_fn=<GatherBackward>)
        #loss = torch.mean(loss)
        #print(f"after torch mean loss: {loss}") # loss: 1.1834330558776855
        #logits = outputs.logits
        #loss = loss_fn(logits, batch["labels"])


        loss.backward()

        optimizer.step()
        #lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

        #steps = (epoch+1)*i
        #if steps % 100 == 0:
        #   print(f"step:{steps}, loss:{loss}")



metric = evaluate.load("accuracy")

num_eval_steps = len(eval_dataloader)
progress_bar = tqdm(range(num_eval_steps))

model.eval()

for batch in eval_dataloader:
    #batch = {k: v.to(device) for k, v in batch.items()}
    batch = batch.to(device)
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])
    progress_bar.update(1)

evaluation = metric.compute()
#print(type(evaluation)) # class dict
#print(evaluation["accuracy"])
print(f"\nMetric: {evaluation}")

evaluation_tensor = torch.tensor(evaluation["accuracy"]).to(device)

dist.reduce(evaluation_tensor, op=torch.distributed.ReduceOp.AVG, dst=0)

if rank == 0:
    print(f"\nAverge Accuracy: {evaluation_tensor.item():.3f}")
```

```
(large-scale-lm) [gpu05]$ torchrun --nproc_per_node=4 ddp.py
W0908 21:52:13.856000 47481656617152 torch/distributed/run.py:757]
W0908 21:52:13.856000 47481656617152 torch/distributed/run.py:757] *****************************************
W0908 21:52:13.856000 47481656617152 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0908 21:52:13.856000 47481656617152 torch/distributed/run.py:757] *****************************************
/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Map:   0%|                                                           | 0/39271 [00:00<?, ? examples/s]/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Map:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 8000/39271 [00:01<00:04, 6525.43 examples/s]/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39271/39271 [00:05<00:00, 6732.23 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39271/39271 [00:05<00:00, 6585.32 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39271/39271 [00:05<00:00, 6578.37 examples/s]
Map:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 32000/39271 [00:04<00:00, 7292.94 examples/s]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Map:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 33000/39271 [00:04<00:00, 7603.37 examples/s]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Map:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 34000/39271 [00:05<00:00, 7669.44 examples/s]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39271/39271 [00:05<00:00, 6713.38 examples/s]
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|                                                                        | 0/2762 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2762/2762 [09:48<00:00,  4.69it/s]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2762/2762 [09:48<00:00,  4.69it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2762/2762 [09:48<00:00,  4.69it/s]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2762/2762 [09:48<00:00,  4.69it/s]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Metric: {'accuracy': 0.8206355673253208}â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 273/307 [00:15<00:01, 19.72it/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 282/307 [00:15<00:01, 18.73it/s]
Metric: {'accuracy': 0.8306172336524751}â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 291/307 [00:16<00:00, 21.02it/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 300/307 [00:16<00:00, 20.47it/s]
Metric: {'accuracy': 0.8219596659197392}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [00:16<00:00, 18.25it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [00:16<00:00, 18.25it/s]

Metric: {'accuracy': 0.8224689346099002}â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 306/307 [00:17<00:00, 22.01it/s]

Averge Accuracy: 0.824
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [00:17<00:00, 17.48it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [00:17<00:00, 17.50it/s]
```

### ê·¸ëŸ°ë° ì ê¹, All-reduceë¥¼ ì–¸ì œ ìˆ˜í–‰í•˜ëŠ”ê²Œ ì¢‹ì„ê¹Œìš”?
- All-reduceë¥¼ `backward()`ì—°ì‚°ê³¼ í•¨ê»˜ í•˜ëŠ”ê²Œ ì¢‹ì„ê¹Œìš”?
- ì•„ë‹ˆë©´ `backward()`ê°€ ëª¨ë‘ ëë‚˜ê³  `step()` ì‹œì‘ ì „ì— í•˜ëŠ”ê²Œ ì¢‹ì„ê¹Œìš”?
   
![](../images/ddp_analysis_1.png)
 
### ê²°ê³¼ì ìœ¼ë¡œ `backward()`ì™€ `all-reduce`ë¥¼ ì¤‘ì²©ì‹œí‚¤ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
ê²°ê³¼ì ìœ¼ë¡œ `backward()`ì™€ `all-reduce`ë¥¼ ì¤‘ì²©ì‹œí‚¤ëŠ” ê²ƒì´ ê°€ì¥ íš¨ìœ¨ì ì¸ ë°©ì‹ì…ë‹ˆë‹¤. `all_reduce`ëŠ” ë„¤íŠ¸ì›Œí¬ í†µì‹ , `backward()`, `step()` ë“±ì€ GPU ì—°ì‚°ì´ê¸° ë•Œë¬¸ì— ë™ì‹œì— ì²˜ë¦¬í•  ìˆ˜ ìˆì£ . ì´ë“¤ì„ ì¤‘ì²©ì‹œí‚¤ë©´ ì¦‰, computationê³¼ communicationì´ ìµœëŒ€í•œìœ¼ë¡œ overlap ë˜ê¸° ë•Œë¬¸ì— ì—°ì‚° íš¨ìœ¨ì´ í¬ê²Œ ì¦ê°€í•©ë‹ˆë‹¤.

![](../images/ddp_analysis_2.png)
   
ë¶„ì„ ê²°ê³¼ `backward()`ì™€ `step()`ì„ ë¹„êµí•´ë³´ë©´ `backward()`ê°€ í›¨ì”¬ ë¬´ê±°ìš´ ì—°ì‚°ì´ì˜€ìŠµë‹ˆë‹¤.
![](../images/ddp_analysis_3.png)
ë‹¹ì—°íˆ ë” ë¬´ê±°ìš´ ì—°ì‚°ì„ ì¤‘ì²©ì‹œí‚¬ ìˆ˜ë¡ ì „ì²´ í•™ìŠµ ê³¼ì •ì„ ìˆ˜í–‰í•˜ëŠ” ì‹œê°„ì´ ì§§ì•„ì§‘ë‹ˆë‹¤. ë¶„ì„ ê²°ê³¼ `backward()`ê°€ ëë‚ ë•Œ ê¹Œì§€ ê¸°ë‹¤ë¦¬ëŠ” ê²ƒ ë³´ë‹¤ `all-reduce`ë¥¼ í•¨ê»˜ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ í›¨ì”¬ ë¹¨ëìŠµë‹ˆë‹¤.\n",
    
![](../images/ddp_analysis_4.png)
  
### ì´ ë•Œ, ìƒê¸¸ ìˆ˜ ìˆëŠ” ê¶ê¸ˆì¦ë“¤...
- Q1: `backward()` ì—°ì‚° ì¤‘ì— Gradientê°€ ëª¨ë‘ ê³„ì‚°ë˜ì§€ ì•Šì•˜ëŠ”ë° ì–´ë–»ê²Œ `all-reduce`ë¥¼ ìˆ˜í–‰í•©ë‹ˆê¹Œ?
  - A1: `backward()`ëŠ” ë’¤ìª½ ë ˆì´ì–´ë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ ì´ë£¨ì–´ì§€ê¸° ë•Œë¬¸ì— ê³„ì‚°ì´ ëë‚œ ë ˆì´ì–´ ë¨¼ì € ì „ì†¡í•˜ë©´ ë©ë‹ˆë‹¤.
    
- Q2: ê·¸ë ‡ë‹¤ë©´ ì–¸ì œë§ˆë‹¤ `all-reduce`ë¥¼ ìˆ˜í–‰í•˜ë‚˜ìš”? ë ˆì´ì–´ë§ˆë‹¤ ì´ë£¨ì–´ì§€ë‚˜ìš”?
  - A2: ì•„ë‹™ë‹ˆë‹¤. Gradient Bucketingì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. Bucketì´ ê°€ë“ì°¨ë©´ All-reduceë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
   
### Gradient Bucekting
Gradient BucektingëŠ” Gradientë¥¼ ì¼ì •í•œ ì‚¬ì´ì¦ˆì˜ bucketì— ì €ì¥í•´ë‘ê³  ê°€ë“ì°¨ë©´ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ë¡œ ì „ì†¡í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ê°€ì¥ ë¨¼ì € `backward()` ì—°ì‚° ë„ì¤‘ ë’¤ìª½ë¶€í„° ê³„ì‚°ëœ Gradientë“¤ì„ ì°¨ë¡€ëŒ€ë¡œ bucketì— ì €ì¥í•˜ë‹¤ê°€ bucketì˜ ìš©ëŸ‰ì´ ê°€ë“ì°¨ë©´ All-reduceë¥¼ ìˆ˜í–‰í•´ì„œ ê° deviceì— Gradientì˜ í•©ì„ ì „ë‹¬í•©ë‹ˆë‹¤. ê·¸ë¦¼ ë•Œë¬¸ì— í—·ê°ˆë¦´ ìˆ˜ë„ ìˆëŠ”ë°, bucketì— ì €ì¥ë˜ëŠ” ê²ƒì€ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ê°€ ì•„ë‹Œ í•´ë‹¹ ë ˆì´ì–´ì—ì„œ ì¶œë ¥ëœ Gradientì…ë‹ˆë‹¤. ëª¨ë“  bucketì€ ì¼ì •í•œ ì‚¬ì´ì¦ˆë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©° `bucket_size_mb` ì¸ìë¥¼ í†µí•´ mega-byte ë‹¨ìœ„ë¡œ ìš©ëŸ‰ì„ ì„¤ì • í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
   
![](../images/ddp_analysis_5.png)


## Distributed Training on a Supercomputer with Horovod
ìŠˆí¼ì»´í“¨í„°ì—ì„œ Horovodë¥¼ ì‚¬ìš©í•œ Distributed Data Parallel ì‹¤ìŠµí•˜ê¸°ëŠ” ì•„ë˜ ê¹ƒí—ˆë¸Œ ì‚¬ì´íŠ¸ì— ìì„¸íˆ ê¸°ìˆ ë˜ì—ˆìŠµë‹ˆë‹¤. 
- https://github.com/hwang2006/KISTI-DL-tutorial-using-horovod


## Distributed Training on a Supercomputer with Pytorch Lightning
ìŠˆí¼ì»´í“¨í„°ì—ì„œ Pytorch Lightningì„ ì‚¬ìš©í•œ Distributed Data Parallel ì‹¤ìŠµí•˜ê¸°ëŠ” ì•„ë˜ ê¹ƒí—ˆë¸Œ ì‚¬ì´íŠ¸ì— ìì„¸íˆ ê¸°ìˆ ë˜ì—ˆìŠµë‹ˆë‹¤. 
- https://github.com/hwang2006/distributed-training-on-supercomputer-with-pytorch-lightning


