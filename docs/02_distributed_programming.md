# Distributed Programming
Large-scale ëª¨ë¸ì€ í¬ê¸°ê°€ í¬ê¸° ë•Œë¬¸ì— ì—¬ëŸ¬ëŒ€ì˜ GPUì— ìª¼ê°œì„œ ëª¨ë¸ì„ ì˜¬ë ¤ì•¼ í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ìª¼ê°œì§„ ê° ëª¨ë¸ì˜ ì¡°ê°ë“¤ë¼ë¦¬ ë„¤íŠ¸ì›Œí¬ë¡œ í†µì‹ ì„ í•˜ë©´ì„œ ê°’ì„ ì£¼ê³  ë°›ì•„ì•¼ í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ì»¤ë‹¤ë€ ë¦¬ì†ŒìŠ¤ë¥¼ ì—¬ëŸ¬ëŒ€ì˜ ì»´í“¨í„° í˜¹ì€ ì—¬ëŸ¬ëŒ€ì˜ ì¥ë¹„ì— ë¶„ì‚°ì‹œì¼œì„œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì„ 'ë¶„ì‚°ì²˜ë¦¬'ë¼ê³  í•©ë‹ˆë‹¤. ì´ë²ˆ ì„¸ì…˜ì—ì„œëŠ” PyTorchë¥¼ ì´ìš©í•œ ë¶„ì‚° í”„ë¡œê·¸ë˜ë°ì˜ ê¸°ì´ˆì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.
  
## 1. Multi-processing with PyTorch
   
ë¶„ì‚°í”„ë¡œê·¸ë˜ë° íŠœí† ë¦¬ì–¼ì— ì•ì„œ PyTorchë¡œ êµ¬í˜„ëœ Multi-processing ì• í”Œë¦¬ì¼€ì´ì…˜ì— ëŒ€í•œ íŠœí† ë¦¬ì–¼ì„ ì§„í–‰í•©ë‹ˆë‹¤. ì“°ë ˆë“œ ë° í”„ë¡œì„¸ìŠ¤ì˜ ê°œë… ë“±ì€ Computer Scienece ì „ê³µìë¼ë©´ ìš´ì˜ì²´ì œ ì‹œê°„ì— ë°°ìš°ëŠ” ê²ƒë“¤ì´ë‹ˆ ìƒëµí•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ë§Œì•½ ì´ëŸ¬í•œ ê°œë…ì— ëŒ€í•´ ì˜ ëª¨ë¥´ì‹ ë‹¤ë©´, êµ¬ê¸€ì— ê²€ìƒ‰í•˜ì‹œê±°ë‚˜ https://www.backblaze.com/blog/whats-the-diff-programs-processes-and-threads/ ì™€ ê°™ì€ ê¸€ì„ ë¨¼ì € ì½ì–´ë³´ëŠ” ê²ƒì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.

### Multi-process í†µì‹ ì— ì“°ì´ëŠ” ê¸°ë³¸ ìš©ì–´
- Node: ì¼ë°˜ì ìœ¼ë¡œ ì»´í“¨í„°ë¼ê³  ìƒê°í•˜ì‹œë©´ ë©ë‹ˆë‹¤. ë…¸ë“œ 3ëŒ€ë¼ê³  í•˜ë©´ ì»´í“¨í„° 3ëŒ€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
- Global Rank: ì›ë˜ëŠ” í”„ë¡œì„¸ìŠ¤ì˜ ìš°ì„ ìˆœìœ„ë¥¼ ì˜ë¯¸í•˜ì§€ë§Œ **MLì—ì„œëŠ” GPUì˜ ID**ë¼ê³  ë³´ì‹œë©´ ë©ë‹ˆë‹¤.
- Local Rank: ì›ë˜ëŠ” í•œ ë…¸ë“œë‚´ì—ì„œì˜ í”„ë¡œì„¸ìŠ¤ ìš°ì„ ìˆœìœ„ë¥¼ ì˜ë¯¸í•˜ì§€ë§Œ **MLì—ì„œëŠ” ë…¸ë“œë‚´ì˜ GPU ID**ë¼ê³  ë³´ì‹œë©´ ë©ë‹ˆë‹¤
- World Size: í”„ë¡œì„¸ìŠ¤ì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
    
![](../images/process_terms.png)
    
### Multi-process Application ì‹¤í–‰ ë°©ë²•
PyTorchë¡œ êµ¬í˜„ëœ Multi-process ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‹¤í–‰ì‹œí‚¤ëŠ” ë°©ë²•ì€ í¬ê²Œ ë‘ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤.
1. ì‚¬ìš©ìì˜ ì½”ë“œê°€ ë©”ì¸í”„ë¡œì„¸ìŠ¤ê°€ ë˜ì–´ íŠ¹ì • í•¨ìˆ˜ë¥¼ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¡œ ë¶„ê¸°í•œë‹¤.
2. PyTorch ëŸ°ì²˜ê°€ ë©”ì¸í”„ë¡œì„¸ìŠ¤ê°€ ë˜ì–´ ì‚¬ìš©ì ì½”ë“œ ì „ì²´ë¥¼ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¡œ ë¶„ê¸°í•œë‹¤.
    
ì´ ë‘ê°€ì§€ ë°©ë²•ì— ëŒ€í•´ ëª¨ë‘ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. ì´ë•Œ, 'ë¶„ê¸°í•œë‹¤.'ë¼ëŠ” í‘œí˜„ì´ ë‚˜ì˜¤ëŠ”ë°, ì´ëŠ” í•œ í”„ë¡œì„¸ìŠ¤ê°€ ë¶€ëª¨ê°€ ë˜ì–´ ì—¬ëŸ¬ê°œì˜ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¥¼ ë™ì‹œì— ì‹¤í–‰ì‹œí‚¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
   
### 1) ì‚¬ìš©ìì˜ ì½”ë“œê°€ ë©”ì¸í”„ë¡œì„¸ìŠ¤ê°€ ë˜ì–´ íŠ¹ì • í•¨ìˆ˜ë¥¼ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¡œ ë¶„ê¸°í•œë‹¤.
ì´ ë°©ì‹ì€ ì‚¬ìš©ìì˜ ì½”ë“œê°€ ë©”ì¸í”„ë¡œì„¸ìŠ¤ê°€ ë˜ë©° íŠ¹ì • functionì„ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¡œì¨ ë¶„ê¸°í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

![](../images/multi_process_1.png)
    
ì¼ë°˜ì ìœ¼ë¡œ `Spawn`ê³¼ `Fork` ë“± ë‘ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¥¼ ë¶„ê¸° í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- `Spawn`
  - ë©”ì¸í”„ë¡œì„¸ìŠ¤ì˜ ìì›ì„ ë¬¼ë ¤ì£¼ì§€ ì•Šê³  í•„ìš”í•œ ë§Œí¼ì˜ ìì›ë§Œ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ì—ê²Œ ìƒˆë¡œ í• ë‹¹.
  - ì†ë„ê°€ ëŠë¦¬ì§€ë§Œ ì•ˆì „í•œ ë°©ì‹.
- `Fork`
  - ë©”ì¸í”„ë¡œì„¸ìŠ¤ì˜ ëª¨ë“  ìì›ì„ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ì™€ ê³µìœ í•˜ê³  í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘.
  - ì†ë„ê°€ ë¹ ë¥´ì§€ë§Œ ìœ„í—˜í•œ ë°©ì‹.
p.s. ì‹¤ì œë¡œëŠ” `Forkserver` ë°©ì‹ë„ ìˆì§€ë§Œ ìì£¼ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ìƒì†Œí•œ ë°©ì‹ì´ê¸°ì— ìƒëµí•©ë‹ˆë‹¤.

```
"""
src/ch2/multi_process_1.py

ì°¸ê³ :
Jupyter notebookì€ ë©€í‹°í”„ë¡œì„¸ì‹± ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ë™í•˜ëŠ”ë°ì— ë§ì€ ì œì•½ì´ ìˆìŠµë‹ˆë‹¤.
ë”°ë¼ì„œ ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì´ê³³ì—ëŠ” ì½”ë“œë§Œ ë™ë´‰í•˜ê³  ì‹¤í–‰ì€ `src` í´ë”ì— ìˆëŠ” ì½”ë“œë¥¼ ë™ì‘ì‹œí‚¤ê² ìŠµë‹ˆë‹¤.
ì‹¤ì œ ì½”ë“œ ë™ì‘ì€ `src` í´ë”ì— ìˆëŠ” ì½”ë“œë¥¼ ì‹¤í–‰ì‹œì¼œì£¼ì„¸ìš”.
"""

import torch.multiprocessing as mp
# ì¼ë°˜ì ìœ¼ë¡œ mpì™€ ê°™ì€ ì´ë¦„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.


# ì„œë¸Œí”„ë¡œì„¸ìŠ¤ì—ì„œ ë™ì‹œì— ì‹¤í–‰ë˜ëŠ” ì˜ì—­
def fn(rank, param1, param2):
    print(f"{param1} {param2} - rank: {rank}")


# ë©”ì¸ í”„ë¡œì„¸ìŠ¤
if __name__ == "__main__":
    processes = []
    # ì‹œì‘ ë°©ë²• ì„¤ì •
    mp.set_start_method("spawn")

    for rank in range(4):
        process = mp.Process(target=fn, args=(rank, "A0", "B1"))
        # ì„œë¸Œí”„ë¡œì„¸ìŠ¤ ìƒì„±
        #process.daemon = False
        process.daemon = True
        # False means that child processes will run independently of the main process
        # and will not be terminated when the main process exits.
        # ë°ëª¬ ì—¬ë¶€ (ë©”ì¸í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œì‹œ í•¨ê»˜ ì¢…ë£Œ)
        process.start()
        # ì„œë¸Œí”„ë¡œì„¸ìŠ¤ ì‹œì‘
        processes.append(process)

    for process in processes:
        process.join()  # main process is waitiing for the subprocesses exit.
        # ì„œë¸Œ í”„ë¡œì„¸ìŠ¤ join (=ì™„ë£Œë˜ë©´ ì¢…ë£Œ)

    print("Main Process is done")
```

ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•´ì„œ ëª¨ë“€ì„ ë¡œë“œí•˜ê³  `src/ch2` ë””ë ‰í† ë¦¬ë¡œ ì´ë™í•˜ê³  ì½˜ë‹¤ í™˜ê²½ì„ í™œì„±í™” ì‹œí‚¨ë‹¤.
```
[glogin01]$ module load gcc/10.2.0 cmake/3.26.2 cuda/12.1
[glogin01]$ cd large-scale-lm-tutorials/src/ch2
[glogin01]$ conda activate largc-scale-lm   
(large-scale-lm) [glogin01]$ python multi_process_1.py
A0 B1 - rank: 0
A0 B1 - rank: 2
A0 B1 - rank: 3
A0 B1 - rank: 1
Main Process is done
```
`torch.multiprocessing.spawn` í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ë©´ ì´ ê³¼ì •ì„ ë§¤ìš° ì‰½ê²Œ ì§„í–‰ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```
"""
src/ch2/multi_process_2.py
"""

import torch.multiprocessing as mp


# ì„œë¸Œí”„ë¡œì„¸ìŠ¤ì—ì„œ ë™ì‹œì— ì‹¤í–‰ë˜ëŠ” ì˜ì—­
def fn(rank, param1, param2):
    # rankëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë“¤ì–´ì˜´. param1, param2ëŠ” spawnì‹œì— ì…ë ¥ë¨.
    print(f"{param1} {param2} - rank: {rank}")


# ë©”ì¸ í”„ë¡œì„¸ìŠ¤
if __name__ == "__main__":
    mp.spawn(
        fn=fn,
        args=("A0", "B1"),
        nprocs=4,  # ë§Œë“¤ í”„ë¡œì„¸ìŠ¤ ê°œìˆ˜
        join=True,  # í”„ë¡œì„¸ìŠ¤ join ì—¬ë¶€
        daemon=False,  # ë°ëª¬ ì—¬ë¶€
        start_method="spawn",  # ì‹œì‘ ë°©ë²• ì„¤ì •
    )
```

```
(large-scale-lm) [glogin01]$ python multi_process_2.py
A0 B1 - rank: 1
A0 B1 - rank: 0
A0 B1 - rank: 3
A0 B1 - rank: 2
```

```
"""
ì°¸ê³ : torch/multiprocessing/spawn.py

mp.spawn í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ì´ ë™ì‘í•©ë‹ˆë‹¤.
"""

def start_processes(fn, args=(), nprocs=1, join=True, daemon=False, start_method='spawn'):
    _python_version_check()
    mp = multiprocessing.get_context(start_method)
    error_queues = []
    processes = []
    for i in range(nprocs):
        error_queue = mp.SimpleQueue()
        process = mp.Process(
            target=_wrap,
            args=(fn, i, args, error_queue),
            daemon=daemon,
        )
        process.start()
        error_queues.append(error_queue)
        processes.append(process)

    context = ProcessContext(processes, error_queues)
    if not join:
        return context

    # Loop on join until it returns True or raises an exception.
    while not context.join():
        pass
```

### 2) PyTorch ëŸ°ì²˜ê°€ ë¶€ëª¨ í”„ë¡œì„¸ìŠ¤ê°€ ë˜ì–´ ì‚¬ìš©ì ì½”ë“œ ì „ì²´ë¥¼ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¡œ ë¶„ê¸°í•œë‹¤.
ì´ ë°©ì‹ì€ torchì— ë‚´ì¥ëœ ë©€í‹°í”„ë¡œì„¸ì‹± ëŸ°ì²˜ê°€ ì‚¬ìš©ì ì½”ë“œ ì „ì²´ë¥¼ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰ì‹œì¼œì£¼ëŠ” ë§¤ìš° í¸ë¦¬í•œ ë°©ì‹ì…ë‹ˆë‹¤.

`python -m torch.distributed.launch --nproc_per_node=n OOO.py` ë˜ëŠ” `torchrun --nproc_per_node=4 000.py`ì™€ ê°™ì€ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
```
"""
src/ch2/multi_process_3.py
"""

# ì½”ë“œ ì „ì²´ê°€ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ê°€ ë©ë‹ˆë‹¤.
import os

# RANK, LOCAL_RANK, WORLD_SIZE ë“±ì˜ ë³€ìˆ˜ê°€ ìë™ìœ¼ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.
print(f"hello world, {os.environ['RANK']}")
```

```
# (large-scale-lm) [glogin01]$ python -m torch.distributed.launch --nproc_per_node=4 multi_process_3.py
(large-scale-lm) [glogin01]$ torchrun --nproc_per_node=4 multi_process_3.py
W0908 14:12:43.550000 140009840834368 torch/distributed/run.py:757]
W0908 14:12:43.550000 140009840834368 torch/distributed/run.py:757] *****************************************
W0908 14:12:43.550000 140009840834368 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0908 14:12:43.550000 140009840834368 torch/distributed/run.py:757] *****************************************
hello world, 0
hello world, 3
hello world, 1
hello world, 2
```
 

## 2. Distributed Programming with PyTorch
### Concept of Message Passing
ë©”ì‹œì§€ íŒ¨ì‹±ì´ë€ ë™ì¼í•œ ì£¼ì†Œê³µê°„ì„ ê³µìœ í•˜ì§€ ì•ŠëŠ” ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ë“¤ì´ ë°ì´í„°ë¥¼ ì£¼ê³  ë°›ì„ ìˆ˜ ìˆë„ë¡ ë©”ì‹œì§€ë¼ëŠ” ê°„ì ‘ ì •ë³´ë¥¼ ì£¼ê³  ë°›ëŠ” ê²ƒì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´ Process-1ì´ íŠ¹ì • íƒœê·¸ê°€ ë‹¬ë¦° ë°ì´í„°ë¥¼ ë©”ì‹œì§€ íì— sendí•˜ë„ë¡, Process-2ê°€ í•´ë‹¹ ë°ì´í„°ë¥¼ receiveí•˜ë„ë¡ ì½”ë”©í•´ë†“ìœ¼ë©´ ë‘ í”„ë¡œì„¸ìŠ¤ê°€ ê³µìœ í•˜ëŠ” ë©”ëª¨ë¦¬ ê³µê°„ ì—†ì´ë„ ë°ì´í„°ë¥¼ ì£¼ê³  ë°›ì„ ìˆ˜ ìˆì£ . Large-scale ëª¨ë¸ ê°œë°œì‹œì— ì‚¬ìš©ë˜ëŠ” ë¶„ì‚° í†µì‹ ì—ëŠ” ëŒ€ë¶€ë¶„ ì´ëŸ¬í•œ message passing ê¸°ë²•ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.
  
![](../images/message_passing.png)
   
### MPI (Massage Passing Interface)
MPIëŠ” Message Passingì— ëŒ€í•œ í‘œì¤€ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. MPIì—ëŠ” Processê°„ì˜ Message Passingì— ì‚¬ìš©ë˜ëŠ” ì—¬ëŸ¬ ì—°ì‚°(e.g. broadcast, reduce, scatter, gather, ...)ì´ ì •ì˜ë˜ì–´ ìˆìœ¼ë©° ëŒ€í‘œì ìœ¼ë¡œ OpenMPIë¼ëŠ” ì˜¤í”ˆì†ŒìŠ¤ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.

![](../images/open_mpi.png)
   
### NCCL & GLOO\n",
ì‹¤ì œë¡œëŠ” openmpi ë³´ë‹¤ëŠ” ncclì´ë‚˜ gloo ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ê²Œ ë©ë‹ˆë‹¤.
- NCCL (NVIDIA Collective Communication Library)
  - NVIDIAì—ì„œ ê°œë°œí•œ GPU íŠ¹í™” Message Passing ë¼ì´ë¸ŒëŸ¬ë¦¬ ('nickel'ì´ë¼ê³  ì½ìŒ)\n",
  - NVIDIA GPUì—ì„œ ì‚¬ìš©ì‹œ, ë‹¤ë¥¸ ë„êµ¬ì— ë¹„í•´ ì›”ë“±íˆ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ìˆìŠµë‹ˆë‹¤.\n",
- GLOO (Facebook's Collective Communication Library)\n",
  - Facebookì—ì„œ ê°œë°œëœ Message Passing ë¼ì´ë¸ŒëŸ¬ë¦¬. \n",
  - `torch`ì—ì„œëŠ” ì£¼ë¡œ CPU ë¶„ì‚°ì²˜ë¦¬ì— ì‚¬ìš©í•˜ë¼ê³  ì¶”ì²œí•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
  
### ë°±ì—”ë“œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ ê°€ì´ë“œ
openmpië¥¼ ì¨ì•¼í•  íŠ¹ë³„í•œ ì´ìœ ê°€ ìˆëŠ” ê²ƒì´ ì•„ë‹ˆë¼ë©´ ncclì´ë‚˜ glooë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, GPUì—ì„œ ì‚¬ìš©ì‹œ nccl, CPUì—ì„œ ì‚¬ìš©ì‹œ glooë¥¼ ì‚¬ìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤. ë” ìì„¸í•œ ì •ë³´ëŠ”  https://pytorch.org/docs/stable/distributed.html ì—¬ê¸°ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.
  
ê° ë°±ì—”ë“œë³„ë¡œ ìˆ˜í–‰ ê°€ëŠ¥í•œ ì—°ì‚°ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

![](../images/backends.png)
    
### `torch.distributed` íŒ¨í‚¤ì§€
`gloo`, `nccl`, `openmpi` ë“±ì„ ì§ì ‘ ì‚¬ìš©í•´ë³´ëŠ” ê²ƒì€ ë¶„ëª… ì¢‹ì€ ê²½í—˜ì´ ë  ê²ƒì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì‹œê°„ ê´€ê³„ìƒ ì´ë“¤ì„ ëª¨ë‘ ë‹¤ë£° ìˆ˜ëŠ” ì—†ê³ , ì´ë“¤ì„ wrapping í•˜ê³  ìˆëŠ” `torch.distributed` íŒ¨í‚¤ì§€ë¥¼  ì‚¬ìš©í•˜ì—¬ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤. ì‹¤ì œë¡œ í™œìš© ë‹¨ìœ¼ë¡œ ê°€ë©´ `nccl` ë“±ì„ ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•Šê³  ëŒ€ë¶€ë¶„ì˜ ê²½ìš° `torch.distributed` ë“±ì˜ í•˜ì´ë ˆë²¨ íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡œê·¸ë˜ë° í•˜ê²Œ ë©ë‹ˆë‹¤.
  
### Process Group
ë§ì€ í”„ë¡œì„¸ìŠ¤ë¥¼ ê´€ë¦¬í•˜ëŠ” ê²ƒì€ ì–´ë ¤ìš´ ì¼ì…ë‹ˆë‹¤. ë”°ë¼ì„œ í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ì„ ë§Œë“¤ì–´ì„œ ê´€ë¦¬ë¥¼ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤. `init_process_group`ë¥¼ í˜¸ì¶œí•˜ë©´ ì „ì²´ í”„ë¡œì„¸ìŠ¤ê°€ ì†í•œ default_pg(process group)ê°€ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ì„ ì´ˆê¸°í™”í•˜ëŠ” `init_process_group` í•¨ìˆ˜ëŠ” **ë°˜ë“œì‹œ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰**ë˜ì–´ì•¼ í•˜ë©°, ë§Œì•½ ì¶”ê°€ë¡œ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ë“¤ë§Œ ëª¨ì•„ì„œ ê·¸ë£¹ì„ ìƒì„±í•˜ë ¤ë©´ `new_group`ì„ í˜¸ì¶œí•˜ë©´ ë©ë‹ˆë‹¤.
```
"""
src/ch2/process_group_1.py
"""

import torch.distributed as dist
# ì¼ë°˜ì ìœ¼ë¡œ distì™€ ê°™ì€ ì´ë¦„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

dist.init_process_group(backend="nccl", rank=0, world_size=1)
# í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ì´ˆê¸°í™”
# ë³¸ ì˜ˆì œì—ì„œëŠ” ê°€ì¥ ìì£¼ ì‚¬ìš©í•˜ëŠ” ncclì„ ê¸°ë°˜ìœ¼ë¡œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.
# backendì— 'nccl' ëŒ€ì‹  'mpi'ë‚˜ 'gloo'ë¥¼ ë„£ì–´ë„ ë©ë‹ˆë‹¤.

process_group = dist.new_group([0])
# 0ë²ˆ í”„ë¡œì„¸ìŠ¤ê°€ ì†í•œ í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ìƒì„±

print(process_group)
```

```
(large-scale-lm) [glogin01]$ python process_group_1.py
Traceback (most recent call last):
  File "/scratch/qualis/git-projects/large-scale-lm-tutorials/src/ch2/process_group_1.py", line 8, in <module>
    dist.init_process_group(backend="nccl", rank=0, world_size=1)
  File "/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
    return func(*args, **kwargs)
  File "/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 89, in wrapper
    func_return = func(*args, **kwargs)
  File "/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1305, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 242, in _env_rendezvous_handler
    master_addr = _get_env_or_raise("MASTER_ADDR")
  File "/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 219, in _get_env_or_raise
    raise _env_error(env_var)
ValueError: Error initializing torch.distributed using env:// rendezvous: environment variable MASTER_ADDR expected, but not set
```

ìœ„ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” `MASTER_ADDR`, `MASTER_PORT` ë“± í•„ìš”í•œ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ ê°’ë“¤ì„ ì„¤ì •í•˜ê³  ë‹¤ì‹œ ì‹¤í–‰ì‹œí‚¤ê² ìŠµë‹ˆë‹¤.
```
"""
src/ch2/process_group_2.py
"""

import torch.distributed as dist
import os


# ì¼ë°˜ì ìœ¼ë¡œ ì´ ê°’ë“¤ë„ í™˜ê²½ë³€ìˆ˜ë¡œ ë“±ë¡í•˜ê³  ì‚¬ìš©í•©ë‹ˆë‹¤.
os.environ["RANK"] = "0"
os.environ["LOCAL_RANK"] = "0"
os.environ["WORLD_SIZE"] = "1"

# í†µì‹ ì— í•„ìš”í•œ ì£¼ì†Œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
os.environ["MASTER_ADDR"] = "localhost"  # í†µì‹ í•  ì£¼ì†Œ (ë³´í†µ localhostë¥¼ ì”ë‹ˆë‹¤.)
os.environ["MASTER_PORT"] = "29500"  # í†µì‹ í•  í¬íŠ¸ (ì„ì˜ì˜ ê°’ì„ ì„¤ì •í•´ë„ ì¢‹ìŠµë‹ˆë‹¤.)

dist.init_process_group(backend="nccl", rank=0, world_size=1)
# í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ì´ˆê¸°í™”

process_group = dist.new_group([0])
# 0ë²ˆ í”„ë¡œì„¸ìŠ¤ê°€ ì†í•œ í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ìƒì„±

print(process_group)
```

```
(large-scale-lm) [globin01]$ python process_group_2.py
<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1ca3ed60f0>
```

ìœ„ ì˜ˆì œëŠ” í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ì˜ APIë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•´ì„œ ë©”ì¸í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. (í”„ë¡œì„¸ìŠ¤ê°€ 1ê°œ ë¿ì´ë¼ ìƒê´€ì—†ì—ˆìŒ) ì‹¤ì œë¡œëŠ” ë©€í‹°í”„ë¡œì„¸ìŠ¤ ì‘ì—…ì‹œ í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ìƒì„± ë“±ì˜ ì‘ì—…ì€ ë°˜ë“œì‹œ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

```
"""
src/process_group_3.py
"""

import torch.multiprocessing as mp
import torch.distributed as dist
import os


# ì„œë¸Œí”„ë¡œì„¸ìŠ¤ì—ì„œ ë™ì‹œì— ì‹¤í–‰ë˜ëŠ” ì˜ì—­
def fn(rank, world_size):
    # rankëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë“¤ì–´ì˜´. world_sizeëŠ” ì…ë ¥ë¨.
    dist.init_process_group(backend="nccl", rank=rank, world_size=world_size)
    group = dist.new_group([_ for _ in range(world_size)])
    print(f"{group} - rank: {rank}")


# ë©”ì¸ í”„ë¡œì„¸ìŠ¤
if __name__ == "__main__":
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "29500"
    os.environ["WORLD_SIZE"] = "4"

    mp.spawn(
        fn=fn,
        args=(4,),  # world_size ì…ë ¥
        nprocs=4,  # ë§Œë“¤ í”„ë¡œì„¸ìŠ¤ ê°œìˆ˜
        join=True,  # í”„ë¡œì„¸ìŠ¤ join ì—¬ë¶€
        daemon=False,  # ë°ëª¬ ì—¬ë¶€
        start_method="spawn",  # ì‹œì‘ ë°©ë²• ì„¤ì •
    )
```

```
(large-scale-lm) [glogin01]$ python process_group_3.py
<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f883a7f5870> - rank: 1
<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1515d05a30> - rank: 2
<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f311f6fb5f0> - rank: 3
<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1c00707d30> - rank: 0
```

`python -m torch.distributed.launch --nproc_per_node=n OOO.py`ë¥¼ ì‚¬ìš©í• ë•ŒëŠ” ì•„ë˜ì™€ ê°™ì´ ì²˜ë¦¬í•©ë‹ˆë‹¤. `dist.get_rank()`, `dist_get_world_size()`ì™€ ê°™ì€ í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ `rank`ì™€ `world_size`ë¥¼ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```
"""
src/ch2/process_group_4.py
"""

import torch.distributed as dist

dist.init_process_group(backend="nccl")
# í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ì´ˆê¸°í™”

group = dist.new_group([_ for _ in range(dist.get_world_size())])
# í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ìƒì„±

print(f"{group} - rank: {dist.get_rank()}\n")
```
```
# (large-scale-lm) [glogin01]$ python -m torch.distributed.launch --nproc_per_node=4 process_group_4.py
(large-scale-lm) [glogin01]$ torchrun --nproc_per_node=4 process_group_4.py
W0908 14:33:37.263000 140621451208512 torch/distributed/run.py:757]
W0908 14:33:37.263000 140621451208512 torch/distributed/run.py:757] *****************************************
W0908 14:33:37.263000 140621451208512 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0908 14:33:37.263000 140621451208512 torch/distributed/run.py:757] *****************************************
<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fee69a086f0> - rank: 3

<torch.distributed.distributed_c10d.ProcessGroup object at 0x7faeaaa9b8f0> - rank: 1

<torch.distributed.distributed_c10d.ProcessGroup object at 0x7efd8c4e1930> - rank: 0

<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fde9bf62d30> - rank: 2
```

### P2P Communication (Point to point)
![](../images/p2p.png)
P2P (Point to point, ì  ëŒ€ ì ) í†µì‹ ì€ íŠ¹ì • í”„ë¡œì„¸ìŠ¤ì—ì„œ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ ë°ì´í„°ë¥¼ ì „ì†¡í•˜ëŠ” í†µì‹ ì´ë©° `torch.distributed` íŒ¨í‚¤ì§€ì˜ `send`, `recv` í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ í†µì‹ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```
"""
src/ch2/p2p_communication.py
"""

import torch
import torch.distributed as dist

dist.init_process_group("gloo")
# í˜„ì¬ ncclì€ send, recvë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (2021/10/21)

if dist.get_rank() == 0:
    tensor = torch.randn(2, 2)
    dist.send(tensor, dst=1)

elif dist.get_rank() == 1:
    tensor = torch.zeros(2, 2)
    print(f"rank 1 before: {tensor}\n")
    dist.recv(tensor, src=0)
    print(f"rank 1 after: {tensor}\n")

else:
    raise RuntimeError("wrong rank")
```

```
#(large-scale-lm) [glogin01]$ python -m torch.distributed.launch --nproc_per_node=2 p2p_communication.py
(large-scale-lm) [glogin01]$ torchrun --nproc_per_node=2 p2p_communication.py
W0908 15:33:07.475000 139958444984128 torch/distributed/run.py:757]
W0908 15:33:07.475000 139958444984128 torch/distributed/run.py:757] *****************************************
W0908 15:33:07.475000 139958444984128 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0908 15:33:07.475000 139958444984128 torch/distributed/run.py:757] *****************************************
rank 1 before: tensor([[0., 0.],
        [0., 0.]])

rank 1 after: tensor([[ 3.4373,  1.1562],
        [ 0.1356, -1.2088]])
```
ì£¼ì˜í•  ê²ƒì€ ì´ë“¤ì´ ë™ê¸°ì ìœ¼ë¡œ í†µì‹ í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ë¹„ë™ê¸° í†µì‹ (non-blocking)ì—ëŠ” `isend`, `irecv`ë¥¼ ì´ìš©í•©ë‹ˆë‹¤. ì´ë“¤ì€ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‘ë™í•˜ê¸° ë•Œë¬¸ì— `wait()` ë©”ì„œë“œë¥¼ í†µí•´ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ì˜ í†µì‹ ì´ ëë‚ ë•Œ ê¹Œì§€ ê¸°ë‹¤ë¦¬ê³  ë‚œ ë’¤ì— ì ‘ê·¼í•´ì•¼í•©ë‹ˆë‹¤.


```
"""
src/ch2/p2p_communication_non_blocking.py
"""

import torch
import torch.distributed as dist

dist.init_process_group("gloo")
# í˜„ì¬ ncclì€ send, recvë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (2021/10/21)

if dist.get_rank() == 0:
    tensor = torch.randn(2, 2)
    request = dist.isend(tensor, dst=1)
elif dist.get_rank() == 1:
    tensor = torch.zeros(2, 2)
    request = dist.irecv(tensor, src=0)
else:
    raise RuntimeError("wrong rank")

request.wait()

print(f"rank {dist.get_rank()}: {tensor}")
```

```
#(large-scale-lm) [globin01]$ python -m torch.distributed.launch --nproc_per_node=2 p2p_communication_non_blocking.py
(large-scale-lm) [glogin01]$ torchrun --nproc_per_node=2 p2p_communication_non_blocking.py
W0908 15:36:30.701000 139730208278336 torch/distributed/run.py:757]
W0908 15:36:30.701000 139730208278336 torch/distributed/run.py:757] *****************************************
W0908 15:36:30.701000 139730208278336 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0908 15:36:30.701000 139730208278336 torch/distributed/run.py:757] *****************************************
rank 0: tensor([[-1.2224,  0.7753],
        [ 0.9491, -1.0411]])
rank 1: tensor([[-1.2224,  0.7753],
        [ 0.9491, -1.0411]])
```

### Collective Communication
Collective Communicationì€ ì§‘í•©í†µì‹ ì´ë¼ëŠ” ëœ»ìœ¼ë¡œ ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ê°€ ì°¸ì—¬í•˜ì—¬ í†µì‹ í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì—°ì‚°ë“¤ì´ ìˆì§€ë§Œ ê¸°ë³¸ì ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì€ 4ê°œì˜ ì—°ì‚°(`broadcast`, `scatter`, `gather`, `reduce`)ì´ ê¸°ë³¸ ì„¸íŠ¸ì…ë‹ˆë‹¤.
   
![](../images/collective.png)
  
ì—¬ê¸°ì— ì¶”ê°€ë¡œ `all-reduce`, `all-gather`, `reduce-scatter` ë“±ì˜ ë³µí•© ì—°ì‚°ê³¼ ë™ê¸°í™” ì—°ì‚°ì¸ `barrier`ê¹Œì§€ ì´ 8ê°œ ì—°ì‚°ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. ì¶”ê°€ë¡œ ë§Œì•½ ì´ëŸ¬í•œ ì—°ì‚°ë“¤ì„ ë¹„ë™ê¸° ëª¨ë“œë¡œ ì‹¤í–‰í•˜ë ¤ë©´ ê° ì—°ì‚° ìˆ˜í–‰ì‹œ `async_op` íŒŒë¼ë¯¸í„°ë¥¼ `True`ë¡œ ì„¤ì •í•˜ë©´ ë©ë‹ˆë‹¤.
   
#### 1) Broadcast
BroadcastëŠ” íŠ¹ì • í”„ë¡œì„¸ìŠ¤ì— ìˆëŠ” ë°ì´í„°ë¥¼ ê·¸ë£¹ë‚´ì˜ ëª¨ë“  í”„ë¡œì„¸ìŠ¤ì— ë³µì‚¬í•˜ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤.
 
![](../images/broadcast.png)

```
"""
src/broadcast.py
"""

import torch
import torch.distributed as dist

dist.init_process_group("nccl")
rank = dist.get_rank()
torch.cuda.set_device(rank)
# deviceë¥¼ settingí•˜ë©´ ì´í›„ì— rankì— ë§ëŠ” ë””ë°”ì´ìŠ¤ì— ì ‘ê·¼ ê°€ëŠ¥í•©ë‹ˆë‹¤.

if rank == 0:
    tensor = torch.randn(2, 2).to(torch.cuda.current_device())
else:
    tensor = torch.zeros(2, 2).to(torch.cuda.current_device())

print(f"before rank {rank}: {tensor}\n")
dist.broadcast(tensor, src=0)
print(f"after rank {rank}: {tensor}\n")
```

Collective Communication ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•´ì„œ ë‰´ë¡  ì‹œìŠ¤í…œì—ì„œ GPU 4ê°œë¥¼ í• ë‹¹ë°›ëŠ”ë‹¤
```
# [glogin01]$ salloc --partition=amd_a100nv_8 -J debug --nodes=1 --time=8:00:00 --gres=gpu:4 --comment pytorch
[glogin01]$ salloc --partition=cas_v100_4 -J debug --nodes=2 --time=12:00:00 --gres=gpu:2 --comment pytorch
salloc: Pending job allocation 340482
salloc: job 340482 queued and waiting for resources
salloc: job 340482 has been allocated resources
salloc: Granted job allocation 340482
salloc: Waiting for resource configuration
salloc: Nodes gpu[10,17] are ready for job
```

```
[gpu10]$ pwd
/scratch/qualis/git-projects/large-scale-lm-tutorials/src/ch2
[gpu10]$ conda activate large-scale-lm
(large-scale-lm) [gpu10]$ module load gcc/10.2.0 cmake/3.26.2 cuda/12.1
(large-scale-lm) [gpu10]$ srun torchrun --nnodes=2 --nproc_per_node=2 --rdzv_backend c10d --rdzv_endpoint gpu10:12345 broadcast.py
W0908 15:57:10.869000 47684370574400 torch/distributed/run.py:757]
W0908 15:57:10.869000 47684370574400 torch/distributed/run.py:757] *****************************************
W0908 15:57:10.869000 47684370574400 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable
for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for
optimal performance in your application as needed.
W0908 15:57:10.869000 47684370574400 torch/distributed/run.py:757] *****************************************
W0908 15:57:11.077000 47206882413632 torch/distributed/run.py:757]
W0908 15:57:11.077000 47206882413632 torch/distributed/run.py:757] *****************************************
W0908 15:57:11.077000 47206882413632 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable
for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for
optimal performance in your application as needed.
W0908 15:57:11.077000 47206882413632 torch/distributed/run.py:757] *****************************************
before rank 1: tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:1')

before rank 0: tensor([[-1.1589, -1.0418, -0.2843,  ...,  0.1931, -2.5755, -0.0232],
        [ 1.5140, -0.9782, -0.3955,  ...,  0.2272,  1.4831,  0.7004],
        [ 0.6604,  0.0533, -2.1374,  ..., -0.7254,  0.1704, -2.5956],
        ...,
        [ 0.9011,  1.0888,  0.0632,  ...,  0.8425, -0.6508,  0.4218],
        [ 0.5068, -0.3434, -0.2022,  ..., -0.0231, -0.1183, -1.1983],
        [ 0.3403, -1.5891,  0.4225,  ..., -0.1733,  1.2176,  1.0935]],
       device='cuda:0')

before rank 3: tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:1')

before rank 2: tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')

after rank 0: tensor([[-1.1589, -1.0418, -0.2843,  ...,  0.1931, -2.5755, -0.0232],
        [ 1.5140, -0.9782, -0.3955,  ...,  0.2272,  1.4831,  0.7004],
        [ 0.6604,  0.0533, -2.1374,  ..., -0.7254,  0.1704, -2.5956],
        ...,
        [ 0.9011,  1.0888,  0.0632,  ...,  0.8425, -0.6508,  0.4218],
        [ 0.5068, -0.3434, -0.2022,  ..., -0.0231, -0.1183, -1.1983],
        [ 0.3403, -1.5891,  0.4225,  ..., -0.1733,  1.2176,  1.0935]],
       device='cuda:0')

after rank 1: tensor([[-1.1589, -1.0418, -0.2843,  ...,  0.1931, -2.5755, -0.0232],
        [ 1.5140, -0.9782, -0.3955,  ...,  0.2272,  1.4831,  0.7004],
        [ 0.6604,  0.0533, -2.1374,  ..., -0.7254,  0.1704, -2.5956],
        ...,
        [ 0.9011,  1.0888,  0.0632,  ...,  0.8425, -0.6508,  0.4218],
        [ 0.5068, -0.3434, -0.2022,  ..., -0.0231, -0.1183, -1.1983],
        [ 0.3403, -1.5891,  0.4225,  ..., -0.1733,  1.2176,  1.0935]],
       device='cuda:1')

after rank 2: tensor([[-1.1589, -1.0418, -0.2843,  ...,  0.1931, -2.5755, -0.0232],
        [ 1.5140, -0.9782, -0.3955,  ...,  0.2272,  1.4831,  0.7004],
        [ 0.6604,  0.0533, -2.1374,  ..., -0.7254,  0.1704, -2.5956],
        ...,
        [ 0.9011,  1.0888,  0.0632,  ...,  0.8425, -0.6508,  0.4218],
        [ 0.5068, -0.3434, -0.2022,  ..., -0.0231, -0.1183, -1.1983],
        [ 0.3403, -1.5891,  0.4225,  ..., -0.1733,  1.2176,  1.0935]],
       device='cuda:0')

after rank 3: tensor([[-1.1589, -1.0418, -0.2843,  ...,  0.1931, -2.5755, -0.0232],
        [ 1.5140, -0.9782, -0.3955,  ...,  0.2272,  1.4831,  0.7004],
        [ 0.6604,  0.0533, -2.1374,  ..., -0.7254,  0.1704, -2.5956],
        ...,
        [ 0.9011,  1.0888,  0.0632,  ...,  0.8425, -0.6508,  0.4218],
        [ 0.5068, -0.3434, -0.2022,  ..., -0.0231, -0.1183, -1.1983],
        [ 0.3403, -1.5891,  0.4225,  ..., -0.1733,  1.2176,  1.0935]],
       device='cuda:1')
```
`send`, `recv` ë“±ì˜ P2P ì—°ì‚°ì´ ì§€ì›ë˜ì§€ ì•Šì„ë•Œ ì•Šì•„ì„œ `broadcast`ë¥¼ P2P í†µì‹  ìš©ë„ë¡œ ì‚¬ìš©í•˜ê¸°ë„ í•©ë‹ˆë‹¤. src=0, dst=1 ì¼ë•Œ, `new_group([0, 1])` ê·¸ë£¹ì„ ë§Œë“¤ê³  `broadcast`ë¥¼ ìˆ˜í–‰í•˜ë©´ 0 -> 1 P2Pì™€ ë™ì¼í•©ë‹ˆë‹¤.

```
"""
ì°¸ê³ : deepspeed/deepspeed/runtime/pipe/p2p.py
"""

def send(tensor, dest_stage, async_op=False):
    global _groups
    assert async_op == False, "Doesnt support async_op true"
    src_stage = _grid.get_stage_id()
    _is_valid_send_recv(src_stage, dest_stage)

    dest_rank = _grid.stage_to_global(stage_id=dest_stage)
    if async_op:
        global _async
        op = dist.isend(tensor, dest_rank)
        _async.append(op)
    else:

        if can_send_recv():
            return dist.send(tensor, dest_rank)
        else:
            group = _get_send_recv_group(src_stage, dest_stage)
            src_rank = _grid.stage_to_global(stage_id=src_stage)
            return dist.broadcast(tensor, src_rank, group=group, async_op=async_op)
```
#### 2) Reduce
ReduceëŠ” ê° í”„ë¡œì„¸ìŠ¤ê°€ ê°€ì§„ ë°ì´í„°ë¡œ íŠ¹ì • ì—°ì‚°ì„ ìˆ˜í–‰í•´ì„œ ì¶œë ¥ì„ í•˜ë‚˜ì˜ ë””ë°”ì´ìŠ¤ë¡œ ëª¨ì•„ì£¼ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤. ì—°ì‚°ì€ ì£¼ë¡œ sum, max, min ë“±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
![](../images/reduce.png)
```
"""
src/reduce_sum.py
"""

import torch
import torch.distributed as dist

dist.init_process_group("nccl")
rank = dist.get_rank()
torch.cuda.set_device(rank)

tensor = torch.ones(2, 2).to(torch.cuda.current_device()) * rank
# rank==0 => [[0, 0], [0, 0]]
# rank==1 => [[1, 1], [1, 1]]
# rank==2 => [[2, 2], [2, 2]]
# rank==3 => [[3, 3], [3, 3]]

dist.reduce(tensor, op=torch.distributed.ReduceOp.SUM, dst=0)

if rank == 0:
    print(tensor)
```
```
[glogin01]$ python -m torch.distributed.launch --nproc_per_node=4 ../src/reduce_sum.py
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being
overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
tensor([[6., 6.],
        [6., 6.]], device='cuda:0')
```

```
"""
src/reduce_max.py
"""

import torch
import torch.distributed as dist

dist.init_process_group("nccl")
rank = dist.get_rank()
torch.cuda.set_device(rank)

tensor = torch.ones(2, 2).to(torch.cuda.current_device()) * rank
# rank==0 => [[0, 0], [0, 0]]
# rank==1 => [[1, 1], [1, 1]]
# rank==2 => [[2, 2], [2, 2]]
# rank==3 => [[3, 3], [3, 3]]

dist.reduce(tensor, op=torch.distributed.ReduceOp.MAX, dst=0)

if rank == 0:
    print(tensor)
```
```
[glogin01]$ python -m torch.distributed.launch --nproc_per_node=4 ../src/reduce_max.py
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being
overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
tensor([[3., 3.],
        [3., 3.]], device='cuda:0')
```
#### 3) Scatter
ScatterëŠ” ì—¬ëŸ¬ê°œì˜ elementë¥¼ ìª¼ê°œì„œ ê° deviceì— ë¿Œë ¤ì£¼ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤.
![](../images/scatter.png)
```
"""
src/scatter.py
"""

import torch
import torch.distributed as dist

dist.init_process_group("gloo")
# ncclì€ scatterë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
rank = dist.get_rank()
torch.cuda.set_device(rank)


output = torch.zeros(1)
print(f"before rank {rank}: {output}\n")

if rank == 0:
    inputs = torch.tensor([10.0, 20.0, 30.0, 40.0])
    inputs = torch.split(inputs, dim=0, split_size_or_sections=1)
    # (tensor([10]), tensor([20]), tensor([30]), tensor([40]))
    dist.scatter(output, scatter_list=list(inputs), src=0)
else:
    dist.scatter(output, src=0)

print(f"after rank {rank}: {output}\n")
```
```
[glogin01]$ python -m torch.distributed.launch --nproc_per_node=4 ../src/scatter.py
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being
overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
before rank 0: tensor([0.])

before rank 3: tensor([0.])

after rank 3: tensor([40.])

before rank 1: tensor([0.])

before rank 2: tensor([0.])

after rank 0: tensor([10.])
after rank 1: tensor([20.])


after rank 2: tensor([30.])
```
ncclì—ì„œëŠ” scatterê°€ ì§€ì›ë˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì•„ë˜ì™€ ê°™ì€ ë°©ë²•ìœ¼ë¡œ scatter ì—°ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
```
"""
src/scatter_nccl.py
"""

import torch
import torch.distributed as dist

dist.init_process_group("nccl")
rank = dist.get_rank()
torch.cuda.set_device(rank)

inputs = torch.tensor([10.0, 20.0, 30.0, 40.0])
inputs = torch.split(tensor=inputs, dim=-1, split_size_or_sections=1)
output = inputs[rank].contiguous().to(torch.cuda.current_device())
print(f"after rank {rank}: {output}\n")
```
```
[glogin01]$ python -m torch.distributed.launch --nproc_per_node=4 ../src/scatter_nccl.py
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system beingoverloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
after rank 2: tensor([30.], device='cuda:2')

after rank 3: tensor([40.], device='cuda:3')

after rank 0: tensor([10.], device='cuda:0')

after rank 1: tensor([20.], device='cuda:1')
```
```
"""
ì°¸ê³ : megatron-lm/megatron/mpu/mappings.py
"""

def _split(input_):
    """Split the tensor along its last dimension and keep the
    corresponding slice."""

    world_size = get_tensor_model_parallel_world_size()
    # Bypass the function if we are using only 1 GPU.
    if world_size==1:
        return input_

    # Split along last dimension.
    input_list = split_tensor_along_last_dim(input_, world_size)

    # Note: torch.split does not create contiguous tensors by default.
    rank = get_tensor_model_parallel_rank()
    output = input_list[rank].contiguous()

    return output

class _ScatterToModelParallelRegion(torch.autograd.Function):
    """Split the input and keep only the corresponding chuck to the rank."""

    @staticmethod
    def symbolic(graph, input_):
        return _split(input_)

    @staticmethod
    def forward(ctx, input_):
        return _split(input_)

    @staticmethod
    def backward(ctx, grad_output):
        return _gather(grad_output)
```
#### 4) Gather
GatherëŠ” ì—¬ëŸ¬ ë””ë°”ì´ìŠ¤ì— ì¡´ì¬í•˜ëŠ” í…ì„œë¥¼ í•˜ë‚˜ë¡œ ëª¨ì•„ì£¼ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤.
![](../images/gather.png)
```
"""
src/gather.py
"""

import torch
import torch.distributed as dist

dist.init_process_group("gloo")
# ncclì€ gatherë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
rank = dist.get_rank()
torch.cuda.set_device(rank)

input = torch.ones(1) * rank
# rank==0 => [0]
# rank==1 => [1]
# rank==2 => [2]
# rank==3 => [3]

if rank == 0:
    outputs_list = [torch.zeros(1), torch.zeros(1), torch.zeros(1), torch.zeros(1)]
    dist.gather(input, gather_list=outputs_list, dst=0)
    print(outputs_list)
else:
    dist.gather(input, dst=0)
```
```
[glogin01]$ python -m torch.distributed.launch --nproc_per_node=4 ../src/gather.py
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being
overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[tensor([0.]), tensor([1.]), tensor([2.]), tensor([3.])]
```
#### 5) All-reduce
ì´ë¦„ ì•ì— All- ì´ ë¶™ì€ ì—°ì‚°ë“¤ì€ í•´ë‹¹ ì—°ì‚°ì„ ìˆ˜í–‰ í•œë’¤, ê²°ê³¼ë¥¼ ëª¨ë“  ë””ë°”ì´ìŠ¤ë¡œ broadcastí•˜ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ All-reduceëŠ” reduceë¥¼ ìˆ˜í–‰í•œ ë’¤, ê³„ì‚°ëœ ê²°ê³¼ë¥¼ ëª¨ë“  ë””ë°”ì´ìŠ¤ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤.\n",
![](../images/allreduce.png)
```
"""
src/allreduce_sum.py
"""

import torch
import torch.distributed as dist

dist.init_process_group("nccl")
rank = dist.get_rank()
torch.cuda.set_device(rank)

tensor = torch.ones(2, 2).to(torch.cuda.current_device()) * rank
# rank==0 => [[0, 0], [0, 0]]
# rank==1 => [[1, 1], [1, 1]]
# rank==2 => [[2, 2], [2, 2]]
# rank==3 => [[3, 3], [3, 3]]

dist.all_reduce(tensor, op=torch.distributed.ReduceOp.SUM)

print(f"rank {rank}: {tensor}\n")
```
```
[glogin01]$ python -m torch.distributed.launch --nproc_per_node=4 ../src/allreduce_sum.py
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being
overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
rank 1: tensor([[6., 6.],
        [6., 6.]], device='cuda:1')

rank 2: tensor([[6., 6.],
        [6., 6.]], device='cuda:2')
rank 0: tensor([[6., 6.],
        [6., 6.]], device='cuda:0')


rank 3: tensor([[6., 6.],
        [6., 6.]], device='cuda:3')

```
```
"""
src/allreduce_max.py
"""

import torch
import torch.distributed as dist

dist.init_process_group("nccl")
rank = dist.get_rank()
torch.cuda.set_device(rank)

tensor = torch.ones(2, 2).to(torch.cuda.current_device()) * rank
# rank==0 => [[0, 0], [0, 0]]
# rank==1 => [[1, 1], [1, 1]]
# rank==2 => [[2, 2], [2, 2]]
# rank==3 => [[3, 3], [3, 3]]

dist.all_reduce(tensor, op=torch.distributed.ReduceOp.MAX)

print(f"rank {rank}: {tensor}\n")
```
```
[glogin01]$ python -m torch.distributed.launch --nproc_per_node=4 ../src/allreduce_max.py
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being
overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
rank 3: tensor([[3., 3.],
        [3., 3.]], device='cuda:3')

rank 1: tensor([[3., 3.],
        [3., 3.]], device='cuda:1')

rank 2: tensor([[3., 3.],
        [3., 3.]], device='cuda:2')

rank 0: tensor([[3., 3.],
        [3., 3.]], device='cuda:0')
```

#### 6) All-gather
All-gatherëŠ” gatherë¥¼ ìˆ˜í–‰í•œ ë’¤, ëª¨ì•„ì§„ ê²°ê³¼ë¥¼ ëª¨ë“  ë””ë°”ì´ìŠ¤ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤.
![](../images/allgather.png)
```
"""
src/allgather.py
"""

import torch
import torch.distributed as dist

dist.init_process_group("nccl")
rank = dist.get_rank()
torch.cuda.set_device(rank)

input = torch.ones(1).to(torch.cuda.current_device()) * rank
# rank==0 => [0]
# rank==1 => [1]
# rank==2 => [2]
# rank==3 => [3]

outputs_list = [
    torch.zeros(1, device=torch.device(torch.cuda.current_device())),
    torch.zeros(1, device=torch.device(torch.cuda.current_device())),
    torch.zeros(1, device=torch.device(torch.cuda.current_device())),
    torch.zeros(1, device=torch.device(torch.cuda.current_device())),
]

dist.all_gather(tensor_list=outputs_list, tensor=input)
print(outputs_list)
```
```
[globin01]$ python -m torch.distributed.launch --nproc_per_node=4 ../src/allgather.py
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being
overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[tensor([0.], device='cuda:1'), tensor([1.], device='cuda:1'), tensor([2.], device='cuda:1'), tensor([3.], device='cuda:1')]
[tensor([0.], device='cuda:0'), tensor([1.], device='cuda:0'), tensor([2.], device='cuda:0'), tensor([3.], device='cuda:0')]
[tensor([0.], device='cuda:2'), tensor([1.], device='cuda:2'), tensor([2.], device='cuda:2'), tensor([3.], device='cuda:2')]
[tensor([0.], device='cuda:3'), tensor([1.], device='cuda:3'), tensor([2.], device='cuda:3'), tensor([3.], device='cuda:3')]
```
#### 7) Reduce-scatter
Reduce scatterëŠ” Reduceë¥¼ ìˆ˜í–‰í•œ ë’¤, ê²°ê³¼ë¥¼ ìª¼ê°œì„œ ë””ë°”ì´ìŠ¤ì— ë°˜í™˜í•©ë‹ˆë‹¤.
![](../images/reduce_scatter.png)
```
"""
src/reduce_scatter.py
"""

import torch
import torch.distributed as dist

dist.init_process_group("nccl")
rank = dist.get_rank()
torch.cuda.set_device(rank)

input_list = torch.tensor([1, 10, 100, 1000]).to(torch.cuda.current_device()) * rank
input_list = torch.split(input_list, dim=0, split_size_or_sections=1)
# rank==0 => [0, 00, 000, 0000]
# rank==1 => [1, 10, 100, 1000]
# rank==2 => [2, 20, 200, 2000]
# rank==3 => [3, 30, 300, 3000]

output = torch.tensor([0], device=torch.device(torch.cuda.current_device()),)

dist.reduce_scatter(
    output=output,
    input_list=list(input_list),
    op=torch.distributed.ReduceOp.SUM,
)

print(f"rank {rank}: {output}\n")
```
```
[glogin01]$ python -m torch.distributed.launch --nproc_per_node=4 ../src/reduce_scatter.py
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being
overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
rank 0: tensor([6], device='cuda:0')
rank 2: tensor([600], device='cuda:2')


rank 1: tensor([60], device='cuda:1')

rank 3: tensor([6000], device='cuda:3')
```

#### 8) Barrier
BarrierëŠ” í”„ë¡œì„¸ìŠ¤ë¥¼ ë™ê¸°í™” í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ë¨¼ì € barrierì— ë„ì°©í•œ í”„ë¡œì„¸ìŠ¤ëŠ” ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ í•´ë‹¹ ì§€ì ê¹Œì§€ ì‹¤í–‰ë˜ëŠ” ê²ƒì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
```
"""
src/barrier.py
"""
import time
import torch.distributed as dist

dist.init_process_group("nccl")
rank = dist.get_rank()

if rank == 0:
    seconds = 0
    while seconds <= 3:
        time.sleep(1)
        seconds += 1
        print(f"rank 0 - seconds: {seconds}\n")

print(f"rank {rank}: no-barrier\n")
dist.barrier()
print(f"rank {rank}: barrier\n")
```
```
[glogin01]$ python -m torch.distributed.launch --nproc_per_node=4 ../src/barrier.py
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being
overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
rank 2: no-barrier
rank 1: no-barrier
rank 3: no-barrier



rank 0 - seconds: 1

rank 0 - seconds: 2

rank 0 - seconds: 3

rank 0 - seconds: 4

rank 0: no-barrier

rank 0: barrier

rank 1: barrier

rank 3: barrier

rank 2: barrier

```
### ë„ˆë¬´ ë§ì£ ...? ğŸ˜…
ì•„ë˜ 4ê°œì˜ ê¸°ë³¸ ì—°ì‚°ë§Œ ì˜ ê¸°ì–µí•´ë‘¬ë„ ëŒ€ë¶€ë¶„ ìœ ì¶”í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
![](../images/collective.png)
4ê°€ì§€ ì—°ì‚°ì„ ê¸°ë°˜ìœ¼ë¡œ ì•„ë˜ì˜ ì‚¬í•­ë“¤ì„ ìµí˜€ë‘ì‹œë©´ ë©ë‹ˆë‹¤.
- `all-reduce`, `all-gather`ëŠ” í•´ë‹¹ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ê³  ë‚˜ì„œ `broadcast` ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ë¼ê³  ìƒê°í•˜ë©´ ë©ë‹ˆë‹¤.
- `reduce-scatter`ëŠ” ë§ ê·¸ëŒ€ë¡œ `reduce` ì—°ì‚°ì˜ ê²°ê³¼ë¥¼ `scatter (ìª¼ê°œê¸°)` ì²˜ë¦¬í•œë‹¤ê³  ìƒê°í•˜ë©´ ë©ë‹ˆë‹¤.
- `barrier`ëŠ” ì˜ì–´ ëœ» ê·¸ëŒ€ë¡œ ë²½ê³¼ ê°™ì€ ê²ƒì…ë‹ˆë‹¤. ë¨¼ì € ë„ì°©í•œ í”„ë¡œì„¸ìŠ¤ë“¤ì´ ëª» ì§€ë‚˜ê°€ê²Œ ë²½ì²˜ëŸ¼ ë§‰ì•„ë‘ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
