# Distributed Programming
Large-scale ëª¨ë¸ì€ í¬ê¸°ê°€ í¬ê¸° ë•Œë¬¸ì— ì—¬ëŸ¬ëŒ€ì˜ GPUì— ìª¼ê°œì„œ ëª¨ë¸ì„ ì˜¬ë ¤ì•¼ í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ìª¼ê°œì§„ ê° ëª¨ë¸ì˜ ì¡°ê°ë“¤ë¼ë¦¬ ë„¤íŠ¸ì›Œí¬ë¡œ í†µì‹ ì„ í•˜ë©´ì„œ ê°’ì„ ì£¼ê³  ë°›ì•„ì•¼ í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ì»¤ë‹¤ë€ ë¦¬ì†ŒìŠ¤ë¥¼ ì—¬ëŸ¬ëŒ€ì˜ ì»´í“¨í„° í˜¹ì€ ì—¬ëŸ¬ëŒ€ì˜ ì¥ë¹„ì— ë¶„ì‚°ì‹œì¼œì„œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì„ 'ë¶„ì‚°ì²˜ë¦¬'ë¼ê³  í•©ë‹ˆë‹¤. ì´ë²ˆ ì„¸ì…˜ì—ì„œëŠ” PyTorchë¥¼ ì´ìš©í•œ ë¶„ì‚° í”„ë¡œê·¸ë˜ë°ì˜ ê¸°ì´ˆì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.
  
## 1. Multi-processing with PyTorch
   
ë¶„ì‚°í”„ë¡œê·¸ë˜ë° íŠœí† ë¦¬ì–¼ì— ì•ì„œ PyTorchë¡œ êµ¬í˜„ëœ Multi-processing ì• í”Œë¦¬ì¼€ì´ì…˜ì— ëŒ€í•œ íŠœí† ë¦¬ì–¼ì„ ì§„í–‰í•©ë‹ˆë‹¤. ì“°ë ˆë“œ ë° í”„ë¡œì„¸ìŠ¤ì˜ ê°œë… ë“±ì€ Computer Scienece ì „ê³µìë¼ë©´ ìš´ì˜ì²´ì œ ì‹œê°„ì— ë°°ìš°ëŠ” ê²ƒë“¤ì´ë‹ˆ ìƒëµí•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ë§Œì•½ ì´ëŸ¬í•œ ê°œë…ì— ëŒ€í•´ ì˜ ëª¨ë¥´ì‹ ë‹¤ë©´, êµ¬ê¸€ì— ê²€ìƒ‰í•˜ì‹œê±°ë‚˜ https://www.backblaze.com/blog/whats-the-diff-programs-processes-and-threads/ ì™€ ê°™ì€ ê¸€ì„ ë¨¼ì € ì½ì–´ë³´ëŠ” ê²ƒì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.

### Multi-process í†µì‹ ì— ì“°ì´ëŠ” ê¸°ë³¸ ìš©ì–´
- Node: ì¼ë°˜ì ìœ¼ë¡œ ì»´í“¨í„°ë¼ê³  ìƒê°í•˜ì‹œë©´ ë©ë‹ˆë‹¤. ë…¸ë“œ 3ëŒ€ë¼ê³  í•˜ë©´ ì»´í“¨í„° 3ëŒ€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
- Global Rank: ì›ë˜ëŠ” í”„ë¡œì„¸ìŠ¤ì˜ ìš°ì„ ìˆœìœ„ë¥¼ ì˜ë¯¸í•˜ì§€ë§Œ **MLì—ì„œëŠ” GPUì˜ ID**ë¼ê³  ë³´ì‹œë©´ ë©ë‹ˆë‹¤.
- Local Rank: ì›ë˜ëŠ” í•œ ë…¸ë“œë‚´ì—ì„œì˜ í”„ë¡œì„¸ìŠ¤ ìš°ì„ ìˆœìœ„ë¥¼ ì˜ë¯¸í•˜ì§€ë§Œ **MLì—ì„œëŠ” ë…¸ë“œë‚´ì˜ GPU ID**ë¼ê³  ë³´ì‹œë©´ ë©ë‹ˆë‹¤
- World Size: í”„ë¡œì„¸ìŠ¤ì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
    
![](../images/process_terms.png)
    
### Multi-process Application ì‹¤í–‰ ë°©ë²•
PyTorchë¡œ êµ¬í˜„ëœ Multi-process ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‹¤í–‰ì‹œí‚¤ëŠ” ë°©ë²•ì€ í¬ê²Œ ë‘ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤.
1. ì‚¬ìš©ìì˜ ì½”ë“œê°€ ë©”ì¸í”„ë¡œì„¸ìŠ¤ê°€ ë˜ì–´ íŠ¹ì • í•¨ìˆ˜ë¥¼ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¡œ ë¶„ê¸°í•œë‹¤.
2. PyTorch ëŸ°ì²˜ê°€ ë©”ì¸í”„ë¡œì„¸ìŠ¤ê°€ ë˜ì–´ ì‚¬ìš©ì ì½”ë“œ ì „ì²´ë¥¼ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¡œ ë¶„ê¸°í•œë‹¤.
    
ì´ ë‘ê°€ì§€ ë°©ë²•ì— ëŒ€í•´ ëª¨ë‘ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. ì´ë•Œ, 'ë¶„ê¸°í•œë‹¤.'ë¼ëŠ” í‘œí˜„ì´ ë‚˜ì˜¤ëŠ”ë°, ì´ëŠ” í•œ í”„ë¡œì„¸ìŠ¤ê°€ ë¶€ëª¨ê°€ ë˜ì–´ ì—¬ëŸ¬ê°œì˜ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¥¼ ë™ì‹œì— ì‹¤í–‰ì‹œí‚¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
   
### 1) ì‚¬ìš©ìì˜ ì½”ë“œê°€ ë©”ì¸í”„ë¡œì„¸ìŠ¤ê°€ ë˜ì–´ íŠ¹ì • í•¨ìˆ˜ë¥¼ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¡œ ë¶„ê¸°í•œë‹¤.
ì´ ë°©ì‹ì€ ì‚¬ìš©ìì˜ ì½”ë“œê°€ ë©”ì¸í”„ë¡œì„¸ìŠ¤ê°€ ë˜ë©° íŠ¹ì • functionì„ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¡œì¨ ë¶„ê¸°í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

![](../images/multi_process_1.png)
    
ì¼ë°˜ì ìœ¼ë¡œ `Spawn`ê³¼ `Fork` ë“± ë‘ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¥¼ ë¶„ê¸° í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- `Spawn`
  - ë©”ì¸í”„ë¡œì„¸ìŠ¤ì˜ ìì›ì„ ë¬¼ë ¤ì£¼ì§€ ì•Šê³  í•„ìš”í•œ ë§Œí¼ì˜ ìì›ë§Œ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ì—ê²Œ ìƒˆë¡œ í• ë‹¹.
  - ì†ë„ê°€ ëŠë¦¬ì§€ë§Œ ì•ˆì „í•œ ë°©ì‹.
- `Fork`
  - ë©”ì¸í”„ë¡œì„¸ìŠ¤ì˜ ëª¨ë“  ìì›ì„ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ì™€ ê³µìœ í•˜ê³  í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘.
  - ì†ë„ê°€ ë¹ ë¥´ì§€ë§Œ ìœ„í—˜í•œ ë°©ì‹.
p.s. ì‹¤ì œë¡œëŠ” `Forkserver` ë°©ì‹ë„ ìˆì§€ë§Œ ìì£¼ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” ìƒì†Œí•œ ë°©ì‹ì´ê¸°ì— ìƒëµí•©ë‹ˆë‹¤.

```
    """
    src/multi_process_1.py\n

    ì°¸ê³ :
    Jupyter notebookì€ ë©€í‹°í”„ë¡œì„¸ì‹± ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ë™í•˜ëŠ”ë°ì— ë§ì€ ì œì•½ì´ ìˆìŠµë‹ˆë‹¤.
    ë”°ë¼ì„œ ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì´ê³³ì—ëŠ” ì½”ë“œë§Œ ë™ë´‰í•˜ê³  ì‹¤í–‰ì€ `src` í´ë”ì— ìˆëŠ” ì½”ë“œë¥¼ ë™ì‘ì‹œí‚¤ê² ìŠµë‹ˆë‹¤.
    ì‹¤ì œ ì½”ë“œ ë™ì‘ì€ `src` í´ë”ì— ìˆëŠ” ì½”ë“œë¥¼ ì‹¤í–‰ì‹œì¼œì£¼ì„¸ìš”.
    """

    import torch.multiprocessing as mp
    # ì¼ë°˜ì ìœ¼ë¡œ mpì™€ ê°™ì€ ì´ë¦„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    

    # ì„œë¸Œí”„ë¡œì„¸ìŠ¤ì—ì„œ ë™ì‹œì— ì‹¤í–‰ë˜ëŠ” ì˜ì—­
    def fn(rank, param1, param2)
        print(f\"{param1} {param2} - rank: {rank}\")
  
    # ë©”ì¸ í”„ë¡œì„¸ìŠ¤
    if __name__ == "__main__":
        processes = []
        # ì‹œì‘ ë°©ë²• ì„¤ì •
        mp.set_start_method(\"spawn\")

        for rank in range(4):
            process = mp.Process(target=fn, args=(rank, \"A0\", \"B1\"))
            # ì„œë¸Œí”„ë¡œì„¸ìŠ¤ ìƒì„±
            process.daemon = False
            # ë°ëª¬ ì—¬ë¶€ (ë©”ì¸í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œì‹œ í•¨ê»˜ ì¢…ë£Œ)
            process.start()
            # ì„œë¸Œí”„ë¡œì„¸ìŠ¤ ì‹œì‘
            processes.append(process)
    
        for process in processes:
            process.join()
            # ì„œë¸Œ í”„ë¡œì„¸ìŠ¤ join (=ì™„ë£Œë˜ë©´ ì¢…ë£Œ)
```

```
[glogin01]$ python ../src/multi_process_1.py
A0 B1 - rank: 0
A0 B1 - rank: 2
A0 B1 - rank: 3
A0 B1 - rank: 1
```
`torch.multiprocessing.spawn` í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ë©´ ì´ ê³¼ì •ì„ ë§¤ìš° ì‰½ê²Œ ì§„í–‰ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```
"""
src/multi_process_2.py
"""

import torch.multiprocessing as mp


# ì„œë¸Œí”„ë¡œì„¸ìŠ¤ì—ì„œ ë™ì‹œì— ì‹¤í–‰ë˜ëŠ” ì˜ì—­
def fn(rank, param1, param2):
    # rankëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë“¤ì–´ì˜´. param1, param2ëŠ” spawnì‹œì— ì…ë ¥ë¨.
    print(f"{param1} {param2} - rank: {rank}")


# ë©”ì¸ í”„ë¡œì„¸ìŠ¤
if __name__ == "__main__":
    mp.spawn(
        fn=fn,
        args=("A0", "B1"),
        nprocs=4,  # ë§Œë“¤ í”„ë¡œì„¸ìŠ¤ ê°œìˆ˜
        join=True,  # í”„ë¡œì„¸ìŠ¤ join ì—¬ë¶€
        daemon=False,  # ë°ëª¬ ì—¬ë¶€
        start_method="spawn",  # ì‹œì‘ ë°©ë²• ì„¤ì •
    )
```

```
[glogin01]$ python ../src/multi_process_2.py
A0 B1 - rank: 1
A0 B1 - rank: 0
A0 B1 - rank: 3
A0 B1 - rank: 2
```

```
"""
ì°¸ê³ : torch/multiprocessing/spawn.py

mp.spawn í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ì´ ë™ì‘í•©ë‹ˆë‹¤.
"""

def start_processes(fn, args=(), nprocs=1, join=True, daemon=False, start_method='spawn'):
    _python_version_check()
    mp = multiprocessing.get_context(start_method)
    error_queues = []
    processes = []
    for i in range(nprocs):
        error_queue = mp.SimpleQueue()
        process = mp.Process(
            target=_wrap,
            args=(fn, i, args, error_queue),
            daemon=daemon,
        )
        process.start()
        error_queues.append(error_queue)
        processes.append(process)

    context = ProcessContext(processes, error_queues)
    if not join:
        return context

    # Loop on join until it returns True or raises an exception.
    while not context.join():
        pass
```

### 2) PyTorch ëŸ°ì²˜ê°€ ë¶€ëª¨ í”„ë¡œì„¸ìŠ¤ê°€ ë˜ì–´ ì‚¬ìš©ì ì½”ë“œ ì „ì²´ë¥¼ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¡œ ë¶„ê¸°í•œë‹¤.
ì´ ë°©ì‹ì€ torchì— ë‚´ì¥ëœ ë©€í‹°í”„ë¡œì„¸ì‹± ëŸ°ì²˜ê°€ ì‚¬ìš©ì ì½”ë“œ ì „ì²´ë¥¼ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰ì‹œì¼œì£¼ëŠ” ë§¤ìš° í¸ë¦¬í•œ ë°©ì‹ì…ë‹ˆë‹¤.

`python -m torch.distributed.launch --nproc_per_node=n OOO.py`ì™€ ê°™ì€ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
```
"""
src/multi_process_3.py
"""

# ì½”ë“œ ì „ì²´ê°€ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ê°€ ë©ë‹ˆë‹¤.
import os

# RANK, LOCAL_RANK, WORLD_SIZE ë“±ì˜ ë³€ìˆ˜ê°€ ìë™ìœ¼ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.
print(f"hello world, {os.environ['RANK']}")
```

```
[glogin01]$ python -m torch.distributed.launch --nproc_per_node=4 ../src/multi_process_3.py
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
hello world, 0
hello world, 1
hello world, 2
hello world, 3
```
 

## 2. Distributed Programming with PyTorch
### Concept of Message Passing
ë©”ì‹œì§€ íŒ¨ì‹±ì´ë€ ë™ì¼í•œ ì£¼ì†Œê³µê°„ì„ ê³µìœ í•˜ì§€ ì•ŠëŠ” ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ë“¤ì´ ë°ì´í„°ë¥¼ ì£¼ê³  ë°›ì„ ìˆ˜ ìˆë„ë¡ ë©”ì‹œì§€ë¼ëŠ” ê°„ì ‘ ì •ë³´ë¥¼ ì£¼ê³  ë°›ëŠ” ê²ƒì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´ Process-1ì´ íŠ¹ì • íƒœê·¸ê°€ ë‹¬ë¦° ë°ì´í„°ë¥¼ ë©”ì‹œì§€ íì— sendí•˜ë„ë¡, Process-2ê°€ í•´ë‹¹ ë°ì´í„°ë¥¼ receiveí•˜ë„ë¡ ì½”ë”©í•´ë†“ìœ¼ë©´ ë‘ í”„ë¡œì„¸ìŠ¤ê°€ ê³µìœ í•˜ëŠ” ë©”ëª¨ë¦¬ ê³µê°„ ì—†ì´ë„ ë°ì´í„°ë¥¼ ì£¼ê³  ë°›ì„ ìˆ˜ ìˆì£ . Large-scale ëª¨ë¸ ê°œë°œì‹œì— ì‚¬ìš©ë˜ëŠ” ë¶„ì‚° í†µì‹ ì—ëŠ” ëŒ€ë¶€ë¶„ ì´ëŸ¬í•œ message passing ê¸°ë²•ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.
  
![](../images/message_passing.png)
   
### MPI (Massage Passing Interface)
MPIëŠ” Message Passingì— ëŒ€í•œ í‘œì¤€ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. MPIì—ëŠ” Processê°„ì˜ Message Passingì— ì‚¬ìš©ë˜ëŠ” ì—¬ëŸ¬ ì—°ì‚°(e.g. broadcast, reduce, scatter, gather, ...)ì´ ì •ì˜ë˜ì–´ ìˆìœ¼ë©° ëŒ€í‘œì ìœ¼ë¡œ OpenMPIë¼ëŠ” ì˜¤í”ˆì†ŒìŠ¤ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.

![](../images/open_mpi.png)
   
### NCCL & GLOO\n",
ì‹¤ì œë¡œëŠ” openmpi ë³´ë‹¤ëŠ” ncclì´ë‚˜ gloo ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ê²Œ ë©ë‹ˆë‹¤.
- NCCL (NVIDIA Collective Communication Library)
  - NVIDIAì—ì„œ ê°œë°œí•œ GPU íŠ¹í™” Message Passing ë¼ì´ë¸ŒëŸ¬ë¦¬ ('nickel'ì´ë¼ê³  ì½ìŒ)\n",
  - NVIDIA GPUì—ì„œ ì‚¬ìš©ì‹œ, ë‹¤ë¥¸ ë„êµ¬ì— ë¹„í•´ ì›”ë“±íˆ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ìˆìŠµë‹ˆë‹¤.\n",
- GLOO (Facebook's Collective Communication Library)\n",
  - Facebookì—ì„œ ê°œë°œëœ Message Passing ë¼ì´ë¸ŒëŸ¬ë¦¬. \n",
  - `torch`ì—ì„œëŠ” ì£¼ë¡œ CPU ë¶„ì‚°ì²˜ë¦¬ì— ì‚¬ìš©í•˜ë¼ê³  ì¶”ì²œí•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
  
### ë°±ì—”ë“œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ ê°€ì´ë“œ
openmpië¥¼ ì¨ì•¼í•  íŠ¹ë³„í•œ ì´ìœ ê°€ ìˆëŠ” ê²ƒì´ ì•„ë‹ˆë¼ë©´ ncclì´ë‚˜ glooë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, GPUì—ì„œ ì‚¬ìš©ì‹œ nccl, CPUì—ì„œ ì‚¬ìš©ì‹œ glooë¥¼ ì‚¬ìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤. ë” ìì„¸í•œ ì •ë³´ëŠ”  https://pytorch.org/docs/stable/distributed.html ì—¬ê¸°ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.
  
ê° ë°±ì—”ë“œë³„ë¡œ ìˆ˜í–‰ ê°€ëŠ¥í•œ ì—°ì‚°ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

![](../images/backends.png)
    
### `torch.distributed` íŒ¨í‚¤ì§€
`gloo`, `nccl`, `openmpi` ë“±ì„ ì§ì ‘ ì‚¬ìš©í•´ë³´ëŠ” ê²ƒì€ ë¶„ëª… ì¢‹ì€ ê²½í—˜ì´ ë  ê²ƒì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì‹œê°„ ê´€ê³„ìƒ ì´ë“¤ì„ ëª¨ë‘ ë‹¤ë£° ìˆ˜ëŠ” ì—†ê³ , ì´ë“¤ì„ wrapping í•˜ê³  ìˆëŠ” `torch.distributed` íŒ¨í‚¤ì§€ë¥¼  ì‚¬ìš©í•˜ì—¬ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤. ì‹¤ì œë¡œ í™œìš© ë‹¨ìœ¼ë¡œ ê°€ë©´ `nccl` ë“±ì„ ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•Šê³  ëŒ€ë¶€ë¶„ì˜ ê²½ìš° `torch.distributed` ë“±ì˜ í•˜ì´ë ˆë²¨ íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡œê·¸ë˜ë° í•˜ê²Œ ë©ë‹ˆë‹¤.
  
### Process Group
ë§ì€ í”„ë¡œì„¸ìŠ¤ë¥¼ ê´€ë¦¬í•˜ëŠ” ê²ƒì€ ì–´ë ¤ìš´ ì¼ì…ë‹ˆë‹¤. ë”°ë¼ì„œ í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ì„ ë§Œë“¤ì–´ì„œ ê´€ë¦¬ë¥¼ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤. `init_process_group`ë¥¼ í˜¸ì¶œí•˜ë©´ ì „ì²´ í”„ë¡œì„¸ìŠ¤ê°€ ì†í•œ default_pg(process group)ê°€ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ì„ ì´ˆê¸°í™”í•˜ëŠ” `init_process_group` í•¨ìˆ˜ëŠ” **ë°˜ë“œì‹œ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰**ë˜ì–´ì•¼ í•˜ë©°, ë§Œì•½ ì¶”ê°€ë¡œ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ë“¤ë§Œ ëª¨ì•„ì„œ ê·¸ë£¹ì„ ìƒì„±í•˜ë ¤ë©´ `new_group`ì„ í˜¸ì¶œí•˜ë©´ ë©ë‹ˆë‹¤.
```
"""
src/process_group_1.py
"""

import torch.distributed as dist
# ì¼ë°˜ì ìœ¼ë¡œ distì™€ ê°™ì€ ì´ë¦„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

dist.init_process_group(backend="nccl", rank=0, world_size=1)
# í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ì´ˆê¸°í™”
# ë³¸ ì˜ˆì œì—ì„œëŠ” ê°€ì¥ ìì£¼ ì‚¬ìš©í•˜ëŠ” ncclì„ ê¸°ë°˜ìœ¼ë¡œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.
# backendì— 'nccl' ëŒ€ì‹  'mpi'ë‚˜ 'gloo'ë¥¼ ë„£ì–´ë„ ë©ë‹ˆë‹¤.

process_group = dist.new_group([0])
# 0ë²ˆ í”„ë¡œì„¸ìŠ¤ê°€ ì†í•œ í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ìƒì„±

print(process_group)
```

```
[glogin01]$ python ../src/process_group_1.py
Traceback (most recent call last):
  File "../src/process_group_1.py", line 8, in <module>
    dist.init_process_group(backend="nccl", rank=0, world_size=1)
  File "/home/ubuntu/kevin/kevin_env/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 436, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/home/ubuntu/kevin/kevin_env/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 166, in _env_rendezvous_handler
    raise _env_error("MASTER_ADDR")
ValueError: Error initializing torch.distributed using env:// rendezvous: environment variable MASTER_ADDR expected, but not set
```
ìœ„ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” `MASTER_ADDR`, `MASTER_PORT` ë“± í•„ìš”í•œ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ ê°’ë“¤ì„ ì„¤ì •í•˜ê³  ë‹¤ì‹œ ì‹¤í–‰ì‹œí‚¤ê² ìŠµë‹ˆë‹¤.
```
"""
src/process_group_2.py
"""

import torch.distributed as dist
import os


# ì¼ë°˜ì ìœ¼ë¡œ ì´ ê°’ë“¤ë„ í™˜ê²½ë³€ìˆ˜ë¡œ ë“±ë¡í•˜ê³  ì‚¬ìš©í•©ë‹ˆë‹¤.
os.environ["RANK"] = "0"
os.environ["LOCAL_RANK"] = "0"
os.environ["WORLD_SIZE"] = "1"

# í†µì‹ ì— í•„ìš”í•œ ì£¼ì†Œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
os.environ["MASTER_ADDR"] = "localhost"  # í†µì‹ í•  ì£¼ì†Œ (ë³´í†µ localhostë¥¼ ì”ë‹ˆë‹¤.)
os.environ["MASTER_PORT"] = "29500"  # í†µì‹ í•  í¬íŠ¸ (ì„ì˜ì˜ ê°’ì„ ì„¤ì •í•´ë„ ì¢‹ìŠµë‹ˆë‹¤.)

dist.init_process_group(backend="nccl", rank=0, world_size=1)
# í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ì´ˆê¸°í™”

process_group = dist.new_group([0])
# 0ë²ˆ í”„ë¡œì„¸ìŠ¤ê°€ ì†í•œ í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ìƒì„±

print(process_group)
```

```
[globin01]$ python ../src/process_group_2.py
<torch.distributed.ProcessGroupNCCL object at 0x7fb7524254b0>
```

ìœ„ ì˜ˆì œëŠ” í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ì˜ APIë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•´ì„œ ë©”ì¸í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. (í”„ë¡œì„¸ìŠ¤ê°€ 1ê°œ ë¿ì´ë¼ ìƒê´€ì—†ì—ˆìŒ) ì‹¤ì œë¡œëŠ” ë©€í‹°í”„ë¡œì„¸ìŠ¤ ì‘ì—…ì‹œ í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ìƒì„± ë“±ì˜ ì‘ì—…ì€ ë°˜ë“œì‹œ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

```
"""
src/process_group_3.py
"""

import torch.multiprocessing as mp
import torch.distributed as dist
import os


# ì„œë¸Œí”„ë¡œì„¸ìŠ¤ì—ì„œ ë™ì‹œì— ì‹¤í–‰ë˜ëŠ” ì˜ì—­
def fn(rank, world_size):
    # rankëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë“¤ì–´ì˜´. world_sizeëŠ” ì…ë ¥ë¨.
    dist.init_process_group(backend="nccl", rank=rank, world_size=world_size)
    group = dist.new_group([_ for _ in range(world_size)])
    print(f"{group} - rank: {rank}")


# ë©”ì¸ í”„ë¡œì„¸ìŠ¤
if __name__ == "__main__":
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "29500"
    os.environ["WORLD_SIZE"] = "4"

    mp.spawn(
        fn=fn,
        args=(4,),  # world_size ì…ë ¥
        nprocs=4,  # ë§Œë“¤ í”„ë¡œì„¸ìŠ¤ ê°œìˆ˜
        join=True,  # í”„ë¡œì„¸ìŠ¤ join ì—¬ë¶€
        daemon=False,  # ë°ëª¬ ì—¬ë¶€
        start_method="spawn",  # ì‹œì‘ ë°©ë²• ì„¤ì •
    )
```

```
[glogin01]$ python ../src/process_group_3.py
<torch.distributed.ProcessGroupNCCL object at 0x7f409fffd270> - rank: 2
<torch.distributed.ProcessGroupNCCL object at 0x7fe7db7d5230> - rank: 3
<torch.distributed.ProcessGroupNCCL object at 0x7fe6f02d6130> - rank: 1
<torch.distributed.ProcessGroupNCCL object at 0x7f7f67c77b70> - rank: 0
```
python -m torch.distributed.launch --nproc_per_node=n OOO.pyë¥¼ ì‚¬ìš©í• ë•ŒëŠ” ì•„ë˜ì™€ ê°™ì´ ì²˜ë¦¬í•©ë‹ˆë‹¤. dist.get_rank(), dist_get_world_size()ì™€ ê°™ì€ í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ rankì™€ world_sizeë¥¼ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```
"""
src/process_group_4.py
"""

import torch.distributed as dist

dist.init_process_group(backend="nccl")
# í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ì´ˆê¸°í™”

group = dist.new_group([_ for _ in range(dist.get_world_size())])
# í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ìƒì„±

print(f"{group} - rank: {dist.get_rank()}\n")
```
```
[glogin01]$ python -m torch.distributed.launch --nproc_per_node=4 ../src/process_group_4.py
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
<torch.distributed.ProcessGroupNCCL object at 0x7fd3059bc4f0> - rank: 1
<torch.distributed.ProcessGroupNCCL object at 0x7f051d38e470> - rank: 3
<torch.distributed.ProcessGroupNCCL object at 0x7fd233c31430> - rank: 2
<torch.distributed.ProcessGroupNCCL object at 0x7f0f34a853f0> - rank: 0
```

### P2P Communication (Point to point)
![](../images/p2p.png)
P2P (Point to point, ì  ëŒ€ ì ) í†µì‹ ì€ íŠ¹ì • í”„ë¡œì„¸ìŠ¤ì—ì„œ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ ë°ì´í„°ë¥¼ ì „ì†¡í•˜ëŠ” í†µì‹ ì´ë©° `torch.distributed` íŒ¨í‚¤ì§€ì˜ `send`, `recv` í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ í†µì‹ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```
"""
src/p2p_communication.py
"""

import torch
import torch.distributed as dist

dist.init_process_group("gloo")
# í˜„ì¬ ncclì€ send, recvë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (2021/10/21)

if dist.get_rank() == 0:
    tensor = torch.randn(2, 2)
    dist.send(tensor, dst=1)

elif dist.get_rank() == 1:
    tensor = torch.zeros(2, 2)
    print(f"rank 1 before: {tensor}\n")
    dist.recv(tensor, src=0)
    print(f"rank 1 after: {tensor}\n")

else:
    raise RuntimeError("wrong rank")
```

```
[glogin01]$ python -m torch.distributed.launch --nproc_per_node=2 ../src/p2p_communication.py
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
rank 1 before: tensor([[0., 0.],
        [0., 0.]])

rank 1 after: tensor([[-0.6160, -0.0912],
        [ 1.0645,  2.5397]])
```
ì£¼ì˜í•  ê²ƒì€ ì´ë“¤ì´ ë™ê¸°ì ìœ¼ë¡œ í†µì‹ í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ë¹„ë™ê¸° í†µì‹ (non-blocking)ì—ëŠ” `isend`, `irecv`ë¥¼ ì´ìš©í•©ë‹ˆë‹¤. ì´ë“¤ì€ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‘ë™í•˜ê¸° ë•Œë¬¸ì— `wait()` ë©”ì„œë“œë¥¼ í†µí•´ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ì˜ í†µì‹ ì´ ëë‚ ë•Œ ê¹Œì§€ ê¸°ë‹¤ë¦¬ê³  ë‚œ ë’¤ì— ì ‘ê·¼í•´ì•¼í•©ë‹ˆë‹¤.


```
"""
src/p2p_communication_non_blocking.py
"""

import torch
import torch.distributed as dist

dist.init_process_group("gloo")
# í˜„ì¬ ncclì€ send, recvë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (2021/10/21)

if dist.get_rank() == 0:
    tensor = torch.randn(2, 2)
    request = dist.isend(tensor, dst=1)
elif dist.get_rank() == 1:
    tensor = torch.zeros(2, 2)
    request = dist.irecv(tensor, src=0)
else:
    raise RuntimeError("wrong rank")

request.wait()

print(f"rank {dist.get_rank()}: {tensor}")
```

```
[globin01]$ python -m torch.distributed.launch --nproc_per_node=2 ../src/p2p_communication_non_blocking.py
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
rank 1: tensor([[-0.7049,  0.8836],
        [-0.4996,  0.4550]])
rank 0: tensor([[-0.7049,  0.8836],
        [-0.4996,  0.4550]])

```

### Collective Communication
Collective Communicationì€ ì§‘í•©í†µì‹ ì´ë¼ëŠ” ëœ»ìœ¼ë¡œ ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ê°€ ì°¸ì—¬í•˜ì—¬ í†µì‹ í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì—°ì‚°ë“¤ì´ ìˆì§€ë§Œ ê¸°ë³¸ì ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì€ 4ê°œì˜ ì—°ì‚°(`broadcast`, `scatter`, `gather`, `reduce`)ì´ ê¸°ë³¸ ì„¸íŠ¸ì…ë‹ˆë‹¤.
   
![](../images/collective.png)
  
ì—¬ê¸°ì— ì¶”ê°€ë¡œ `all-reduce`, `all-gather`, `reduce-scatter` ë“±ì˜ ë³µí•© ì—°ì‚°ê³¼ ë™ê¸°í™” ì—°ì‚°ì¸ `barrier`ê¹Œì§€ ì´ 8ê°œ ì—°ì‚°ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. ì¶”ê°€ë¡œ ë§Œì•½ ì´ëŸ¬í•œ ì—°ì‚°ë“¤ì„ ë¹„ë™ê¸° ëª¨ë“œë¡œ ì‹¤í–‰í•˜ë ¤ë©´ ê° ì—°ì‚° ìˆ˜í–‰ì‹œ `async_op` íŒŒë¼ë¯¸í„°ë¥¼ `True`ë¡œ ì„¤ì •í•˜ë©´ ë©ë‹ˆë‹¤.
   
#### 1) Broadcast
BroadcastëŠ” íŠ¹ì • í”„ë¡œì„¸ìŠ¤ì— ìˆëŠ” ë°ì´í„°ë¥¼ ê·¸ë£¹ë‚´ì˜ ëª¨ë“  í”„ë¡œì„¸ìŠ¤ì— ë³µì‚¬í•˜ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤.
 
![](../images/broadcast.png)

```
"""
src/broadcast.py
"""

import torch
import torch.distributed as dist

dist.init_process_group("nccl")
rank = dist.get_rank()
torch.cuda.set_device(rank)
# deviceë¥¼ settingí•˜ë©´ ì´í›„ì— rankì— ë§ëŠ” ë””ë°”ì´ìŠ¤ì— ì ‘ê·¼ ê°€ëŠ¥í•©ë‹ˆë‹¤.

if rank == 0:
    tensor = torch.randn(2, 2).to(torch.cuda.current_device())
else:
    tensor = torch.zeros(2, 2).to(torch.cuda.current_device())

print(f"before rank {rank}: {tensor}\n")
dist.broadcast(tensor, src=0)
print(f"after rank {rank}: {tensor}\n")
```
```
[glogin01]$ python -m torch.distributed.launch --nproc_per_node=4 ../src/broadcast.py
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
before rank 3: tensor([[0., 0.],
        [0., 0.]], device='cuda:3')
before rank 1: tensor([[0., 0.],
        [0., 0.]], device='cuda:1')


before rank 2: tensor([[0., 0.],
        [0., 0.]], device='cuda:2')

before rank 0: tensor([[-0.7522, -0.2532],
        [ 0.9788,  1.0834]], device='cuda:0')

after rank 0: tensor([[-0.7522, -0.2532],
        [ 0.9788,  1.0834]], device='cuda:0')

after rank 1: tensor([[-0.7522, -0.2532],
        [ 0.9788,  1.0834]], device='cuda:1')

after rank 3: tensor([[-0.7522, -0.2532],
        [ 0.9788,  1.0834]], device='cuda:3')

after rank 2: tensor([[-0.7522, -0.2532],
        [ 0.9788,  1.0834]], device='cuda:2')
```
`send`, `recv` ë“±ì˜ P2P ì—°ì‚°ì´ ì§€ì›ë˜ì§€ ì•Šì„ë•Œ ì•Šì•„ì„œ `broadcast`ë¥¼ P2P í†µì‹  ìš©ë„ë¡œ ì‚¬ìš©í•˜ê¸°ë„ í•©ë‹ˆë‹¤. src=0, dst=1 ì¼ë•Œ, `new_group([0, 1])` ê·¸ë£¹ì„ ë§Œë“¤ê³  `broadcast`ë¥¼ ìˆ˜í–‰í•˜ë©´ 0 -> 1 P2Pì™€ ë™ì¼í•©ë‹ˆë‹¤.

```
"""
ì°¸ê³ : deepspeed/deepspeed/runtime/pipe/p2p.py
"""

def send(tensor, dest_stage, async_op=False):
    global _groups
    assert async_op == False, "Doesnt support async_op true"
    src_stage = _grid.get_stage_id()
    _is_valid_send_recv(src_stage, dest_stage)

    dest_rank = _grid.stage_to_global(stage_id=dest_stage)
    if async_op:
        global _async
        op = dist.isend(tensor, dest_rank)
        _async.append(op)
    else:

        if can_send_recv():
            return dist.send(tensor, dest_rank)
        else:
            group = _get_send_recv_group(src_stage, dest_stage)
            src_rank = _grid.stage_to_global(stage_id=src_stage)
            return dist.broadcast(tensor, src_rank, group=group, async_op=async_op)
```
#### 2) Reduce
ReduceëŠ” ê° í”„ë¡œì„¸ìŠ¤ê°€ ê°€ì§„ ë°ì´í„°ë¡œ íŠ¹ì • ì—°ì‚°ì„ ìˆ˜í–‰í•´ì„œ ì¶œë ¥ì„ í•˜ë‚˜ì˜ ë””ë°”ì´ìŠ¤ë¡œ ëª¨ì•„ì£¼ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤. ì—°ì‚°ì€ ì£¼ë¡œ sum, max, min ë“±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
![](../images/reduce.png)"
```
"""
src/reduce_sum.py
"""

import torch
import torch.distributed as dist

dist.init_process_group("nccl")
rank = dist.get_rank()
torch.cuda.set_device(rank)

tensor = torch.ones(2, 2).to(torch.cuda.current_device()) * rank
# rank==0 => [[0, 0], [0, 0]]
# rank==1 => [[1, 1], [1, 1]]
# rank==2 => [[2, 2], [2, 2]]
# rank==3 => [[3, 3], [3, 3]]

dist.reduce(tensor, op=torch.distributed.ReduceOp.SUM, dst=0)

if rank == 0:
    print(tensor)
```
```
[glogin01]$ python -m torch.distributed.launch --nproc_per_node=4 ../src/reduce_sum.py
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
tensor([[6., 6.],
        [6., 6.]], device='cuda:0')
```

```
"""
src/reduce_max.py
"""

import torch
import torch.distributed as dist

dist.init_process_group("nccl")
rank = dist.get_rank()
torch.cuda.set_device(rank)

tensor = torch.ones(2, 2).to(torch.cuda.current_device()) * rank
# rank==0 => [[0, 0], [0, 0]]
# rank==1 => [[1, 1], [1, 1]]
# rank==2 => [[2, 2], [2, 2]]
# rank==3 => [[3, 3], [3, 3]]

dist.reduce(tensor, op=torch.distributed.ReduceOp.MAX, dst=0)

if rank == 0:
    print(tensor)
```
```
[glogin01]$ python -m torch.distributed.launch --nproc_per_node=4 ../src/reduce_max.py
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
tensor([[3., 3.],
        [3., 3.]], device='cuda:0')
3) Scatter
```
 
  
    "#### 3) Scatter\n",e",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "after rank 2: tensor([30.], device='cuda:2')\n",
      "\n",
      "after rank 3: tensor([40.], device='cuda:3')\n",
      "\n",
      "after rank 0: tensor([10.], device='cuda:0')\n",
      "\n",
      "after rank 1: tensor([20.], device='cuda:1')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=4 ../src/scatter_nccl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ì°¸ê³ : megatron-lm/megatron/mpu/mappings.py\n",
    "\"\"\"\n",
    "\n",
    "def _split(input_):\n",
    "    \"\"\"Split the tensor along its last dimension and keep the\n",
    "    corresponding slice.\"\"\"\n",
    "\n",
    "    world_size = get_tensor_model_parallel_world_size()\n",
    "    # Bypass the function if we are using only 1 GPU.\n",
    "    if world_size==1:\n",
    "        return input_\n",
    "\n",
    "    # Split along last dimension.\n",
    "    input_list = split_tensor_along_last_dim(input_, world_size)\n",
    "\n",
    "    # Note: torch.split does not create contiguous tensors by default.\n",
    "    rank = get_tensor_model_parallel_rank()\n",
    "    output = input_list[rank].contiguous()\n",
    "\n",
    "    return output\n",
    "\n",
    "class _ScatterToModelParallelRegion(torch.autograd.Function):\n",
    "    \"\"\"Split the input and keep only the corresponding chuck to the rank.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def symbolic(graph, input_):\n",
    "        return _split(input_)\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_):\n",
    "        return _split(input_)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return _gather(grad_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 4) Gather\n",
    "GatherëŠ” ì—¬ëŸ¬ ë””ë°”ì´ìŠ¤ì— ì¡´ì¬í•˜ëŠ” í…ì„œë¥¼ í•˜ë‚˜ë¡œ ëª¨ì•„ì£¼ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤.\n",
    "![](../images/gather.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/gather.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"gloo\")\n",
    "# ncclì€ gatherë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "\n",
    "input = torch.ones(1) * rank\n",
    "# rank==0 => [0]\n",
    "# rank==1 => [1]\n",
    "# rank==2 => [2]\n",
    "# rank==3 => [3]\n",
    "\n",
    "if rank == 0:\n",
    "    outputs_list = [torch.zeros(1), torch.zeros(1), torch.zeros(1), torch.zeros(1)]\n",
    "    dist.gather(input, gather_list=outputs_list, dst=0)\n",
    "    print(outputs_list)\n",
    "else:\n",
    "    dist.gather(input, dst=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "[tensor([0.]), tensor([1.]), tensor([2.]), tensor([3.])]\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=4 ../src/gather.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 5) All-reduce\n",
    "ì´ë¦„ ì•ì— All- ì´ ë¶™ì€ ì—°ì‚°ë“¤ì€ í•´ë‹¹ ì—°ì‚°ì„ ìˆ˜í–‰ í•œë’¤, ê²°ê³¼ë¥¼ ëª¨ë“  ë””ë°”ì´ìŠ¤ë¡œ broadcastí•˜ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ All-reduceëŠ” reduceë¥¼ ìˆ˜í–‰í•œ ë’¤, ê³„ì‚°ëœ ê²°ê³¼ë¥¼ ëª¨ë“  ë””ë°”ì´ìŠ¤ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤.\n",
    "![](../images/allreduce.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/allreduce_sum.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "\n",
    "tensor = torch.ones(2, 2).to(torch.cuda.current_device()) * rank\n",
    "# rank==0 => [[0, 0], [0, 0]]\n",
    "# rank==1 => [[1, 1], [1, 1]]\n",
    "# rank==2 => [[2, 2], [2, 2]]\n",
    "# rank==3 => [[3, 3], [3, 3]]\n",
    "\n",
    "dist.all_reduce(tensor, op=torch.distributed.ReduceOp.SUM)\n",
    "\n",
    "print(f\"rank {rank}: {tensor}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "rank 1: tensor([[6., 6.],\n",
      "        [6., 6.]], device='cuda:1')\n",
      "\n",
      "rank 2: tensor([[6., 6.],\n",
      "        [6., 6.]], device='cuda:2')\n",
      "rank 0: tensor([[6., 6.],\n",
      "        [6., 6.]], device='cuda:0')\n",
      "\n",
      "\n",
      "rank 3: tensor([[6., 6.],\n",
      "        [6., 6.]], device='cuda:3')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=4 ../src/allreduce_sum.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/allreduce_max.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "\n",
    "tensor = torch.ones(2, 2).to(torch.cuda.current_device()) * rank\n",
    "# rank==0 => [[0, 0], [0, 0]]\n",
    "# rank==1 => [[1, 1], [1, 1]]\n",
    "# rank==2 => [[2, 2], [2, 2]]\n",
    "# rank==3 => [[3, 3], [3, 3]]\n",
    "\n",
    "dist.all_reduce(tensor, op=torch.distributed.ReduceOp.MAX)\n",
    "\n",
    "print(f\"rank {rank}: {tensor}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "rank 3: tensor([[3., 3.],\n",
      "        [3., 3.]], device='cuda:3')\n",
      "\n",
      "rank 1: tensor([[3., 3.],\n",
      "        [3., 3.]], device='cuda:1')\n",
      "\n",
      "rank 2: tensor([[3., 3.],\n",
      "        [3., 3.]], device='cuda:2')\n",
      "\n",
      "rank 0: tensor([[3., 3.],\n",
      "        [3., 3.]], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=4 ../src/allreduce_max.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) All-gather\n",
    "All-gatherëŠ” gatherë¥¼ ìˆ˜í–‰í•œ ë’¤, ëª¨ì•„ì§„ ê²°ê³¼ë¥¼ ëª¨ë“  ë””ë°”ì´ìŠ¤ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "![](../images/allgather.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/allgather.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "\n",
    "input = torch.ones(1).to(torch.cuda.current_device()) * rank\n",
    "# rank==0 => [0]\n",
    "# rank==1 => [1]\n",
    "# rank==2 => [2]\n",
    "# rank==3 => [3]\n",
    "\n",
    "outputs_list = [\n",
    "    torch.zeros(1, device=torch.device(torch.cuda.current_device())),\n",
    "    torch.zeros(1, device=torch.device(torch.cuda.current_device())),\n",
    "    torch.zeros(1, device=torch.device(torch.cuda.current_device())),\n",
    "    torch.zeros(1, device=torch.device(torch.cuda.current_device())),\n",
    "]\n",
    "\n",
    "dist.all_gather(tensor_list=outputs_list, tensor=input)\n",
    "print(outputs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "[tensor([0.], device='cuda:1'), tensor([1.], device='cuda:1'), tensor([2.], device='cuda:1'), tensor([3.], device='cuda:1')]\n",
      "[tensor([0.], device='cuda:0'), tensor([1.], device='cuda:0'), tensor([2.], device='cuda:0'), tensor([3.], device='cuda:0')]\n",
      "[tensor([0.], device='cuda:2'), tensor([1.], device='cuda:2'), tensor([2.], device='cuda:2'), tensor([3.], device='cuda:2')]\n",
      "[tensor([0.], device='cuda:3'), tensor([1.], device='cuda:3'), tensor([2.], device='cuda:3'), tensor([3.], device='cuda:3')]\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=4 ../src/allgather.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) Reduce-scatter\n",
    "Reduce scatterëŠ” Reduceë¥¼ ìˆ˜í–‰í•œ ë’¤, ê²°ê³¼ë¥¼ ìª¼ê°œì„œ ë””ë°”ì´ìŠ¤ì— ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "![](../images/reduce_scatter.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/reduce_scatter.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "torch.cuda.set_device(rank)\n",
    "\n",
    "input_list = torch.tensor([1, 10, 100, 1000]).to(torch.cuda.current_device()) * rank\n",
    "input_list = torch.split(input_list, dim=0, split_size_or_sections=1)\n",
    "# rank==0 => [0, 00, 000, 0000]\n",
    "# rank==1 => [1, 10, 100, 1000]\n",
    "# rank==2 => [2, 20, 200, 2000]\n",
    "# rank==3 => [3, 30, 300, 3000]\n",
    "\n",
    "output = torch.tensor([0], device=torch.device(torch.cuda.current_device()),)\n",
    "\n",
    "dist.reduce_scatter(\n",
    "    output=output,\n",
    "    input_list=list(input_list),\n",
    "    op=torch.distributed.ReduceOp.SUM,\n",
    ")\n",
    "\n",
    "print(f\"rank {rank}: {output}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "rank 0: tensor([6], device='cuda:0')\n",
      "rank 2: tensor([600], device='cuda:2')\n",
      "\n",
      "\n",
      "rank 1: tensor([60], device='cuda:1')\n",
      "\n",
      "rank 3: tensor([6000], device='cuda:3')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=4 ../src/reduce_scatter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8) Barrier\n",
    "BarrierëŠ” í”„ë¡œì„¸ìŠ¤ë¥¼ ë™ê¸°í™” í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ë¨¼ì € barrierì— ë„ì°©í•œ í”„ë¡œì„¸ìŠ¤ëŠ” ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ í•´ë‹¹ ì§€ì ê¹Œì§€ ì‹¤í–‰ë˜ëŠ” ê²ƒì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/barrier.py\n",
    "\"\"\"\n",
    "import time\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "rank = dist.get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    seconds = 0\n",
    "    while seconds <= 3:\n",
    "        time.sleep(1)\n",
    "        seconds += 1\n",
    "        print(f\"rank 0 - seconds: {seconds}\\n\")\n",
    "\n",
    "print(f\"rank {rank}: no-barrier\\n\")\n",
    "dist.barrier()\n",
    "print(f\"rank {rank}: barrier\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "rank 2: no-barrier\n",
      "rank 1: no-barrier\n",
      "rank 3: no-barrier\n",
      "\n",
      "\n",
      "\n",
      "rank 0 - seconds: 1\n",
      "\n",
      "rank 0 - seconds: 2\n",
      "\n",
      "rank 0 - seconds: 3\n",
      "\n",
      "rank 0 - seconds: 4\n",
      "\n",
      "rank 0: no-barrier\n",
      "\n",
      "rank 0: barrier\n",
      "\n",
      "rank 1: barrier\n",
      "\n",
      "rank 3: barrier\n",
      "\n",
      "rank 2: barrier\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=4 ../src/barrier.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë„ˆë¬´ ë§ì£ ...? ğŸ˜…\n",
    "ì•„ë˜ 4ê°œì˜ ê¸°ë³¸ ì—°ì‚°ë§Œ ì˜ ê¸°ì–µí•´ë‘¬ë„ ëŒ€ë¶€ë¶„ ìœ ì¶”í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "![](../images/collective.png)\n",
    "\n",
    "4ê°€ì§€ ì—°ì‚°ì„ ê¸°ë°˜ìœ¼ë¡œ ì•„ë˜ì˜ ì‚¬í•­ë“¤ì„ ìµí˜€ë‘ì‹œë©´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "- `all-reduce`, `all-gather`ëŠ” í•´ë‹¹ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ê³  ë‚˜ì„œ `broadcast` ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ë¼ê³  ìƒê°í•˜ë©´ ë©ë‹ˆë‹¤.\n",
    "- `reduce-scatter`ëŠ” ë§ ê·¸ëŒ€ë¡œ `reduce` ì—°ì‚°ì˜ ê²°ê³¼ë¥¼ `scatter (ìª¼ê°œê¸°)` ì²˜ë¦¬í•œë‹¤ê³  ìƒê°í•˜ë©´ ë©ë‹ˆë‹¤.\n",
    "- `barrier`ëŠ” ì˜ì–´ ëœ» ê·¸ëŒ€ë¡œ ë²½ê³¼ ê°™ì€ ê²ƒì…ë‹ˆë‹¤. ë¨¼ì € ë„ì°©í•œ í”„ë¡œì„¸ìŠ¤ë“¤ì´ ëª» ì§€ë‚˜ê°€ê²Œ ë²½ì²˜ëŸ¼ ë§‰ì•„ë‘ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
