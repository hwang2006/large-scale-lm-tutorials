# Pipeline Parallelism
ì´ë²ˆ ì„¸ì…˜ì—ì„œëŠ” íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™”ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.
   
## 1. Inter-layer model parallelism
íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™”ëŠ” Inter-layer ëª¨ë¸ ë³‘ë ¬í™”ë¥¼ ê°œì„ í•œ ê²ƒì…ë‹ˆë‹¤. Inter-layer ëª¨ë¸ ë³‘ë ¬í™”ëŠ” ì•„ë˜ì™€ ê°™ì´ íŠ¹ì • GPUì— íŠ¹ì • ë ˆì´ì–´ë“¤ì„ í• ë‹¹í•˜ëŠ” ëª¨ë¸ ë³‘ë ¬í™” ë°©ë²•ì´ì˜€ì£ . ì•„ë˜ ê·¸ë¦¼ì—ì„œëŠ” GPU1ë²ˆì— 1,2,3ë²ˆ ë ˆì´ì–´ê°€ í• ë‹¹ë˜ì—ˆê³ , GPU2ë²ˆì— 4,5ë²ˆ ë ˆì´ì–´ê°€ í• ë‹¹ ë˜ì—ˆëŠ”ë°, ì´ ë•Œ ìª¼ê°œì§„ í•˜ë‚˜ì˜ ì¡°ê°ì„ `stage(ìŠ¤í…Œì´ì§€)`ë¼ê³  í•©ë‹ˆë‹¤. ì•„ë˜ ì˜ˆì‹œì˜ ê²½ìš° 2ê°œì˜ ìŠ¤í…Œì´ì§€ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.
  
![](../images/inter_layer.png)
   
ê·¸ëŸ¬ë‚˜ ì´ì „ ë ˆì´ì–´ì˜ ì¶œë ¥ì„ ë‹¤ìŒ ë ˆì´ì–´ì˜ ì…ë ¥ìœ¼ë¡œ í•˜ëŠ” ì‹ ê²½ë§ì˜ íŠ¹ì„±ìƒ íŠ¹ì • GPUì˜ ì—°ì‚°ì´ ëë‚˜ì•¼ ë‹¤ë¥¸ GPUê°€ ì—°ì‚°ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¦‰, ì•„ë˜ì˜ ê·¸ë¦¼ì²˜ëŸ¼ Inter-layer ëª¨ë¸ ë³‘ë ¬í™”ëŠ” ë™ì‹œì— í•˜ë‚˜ì˜ GPUë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ì¹˜ëª…ì ì¸ í•œê³„ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
    
![](../images/inter_layer_2.png)
![](../images/inter_layer_3.gif)

## 2. GPipe
GPipeëŠ” Googleì—ì„œ ê°œë°œëœ íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™” ê¸°ë²•ìœ¼ë¡œ Inter Layer ëª¨ë¸ ë³‘ë ¬í™” ì‹œ GPUê°€ ì‰¬ëŠ” ì‹œê°„ (idle time)ì„ ì¤„ì´ê¸° ìœ„í•´ ë“±ì¥í–ˆìœ¼ë©°, mini-batchë¥¼ micro-batchë¡œ í•œë²ˆ ë” ìª¼ê°œì„œ í•™ìŠµ ê³¼ì •ì„ íŒŒì´í”„ë¼ì´ë‹ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.
    
![](../images/gpipe_1.png)

![](../images/pipeline_parallelism2.png)
 
### Micro-batch
- Mini-batchëŠ” ì „ì²´ ë°ì´í„°ì…‹ì„ nê°œë¡œ ë¶„í• í•œ ì„œë¸Œìƒ˜í”Œ ì§‘í•©ì…ë‹ˆë‹¤.
- Micro-batchëŠ” Mini-batchë¥¼ mê°œë¡œ í•œë²ˆ ë” ë¶„í• í•œ ì„œë¸Œìƒ˜í”Œ ì§‘í•©ì…ë‹ˆë‹¤.
    
![](../images/gpipe_2.png)
 
### Pipelining
GPipeëŠ” ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ë§ˆì´í¬ë¡œ ë°°ì¹˜ë¡œ ìª¼ê°œê³  ì—°ì‚°ì„ íŒŒì´í”„ë¼ì´ë‹ í•©ë‹ˆë‹¤. ë¶‰ì€ìƒ‰ (GPUê°€ ì‰¬ëŠ” ë¶€ë¶„)ì„ Bubble timeì´ë¼ê³  í•˜ëŠ”ë°, Micro batch ì‚¬ì´ì¦ˆê°€ ì»¤ì§ˆ ìˆ˜ë¡ Bubble timeì´ ì¤„ì–´ë“œëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
   
![](../images/gpipe_3.gif)
 
### GPipe with PyTorch
kakaobrainì—ì„œ ê³µê°œí•œ `torchgpipe`ë¥¼ ì‚¬ìš©í•˜ë©´ ì†ì‰½ê²Œ GPipeë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¨, `nn.Sequential`ë¡œ ë˜í•‘ëœ ëª¨ë¸ë§Œ ì‚¬ìš© ê°€ëŠ¥í•˜ë©° ëª¨ë“  ëª¨ë“ˆì˜ ì…ë ¥ê³¼ ì¶œë ¥ íƒ€ì…ì€ `torch.Tensor` í˜¹ì€ `Tuple[torch.Tensor]`ë¡œ ì œí•œë©ë‹ˆë‹¤. ë”°ë¼ì„œ ì½”ë”©í•˜ê¸°ê°€ ìƒë‹¹íˆ ê¹Œë‹¤ë¡­ìŠµë‹ˆë‹¤.
```
"""
src/ch5/gpipe.org.py
"""

import torch
import torch.nn as nn
from datasets import load_dataset
from torch.optim import Adam
from torch.utils.data import DataLoader
from torchgpipe import GPipe
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from transformers.models.gpt2.modeling_gpt2 import GPT2Block as GPT2BlockBase


class GPT2Preprocessing(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.embed_dim = config.hidden_size
        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)
        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)
        self.drop = nn.Dropout(config.embd_pdrop)

    def forward(self, input_ids):
        input_shape = input_ids.size()
        input_ids = input_ids.view(-1, input_shape[-1])
        position_ids = torch.arange(
            0, input_shape[-1], dtype=torch.long, device=input_ids.device
        )
        position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])
        inputs_embeds = self.wte(input_ids)
        position_embeds = self.wpe(position_ids)
        hidden_states = inputs_embeds + position_embeds
        hidden_states = self.drop(hidden_states)
        return hidden_states


class GPT2Block(GPT2BlockBase):
    def forward(self, hidden_states):
        hidden_states = super(GPT2Block, self).forward(
            hidden_states=hidden_states,
        )
        return hidden_states[0]


class GPT2Postprocessing(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.ln_f = nn.LayerNorm(
            config.hidden_size,
            eps=config.layer_norm_epsilon,
        )
        self.lm_head = nn.Linear(
            config.hidden_size,
            config.vocab_size,
            bias=False,
        )

    def forward(self, hidden_states):
        hidden_states = self.ln_f(hidden_states)
        lm_logits = self.lm_head(hidden_states)
        return lm_logits


def create_model_from_pretrained(model_name):
    pretrained = GPT2LMHeadModel.from_pretrained(model_name)
    preprocess = GPT2Preprocessing(pretrained.config)
    preprocess.wte.weight = pretrained.transformer.wte.weight
    preprocess.wpe.weight = pretrained.transformer.wpe.weight

    blocks = pretrained.transformer.h
    for i, block in enumerate(blocks):
        block.__class__ = GPT2Block

    postprocess = GPT2Postprocessing(pretrained.config)
    postprocess.ln_f.weight = pretrained.transformer.ln_f.weight
    postprocess.ln_f.bias = pretrained.transformer.ln_f.bias
    postprocess.lm_head.weight.data = pretrained.lm_head.weight.data.clone()

    return nn.Sequential(preprocess, *blocks, postprocess)


if __name__ == "__main__":
    world_size = 4

    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token

    model = create_model_from_pretrained(model_name="gpt2")
    model = GPipe(
        model,
        balance=[4, 3, 3, 4],
        devices=[0, 1, 2, 3],
        chunks=world_size,
    )

    datasets = load_dataset("squad").data["train"]["context"]
    datasets = [str(sample) for sample in datasets]
    data_loader = DataLoader(datasets, batch_size=8, num_workers=8)

    optimizer = Adam(model.parameters(), lr=3e-5)
    loss_fn = nn.CrossEntropyLoss()

    for i, data in enumerate(data_loader):
        optimizer.zero_grad()
        tokens = tokenizer(data, return_tensors="pt", truncation=True, padding=True)
        input_ids = tokens.input_ids.to(0)
        labels = tokens.input_ids.to(world_size - 1)

        lm_logits = model(input_ids)
        shift_logits = lm_logits[..., :-1, :].contiguous() # ë§ˆì§€ë§‰ í† í° ì œì™¸, ë§ˆì§€ë§‰ì—ì„œ ì²«ë²ˆì§¸ ê¹Œì§€
        shift_labels = labels[..., 1:].contiguous() # ì²« í† í° ì œì™¸, ë‘ë²ˆì§¸ í† í°ë¶€í„° ë§ˆì§€ë§‰ í† í°ê¹Œì§€
        loss = nn.CrossEntropyLoss()(
            shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)
        )
        loss.backward()
        optimizer.step()

        if i % 10 == 0:
            print(f"step: {i}, loss: {loss}")
        if i == 300:
            break
```
```
(large-scale-lm) [gpu05]$ pip install torchgpipe
```
```
# (large-scale-lm) [gpu05]$ python -m torch.distributed.launch --nproc_per_node=4 gpipe.org.py
# (large-scale-lm) [gpu05]$ torchrun --nproc_per_node=4 gpipe.org.py
# (large-scale-lm) [gpu05]$ srun torchrun --nproc_per_node=4 gpipe.org.py
(large-scale-lm) [gpu05]$ python gpipe.org.py
/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/torchgpipe/stream.py:99: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  tensor = tensor.new_empty([0]).set_(tensor.storage())
step: 0, loss: 6.082091331481934
step: 10, loss: 3.249594211578369
step: 20, loss: 2.804326295852661
step: 30, loss: 2.5457987785339355
step: 40, loss: 2.852287530899048
step: 50, loss: 2.3473057746887207
step: 60, loss: 2.5315937995910645
step: 70, loss: 2.2457172870635986
step: 80, loss: 2.4787349700927734
step: 90, loss: 2.935778856277466
step: 100, loss: 2.8190064430236816
step: 110, loss: 2.4800403118133545
step: 120, loss: 2.951277494430542
step: 130, loss: 2.404855728149414
step: 140, loss: 2.9455151557922363
step: 150, loss: 3.9279768466949463
step: 160, loss: 3.024629592895508
step: 170, loss: 3.0173544883728027
step: 180, loss: 1.6787645816802979
step: 190, loss: 3.5459213256835938
step: 200, loss: 3.663504123687744
step: 210, loss: 3.522679328918457
step: 220, loss: 2.9959702491760254
step: 230, loss: 3.183525562286377
step: 240, loss: 2.5458672046661377
step: 250, loss: 3.148806571960449
step: 260, loss: 3.4613168239593506
step: 270, loss: 3.1852593421936035
step: 280, loss: 2.616974115371704
step: 290, loss: 2.1446080207824707
step: 300, loss: 3.436387300491333
```

```
"""
src/ch5/gpipe.py
"""

import torch
import torch.nn as nn
from datasets import load_dataset
from torch.optim import Adam
from torch.utils.data import DataLoader
from torchgpipe import GPipe
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from transformers.models.gpt2.modeling_gpt2 import GPT2Block as GPT2BlockBase


class GPT2Preprocessing(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.embed_dim = config.hidden_size
        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)
        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)
        self.drop = nn.Dropout(config.embd_pdrop)

    def forward(self, input_ids):
        input_shape = input_ids.size()
        input_ids = input_ids.view(-1, input_shape[-1])
        position_ids = torch.arange(
            0, input_shape[-1], dtype=torch.long, device=input_ids.device
        )
        position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])
        inputs_embeds = self.wte(input_ids)
        position_embeds = self.wpe(position_ids)
        hidden_states = inputs_embeds + position_embeds
        hidden_states = self.drop(hidden_states)
        return hidden_states


class GPT2Block(GPT2BlockBase):
    def forward(self, hidden_states):
        hidden_states = super(GPT2Block, self).forward(
            hidden_states=hidden_states,
        )
        return hidden_states[0]


class GPT2Postprocessing(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.ln_f = nn.LayerNorm(
            config.hidden_size,
            eps=config.layer_norm_epsilon,
        )
        self.lm_head = nn.Linear(
            config.hidden_size,
            config.vocab_size,
            bias=False,
        )

    def forward(self, hidden_states):
        hidden_states = self.ln_f(hidden_states)
        lm_logits = self.lm_head(hidden_states)
        return lm_logits


def create_model_from_pretrained(model_name):
    pretrained = GPT2LMHeadModel.from_pretrained(model_name)
    preprocess = GPT2Preprocessing(pretrained.config)
    preprocess.wte.weight = pretrained.transformer.wte.weight
    preprocess.wpe.weight = pretrained.transformer.wpe.weight

    blocks = pretrained.transformer.h
    for i, block in enumerate(blocks):
        block.__class__ = GPT2Block

    postprocess = GPT2Postprocessing(pretrained.config)
    postprocess.ln_f.weight = pretrained.transformer.ln_f.weight
    postprocess.ln_f.bias = pretrained.transformer.ln_f.bias
    postprocess.lm_head.weight.data = pretrained.lm_head.weight.data.clone()

    return nn.Sequential(preprocess, *blocks, postprocess)


if __name__ == "__main__":
    world_size = 4

    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token

    model = create_model_from_pretrained(model_name="gpt2")
    model = GPipe(
        model,
        #balance=[4,3,3,4],
        balance=[3,5,5,1],
        devices=[0, 1, 2, 3],
        chunks=world_size,
    )

    datasets = load_dataset("squad").data["train"]["context"]
    datasets = [str(sample) for sample in datasets]
    #data_loader = DataLoader(datasets, batch_size=8, num_workers=8)
    data_loader = DataLoader(datasets, batch_size=24, num_workers=8)

    optimizer = Adam(model.parameters(), lr=3e-5)
    loss_fn = nn.CrossEntropyLoss()

    for i, data in enumerate(data_loader):
        optimizer.zero_grad()
        tokens = tokenizer(data, return_tensors="pt", truncation=True, padding=True)
        input_ids = tokens.input_ids.to(0)
        labels = tokens.input_ids.to(world_size - 1)

        lm_logits = model(input_ids)
        shift_logits = lm_logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        loss = nn.CrossEntropyLoss()(
            shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)
        )
        loss.backward()
        optimizer.step()

        if i % 10 == 0:
            print(f"step: {i}, loss: {loss}")
        if i == 300:
             break
```
```
(large-scale-lm) [gpu05]$ python gpipe.py
/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/torchgpipe/stream.py:99: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  tensor = tensor.new_empty([0]).set_(tensor.storage())
step: 0, loss: 7.221696376800537
step: 10, loss: 2.02380633354187
step: 20, loss: 2.257195472717285
step: 30, loss: 2.9732449054718018
step: 40, loss: 3.133763551712036
step: 50, loss: 3.2365360260009766
step: 60, loss: 1.7914276123046875
step: 70, loss: 3.4165256023406982
step: 80, loss: 2.8382716178894043
step: 90, loss: 2.248094081878662
step: 100, loss: 2.6100006103515625
step: 110, loss: 1.4518338441848755
step: 120, loss: 2.2637827396392822
step: 130, loss: 2.457871913909912
step: 140, loss: 2.3267903327941895
step: 150, loss: 2.572256326675415
step: 160, loss: 2.491018056869507
step: 170, loss: 1.8535436391830444
step: 180, loss: 0.9895756244659424
step: 190, loss: 1.5809266567230225
step: 200, loss: 2.164494276046753
step: 210, loss: 2.1609110832214355
step: 220, loss: 2.5104143619537354
step: 230, loss: 2.2100632190704346
step: 240, loss: 2.479640483856201
step: 250, loss: 2.0741147994995117
step: 260, loss: 2.7896156311035156
step: 270, loss: 3.0486862659454346
step: 280, loss: 1.6412955522537231
step: 290, loss: 2.4288651943206787
step: 300, loss: 2.189744234085083
```

## 3. 1F1B Pipelining (PipeDream)
Microsoftì—ì„œ ê³µê°œí•œ `PipeDream`ì€ `GPipe`ì™€ëŠ” ì•½ê°„ ë‹¤ë¥¸ ë°©ì‹ì˜ íŒŒì´í”„ë¼ì´ë‹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. í”íˆ ì´ ë°©ë²•ì„ 1F1Bë¼ê³  ë¶€ë¥´ëŠ”ë°, ëª¨ë“  Forwardê°€ ëë‚˜ê³  ë‚˜ì„œ Backwardë¥¼ ìˆ˜í–‰í•˜ëŠ” GPipeì™€ ë‹¬ë¦¬ `PipeDream`ì€ Forwardì™€ Backwardë¥¼ ë²ˆê°ˆì•„ê°€ë©´ì„œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
   
![](../images/1f1b.png)

1F1B Pipeliningì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‘ê°€ì§€ ì±Œë¦°ì§€ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.
1. Weight version managing
2. Work partitioning
 
### 1) Weight version managinig
GPipeì˜ ê²½ìš° í•˜ë‚˜ì˜ weight ë²„ì „ë§Œ ìš´ìš©í•˜ì§€ë§Œ ì£¼ê¸°ì ìœ¼ë¡œ Pipeline flushê°€ ì¼ì–´ë‚©ë‹ˆë‹¤. **Pipeline flushë€ ê³„ì‚°ëœ Gradientë¥¼ í†µí•´ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸(`optimizer.step()`) í•˜ëŠ” ê³¼ì •**ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ flush ê³¼ì • ì¤‘ì—ëŠ” ì–´ë– í•œ **forward, backward ì—°ì‚°ë„ í•˜ì§€ ì•Šê¸°** ë•Œë¬¸ì— ì²˜ë¦¬ íš¨ìœ¨ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤.

![](../images/pipeline_flush.png)
   
PipeDreamì€ ì´ëŸ¬í•œ flush ì—†ì´ ê³„ì†í•´ì„œ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸ í•´ë‚˜ê°‘ë‹ˆë‹¤. ë”°ë¼ì„œ forwardì™€ backwardê°€ ëª¨ë‘ ì‰¬ëŠ” ì‹œê°„ì´ ì‚¬ë¼ì§‘ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ë¥¼ ìœ„í•´ì„œëŠ” ì—¬ëŸ¬ ë²„ì „ì˜ íŒŒë¼ë¯¸í„° ìƒíƒœë¥¼ ì§€ì†ì ìœ¼ë¡œ ê´€ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. ë§Œì•½ ìµœì‹ ë²„ì „ì˜ íŒŒë¼ë¯¸í„°ë§Œ ì €ì¥í•˜ê³  ìˆìœ¼ë©´ ì´ì „ layerì˜ ì¶œë ¥ì´ ë‹¤ìŒ layerë¡œ ì „ì†¡ë  ë•Œ, ë‹¤ìŒ layer ë¶€ë¶„ì´ ì—…ë°ì´íŠ¸ ë  ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì´ì£ .

![](../images/1f1b.gif)

ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ë§‰ê¸° ìœ„í•´ ì—¬ëŸ¬ ë²„ì „ì˜ weightë¥¼ ì €ì¥í•˜ì—¬ ê´€ë¦¬í•˜ëŠ”ë° ê·¸ëŸ¬ë©´ weightë¥¼ ì €ì¥í•˜ë©´ ë©”ëª¨ë¦¬ ê³µê°„ì„ ë§ì´ ì°¨ì§€í•˜ê²Œ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ ë¶€ë¶„ì—ì„œ íŠ¸ë ˆì´ë“œ ì˜¤í”„ê°€ ë°œìƒí•©ë‹ˆë‹¤.
- GPipe: ë©”ëª¨ë¦¬ íš¨ìœ¨ì , í”„ë¡œì„¸ì‹± ë¹„íš¨ìœ¨ì 
- PipeDream: ë©”ëª¨ë¦¬ ë¹„íš¨ìœ¨ì , í”„ë¡œì„¸ì‹± íš¨ìœ¨ì 
    
### 2) Work Partitioning
ë‘ë²ˆì¨° ë¬¸ì œëŠ” ë‰´ëŸ´ë„·ì„ ì–´ë–»ê²Œ ìª¼ê°¤ê±´ì§€ì— ëŒ€í•œ ë¬¸ì œì…ë‹ˆë‹¤. ë‹¨ìˆœíˆ Layerë³„ë¡œ ë™ì¼í•œ ìˆ˜ì˜ ë ˆì´ì–´ë¥¼ ê°–ê²Œë” í•˜ëŠ” ê²ƒì´ í•­ìƒ ìµœê³ ì˜ ì†”ë£¨ì…˜ì´ë¼ê³  í•  ìˆ˜ëŠ” ì—†ê² ì£ . ìš°ë¦¬ì—ê²Œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ idle timeì„ ìµœì†Œí™”ì„ ìµœì†Œí™” í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” ê° íŒŒí‹°ì…˜ì˜ running timeì´ ë¹„ìŠ·í•´ì•¼ê² ì£ . ê·¸ ì´ì™¸ì—ë„ ì¶”ê°€ë¡œ parameter size, activation memory ë“±ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.

![](../images/pipe_dream.png)
    
PipeDreamì€ Profilingê³¼ Optimizingì„ í†µí•´ ìµœì ì˜ Partioning ì „ëµì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤.
  
## 4. Variations of 1F1B Pipelining

PipeDreamì˜ 1F1B íŒŒì´í”„ë¼ì´ë‹ì„ ê°œì„ í•œ ë‘ê°€ì§€ ë²„ì „ì˜ íŒŒì´í”„ë¼ì¸ ì „ëµì„ ì†Œê°œí•©ë‹ˆë‹¤.   
### 1) PipeDream 2BW (2-buffered weight update)
PipeDream 2BWëŠ” PipeDreamì˜ ë©”ëª¨ë¦¬ ë¹„íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ë“±ì¥í–ˆìŠµë‹ˆë‹¤. í•µì‹¬ ì•„ì´ë””ì–´ëŠ” íŒŒì´í”„ë¼ì´ë‹ ì¤‘ì— Gradient Accumulationì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì—¬ëŸ¬ê°œì˜ Gradientë“¤ì„ ëª¨ì•„ë‘ë‹¤ê°€ í•œë²ˆì— ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë©”ëª¨ë¦¬ ë¹„íš¨ìœ¨ì„± ë¬¸ì œë¥¼ í•´ê²°í–ˆì£ . 2BWëŠ” ì´ì „ê³¼ ë‹¬ë¦¬ ë‹¨ ë‘ê°œì˜ weight versionë§Œ ìœ ì§€í•˜ë©´ ë©ë‹ˆë‹¤.
   
![](../images/pipe_dream_2bw.png)
  
### 2) PipeDream Flush
PipeDream FlushëŠ” 1F1Bì™€ Pipeline Flushë¥¼ ê²°í•©í•œ íŒŒì´í”„ë¼ì´ë‹ ë°©ë²•ì…ë‹ˆë‹¤. ì´ íŒŒì´í”„ë¼ì´ë‹ ë°©ë²•ì€ Flushê°€ ì¼ì–´ë‚˜ê¸° ë•Œë¬¸ì— GPIpeì™€ ë¹„êµí•˜ì—¬ idle timeì€ ë¹„ìŠ·í•˜ë‚˜, forward-backward ê³¼ì •ì—ì„œ ìœ ì§€í•´ì•¼ í•˜ëŠ” **activation memoryê°€ ì¤„ì–´ë“­ë‹ˆë‹¤.** PipeDream FlushëŠ” Flushê°€ ì¼ì–´ë‚˜ê¸° ë•Œë¬¸ì— ì—¬ëŸ¬ë²„ì „ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê´€ë¦¬í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¨ì¼ ê°€ì¤‘ì¹˜ë§Œ ìœ ì§€í•˜ë©´ ë˜ê¸° ë•Œë¬¸ì— PipeDream 2BWë³´ë‹¤ë„ ë” ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì…ë‹ˆë‹¤. (ì§€ê¸ˆê¹Œì§€ ì†Œê°œë“œë¦° ê¸°ë²•ë“¤ ì¤‘ ê°€ì¥ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì…ë‹ˆë‹¤.)
    
![](../images/pipe_dream_flush.png)

![](../images/pipe_dream_flush_2.png)
 
### ì ê¹... ê·¼ë° Activation Memoryê°€ ë­ì•¼?
ëŒ€ë¶€ë¶„ì˜ Layerë“¤ì€ Backwardë¥¼ í˜¸ì¶œí•˜ê¸° ì „ì— Forwardì—ì„œ ë‚˜ì˜¨ ì¶œë ¥ê°’ë“¤ì„ ì €ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” `torch.autograd.Function`ì„ ì‚¬ìš©í•´ë³´ì‹  ë¶„ë“¤ì€ ì˜ ì•„ì‹¤í…ë°ìš”. `ctx`ë³€ìˆ˜ì— forward ë ˆì´ì–´ì˜ ì¶œë ¥ê°’ë“¤ì„ ì €ì¥í•´ë‘¡ë‹ˆë‹¤.
```
"""
ì°¸ê³ : https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html
"""

import torch


class ReLU(torch.autograd.Function):

    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        # input ê°’ì„ ì €ì¥í•˜ê³  ìˆìŒ.
        
        return input.clamp(min=0)

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input < 0] = 0
        return grad_input
```
 
ì´ëŠ” ë¯¸ë¶„ê°’(Gradient)ì„ ê³„ì‚°í• ë•Œ Forward ê³¼ì •ì—ì„œ ì‚¬ìš©í–ˆë˜ ê°’ë“¤ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ì˜ˆì‹œë¥¼ ë´…ì‹œë‹¤.
  
![](../images/max_pooling.png)
    
ìœ„ëŠ” Max Pooling ì—°ì‚°ê³¼ ê·¸ì— ëŒ€í•œ Gradientë¥¼ ê³„ì‚°í•œ ê²ƒì…ë‹ˆë‹¤. Backwardë¥¼ ìˆ˜í–‰í• ë•ŒëŠ” [[0.8, 1.2], [0.9, 0.5]]ì™€ ê°™ì€ (2, 2) í…ì„œê°€ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì˜µë‹ˆë‹¤. ì´ ê°’ì„ ê°€ì§€ê³  ì˜¤ë¥¸ìª½ì˜ Gradient Matrixë¥¼ ì°¾ì•„ë‚´ì•¼ í•˜ëŠ”ë° ë°˜ë“œì‹œ Forwardì—ì„œ ë°›ì•˜ë˜ (4, 4)ì˜ í…ì„œê°€ í•„ìš”í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ í…ì„œë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ê³  ìˆëŠ” ê²ƒì´ì£ . ì´ë ‡ê²Œ Backwardë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ Forward ë‹¹ì‹œì— ì“°ì˜€ë˜ í…ì„œë“¤ì„ ì €ì¥í•´ë‘ê¸° ìœ„í•´ í•„ìš”í•œ ë©”ëª¨ë¦¬ë¥¼ Activation Memoryë¼ê³  í•©ë‹ˆë‹¤."
 
ì´ì œ Activation Memoryê°€ ë­”ì§€ ì•Œì•˜ìœ¼ë‹ˆ, PipeDreamì„ ì‹¤ìŠµí•´ë³¼ê¹Œìš”? **PipeDream FlushëŠ” MSì˜ ë¶„ì‚°ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ DeepSpeedì— êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.** (ì°¸ê³ : https://github.com/microsoft/DeepSpeed/issues/1110) ë”°ë¼ì„œ DeepSpeedë¥¼ ì‚¬ìš©í•´ë´…ì‹œë‹¤.

### DeepSpeed ëª…ë ¹ì–´ ì‚¬ìš©ë²•
ì•„ ì°¸, ê·¸ ì „ì— `deepspeed`ê°€ ì œê³µí•˜ëŠ” ë§¤ìš° í¸ë¦¬í•œ ê¸°ëŠ¥ì„ ë¨¼ì € ì•Œì•„ë³´ê³  ê°€ê² ìŠµë‹ˆë‹¤. ê¸°ì¡´ì—ëŠ” ë¶„ì‚°ì²˜ë¦¬ë¥¼ ìœ„í•´ `python -m torch.distributed.launch --nproc_per_node=n OOO.py`ë¥¼ ì‚¬ìš©í–ˆìœ¼ë‚˜ ë„ˆë¬´ ê¸¸ì–´ì„œ ë¶ˆí¸í–ˆì£ . DeepSpeedëŠ” `deepspeed` í˜¹ì€ `ds`ì™€ ê°™ì€ ëª…ë ¹ì–´ë¥¼ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤. 
- `ds --num_gpus=n OOO.py`
- `deepspeed --num_gpus=n OOO.py`
  
ìœ„ì™€ ê°™ì€ ëª…ë ¹ì–´ë¥¼ ì…ë ¥í•˜ë©´ `torch.distributed.launch`ì™€ ë™ì¼í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤. ì´ì œë¶€í„°ëŠ” ëª¨ë“  ë¶„ì‚°ì²˜ë¦¬ í”„ë¡œê·¸ë¨ì— `deepspeed`ì˜ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. (ì†”ì§íˆ `torch.distributed.launch`ëŠ” ë„ˆë¬´ ê¸¸ì–´ìš” ğŸ˜­)
```
"""
src/ch5/pipe_dream.org.py
"""
import deepspeed
import torch
import torch.nn as nn
from datasets import load_dataset
from deepspeed import PipelineModule
from torch.optim import Adam
from torch.utils.data import DataLoader
from tqdm import tqdm
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from transformers.models.gpt2.modeling_gpt2 import GPT2Block as GPT2BlockBase
import torch.distributed as dist


class GPT2Preprocessing(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.embed_dim = config.hidden_size
        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)
        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)
        self.drop = nn.Dropout(config.embd_pdrop)

    def forward(self, input_ids):
        input_shape = input_ids.size()
        input_ids = input_ids.view(-1, input_shape[-1])
        position_ids = torch.arange(
            0, input_shape[-1], dtype=torch.long, device=input_ids.device
        )
        position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])
        inputs_embeds = self.wte(input_ids)
        position_embeds = self.wpe(position_ids)
        hidden_states = inputs_embeds + position_embeds
        hidden_states = self.drop(hidden_states)
        return hidden_states


class GPT2Block(GPT2BlockBase):
    def forward(self, hidden_states):
        hidden_states = super(GPT2Block, self).forward(
            hidden_states=hidden_states,
        )
        return hidden_states[0]


class GPT2Postprocessing(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.ln_f = nn.LayerNorm(
            config.hidden_size,
            eps=config.layer_norm_epsilon,
        )
        self.lm_head = nn.Linear(
            config.hidden_size,
            config.vocab_size,
            bias=False,
        )

    def forward(self, hidden_states):
        hidden_states = self.ln_f(hidden_states)
        lm_logits = self.lm_head(hidden_states)
        return lm_logits


def create_model_from_pretrained(model_name):
    pretrained = GPT2LMHeadModel.from_pretrained(model_name)
    preprocess = GPT2Preprocessing(pretrained.config)
    preprocess.wte.weight = pretrained.transformer.wte.weight
    preprocess.wpe.weight = pretrained.transformer.wpe.weight

    blocks = pretrained.transformer.h
    for i, block in enumerate(blocks):
        block.__class__ = GPT2Block

    postprocess = GPT2Postprocessing(pretrained.config)
    postprocess.ln_f.weight = pretrained.transformer.ln_f.weight
    postprocess.ln_f.bias = pretrained.transformer.ln_f.bias
    postprocess.lm_head.weight.data = pretrained.lm_head.weight.data.clone()

    return nn.Sequential(preprocess, *blocks, postprocess)


def collate_fn(batch):
    batch_encoding = tokenizer.pad(
        {"input_ids": batch}, padding="max_length", max_length=1024
    )
    return batch_encoding.input_ids


def batch_fn(data):
    input_ids = data
    labels = data
    return input_ids, labels


def loss_fn(logits, labels):
    logits = logits[..., :-1, :].contiguous()
    labels = labels[..., 1:].contiguous()

    return nn.CrossEntropyLoss()(
        logits.view(-1, logits.size(-1)),
        labels.view(-1),
    )


if __name__ == "__main__":
    dist.init_process_group("nccl")
    world_size, rank = dist.get_world_size(), dist.get_rank()
    batch_size, train_steps = 16, 300
    train_samples = batch_size * train_steps

    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token

    model = PipelineModule(
        create_model_from_pretrained(model_name="gpt2"),
        loss_fn=loss_fn,
        num_stages=world_size,
        partition_method="type:GPT2Block"
        # partition_methodë¥¼ í†µí•´ ë³‘ë ¬í™” í•˜ê³  ì‹¶ì€ ë ˆì´ì–´ë¥¼ ê³ ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    )
    engine, optimizer, _, _ = deepspeed.initialize(
        model=model,
        optimizer=Adam(model.parameters(), lr=3e-5),
        config={
            "train_batch_size": batch_size,
            "steps_per_print": 9999999,
            # turn off: https://github.com/microsoft/DeepSpeed/issues/1119
        },
    )
    engine.set_batch_fn(batch_fn)

    datasets = load_dataset("squad").data["train"]["context"]
    datasets = [str(sample) for i, sample in enumerate(datasets) if i < train_samples]
    datasets = [
        tokenizer(data, return_tensors="pt", max_length=1024).input_ids[0]
        for data in tqdm(datasets)
    ]
    data_loader = iter(
        DataLoader(
            sorted(datasets, key=len, reverse=True),
            # uniform length batching
            # https://mccormickml.com/2020/07/29/smart-batching-tutorial/
            batch_size=batch_size,
            #num_workers=8,
            num_workers=0,
            collate_fn=collate_fn,
            shuffle=False,
        )
    )

    for i in range(train_steps):
        loss = engine.train_batch(data_loader)

        if i % 10 == 0 and rank == 0:
            print(f"step: {i}, loss: {loss}")
```

```
(large-scale-lm) [gpu05]$ pip install deepspeed==0.15.1
Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
Collecting deepspeed
  Downloading deepspeed-0.15.1.tar.gz (1.4 MB)
     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.4/1.4 MB 16.5 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... done
Collecting hjson (from deepspeed)
  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)
Collecting ninja (from deepspeed)
  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)
Requirement already satisfied: numpy in /scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages (from deepspeed) (2.0.1)
Requirement already satisfied: packaging>=20.0 in /scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages (from deepspeed) (24.1)
Collecting psutil (from deepspeed)
  Downloading psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)
Collecting py-cpuinfo (from deepspeed)
  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)
Collecting pydantic>=2.0.0 (from deepspeed)
  Downloading pydantic-2.9.0-py3-none-any.whl.metadata (146 kB)
Requirement already satisfied: torch in /scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages (from deepspeed) (2.3.0)
Requirement already satisfied: tqdm in /scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages (from deepspeed) (4.66.5)
Collecting nvidia-ml-py (from deepspeed)
  Downloading nvidia_ml_py-12.560.30-py3-none-any.whl.metadata (8.6 kB)
Collecting annotated-types>=0.4.0 (from pydantic>=2.0.0->deepspeed)
  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)
Collecting pydantic-core==2.23.2 (from pydantic>=2.0.0->deepspeed)
  Downloading pydantic_core-2.23.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)
Requirement already satisfied: typing-extensions>=4.6.1 in /scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages (from pydantic>=2.0.0->deepspeed) (4.11.0)
Requirement already satisfied: tzdata in /scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages (from pydantic>=2.0.0->deepspeed) (2024.1)
Requirement already satisfied: filelock in /scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages (from torch->deepspeed) (3.13.1)
Requirement already satisfied: sympy in /scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages (from torch->deepspeed) (1.13.2)
Requirement already satisfied: networkx in /scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages (from torch->deepspeed) (3.2.1)
Requirement already satisfied: jinja2 in /scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages (from torch->deepspeed) (3.1.4)
Requirement already satisfied: fsspec in /scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages (from torch->deepspeed) (2024.6.1)
Requirement already satisfied: MarkupSafe>=2.0 in /scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages (from jinja2->torch->deepspeed) (2.1.3)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages (from sympy->torch->deepspeed) (1.3.0)
Downloading pydantic-2.9.0-py3-none-any.whl (434 kB)
Downloading pydantic_core-2.23.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2.1/2.1 MB 27.5 MB/s eta 0:00:00
Downloading hjson-3.1.0-py3-none-any.whl (54 kB)
Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)
Downloading nvidia_ml_py-12.560.30-py3-none-any.whl (40 kB)
Downloading psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (290 kB)
Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)
Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)
Building wheels for collected packages: deepspeed
  Building wheel for deepspeed (setup.py) ... done
  Created wheel for deepspeed: filename=deepspeed-0.15.1-py3-none-any.whl size=1483871 sha256=8df093efa22a297d30e79d70db35ce6ff73e58b19dac1ff3015eaf3a9a2cd60b
  Stored in directory: /tmp/pip-ephem-wheel-cache-c_ezqmsr/wheels/da/cb/14/9cbba50c73df044eb32a7ca29e34844c5f8959e12d22ae8b60
Successfully built deepspeed
Installing collected packages: py-cpuinfo, nvidia-ml-py, ninja, hjson, pydantic-core, psutil, annotated-types, pydantic, deepspeed
Successfully installed annotated-types-0.7.0 deepspeed-0.15.1 hjson-3.1.0 ninja-1.11.1.1 nvidia-ml-py-12.560.30 psutil-6.0.0 py-cpuinfo-9.0.0 pydantic-2.9.0 pydantic-core-2.23.2
```
```
(large-scale-lm) [gpu05]$ ds --num_gpus=4 pipe_dream.org.py
[2024-09-08 23:02:41,542] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-08 23:02:45,778] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-09-08 23:02:45,778] [INFO] [runner.py:585:main] cmd = /scratch/qualis/miniconda3/envs/large-scale-lm/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None pipe_dream.org.py
[2024-09-08 23:02:47,226] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-08 23:02:49,632] [INFO] [launch.py:139:main] 0 NCCL_HOME=/scratch/qualis/nccl_2.11.4-1+cuda11.4_x86_64
[2024-09-08 23:02:49,632] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-09-08 23:02:49,632] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-09-08 23:02:49,632] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-09-08 23:02:49,632] [INFO] [launch.py:164:main] dist_world_size=4
[2024-09-08 23:02:49,632] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-09-08 23:02:49,633] [INFO] [launch.py:256:main] process 51266 spawned with command: ['/scratch/qualis/miniconda3/envs/large-scale-lm/bin/python', '-u', 'pipe_dream.org.py', '--local_rank=0']
[2024-09-08 23:02:49,634] [INFO] [launch.py:256:main] process 51267 spawned with command: ['/scratch/qualis/miniconda3/envs/large-scale-lm/bin/python', '-u', 'pipe_dream.org.py', '--local_rank=1']
[2024-09-08 23:02:49,635] [INFO] [launch.py:256:main] process 51268 spawned with command: ['/scratch/qualis/miniconda3/envs/large-scale-lm/bin/python', '-u', 'pipe_dream.org.py', '--local_rank=2']
[2024-09-08 23:02:49,636] [INFO] [launch.py:256:main] process 51269 spawned with command: ['/scratch/qualis/miniconda3/envs/large-scale-lm/bin/python', '-u', 'pipe_dream.org.py', '--local_rank=3']
[2024-09-08 23:02:51,219] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-08 23:02:51,257] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-08 23:02:51,266] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-08 23:02:51,269] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-08 23:02:56,095] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-09-08 23:02:56,095] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-09-08 23:02:56,095] [INFO] [comm.py:652:init_distributed] cdb=None
[2024-09-08 23:02:56,095] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-09-08 23:02:56,095] [INFO] [comm.py:652:init_distributed] cdb=None
/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
/scratch/qualis/miniconda3/envs/large-scale-lm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
[2024-09-08 23:02:58,121] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 1
SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
Using topology: {ProcessCoord(pipe=0, data=0): 0, ProcessCoord(pipe=1, data=0): 1, ProcessCoord(pipe=2, data=0): 2, ProcessCoord(pipe=3, data=0): 3}
[2024-09-08 23:02:58,452] [INFO] [module.py:396:_partition_layers] Partitioning pipeline stages with method type:GPT2Block
stage=0 layers=4
     0: GPT2Preprocessing
     1: GPT2Block
     2: GPT2Block
     3: GPT2Block
stage=1 layers=3
     4: GPT2Block
     5: GPT2Block
     6: GPT2Block
stage=2 layers=3
     7: GPT2Block
     8: GPT2Block
     9: GPT2Block
stage=3 layers=4
    10: GPT2Block
    11: GPT2Block
    12: GPT2Block
    13: GPT2Postprocessing
  loss: loss_fn
[2024-09-08 23:02:58,513] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2024-09-08 23:02:58,539] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 1
[2024-09-08 23:02:58,753] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.1, git-hash=unknown, git-branch=unknown
[2024-09-08 23:02:58,753] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 1
[2024-09-08 23:02:58,757] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2024-09-08 23:02:58,911] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 1
[2024-09-08 23:02:58,980] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-09-08 23:02:58,981] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-09-08 23:02:58,981] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-09-08 23:02:58,982] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam
[2024-09-08 23:02:58,983] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2024-09-08 23:02:58,983] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2024-09-08 23:02:58,983] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-09-08 23:02:58,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3e-05], mom=[(0.9, 0.999)]
[2024-09-08 23:02:58,984] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-09-08 23:02:58,985] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2024-09-08 23:02:58,985] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-09-08 23:02:58,985] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-09-08 23:02:58,985] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-09-08 23:02:58,986] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2024-09-08 23:02:58,986] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False
[2024-09-08 23:02:58,986] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-09-08 23:02:58,986] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-09-08 23:02:58,986] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-09-08 23:02:58,986] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-09-08 23:02:58,987] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x2af14644d480>
[2024-09-08 23:02:58,987] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-09-08 23:02:58,987] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-09-08 23:02:58,987] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-09-08 23:02:58,987] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-09-08 23:02:58,987] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-09-08 23:02:58,987] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-09-08 23:02:58,987] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-09-08 23:02:58,987] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-09-08 23:02:58,987] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-09-08 23:02:58,987] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-09-08 23:02:58,987] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-09-08 23:02:58,987] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-09-08 23:02:58,987] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-09-08 23:02:58,987] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-09-08 23:02:58,987] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-09-08 23:02:58,987] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-09-08 23:02:58,987] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-09-08 23:02:58,988] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-09-08 23:02:58,988] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-09-08 23:02:58,988] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2024-09-08 23:02:58,988] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-09-08 23:02:58,988] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-09-08 23:02:58,988] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-09-08 23:02:58,988] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-09-08 23:02:58,988] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-09-08 23:02:58,988] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[2024-09-08 23:02:58,988] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.0
[2024-09-08 23:02:58,988] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-09-08 23:02:58,988] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-09-08 23:02:58,988] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-09-08 23:02:58,988] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 65536
[2024-09-08 23:02:58,988] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-09-08 23:02:58,989] [INFO] [config.py:1003:print]   loss_scale ................... 0
[2024-09-08 23:02:58,989] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-09-08 23:02:58,989] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-09-08 23:02:58,989] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-09-08 23:02:58,989] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-09-08 23:02:58,989] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2024-09-08 23:02:58,989] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-09-08 23:02:58,989] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-09-08 23:02:58,989] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-09-08 23:02:58,989] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-09-08 23:02:58,989] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-09-08 23:02:58,989] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-09-08 23:02:58,989] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-09-08 23:02:58,989] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-09-08 23:02:58,989] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-09-08 23:02:58,990] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-09-08 23:02:58,990] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-09-08 23:02:58,990] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-09-08 23:02:58,990] [INFO] [config.py:1003:print]   steps_per_print .............. 9999999
[2024-09-08 23:02:58,990] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-09-08 23:02:58,990] [INFO] [config.py:1003:print]   train_batch_size ............. 16
[2024-09-08 23:02:58,990] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  16
[2024-09-08 23:02:58,990] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-09-08 23:02:58,990] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-09-08 23:02:58,990] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-09-08 23:02:58,990] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-09-08 23:02:58,990] [INFO] [config.py:1003:print]   world_size ................... 1
[2024-09-08 23:02:58,990] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
[2024-09-08 23:02:58,990] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-09-08 23:02:58,990] [INFO] [config.py:1003:print]   zero_enabled ................. False
[2024-09-08 23:02:58,990] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-09-08 23:02:58,990] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0
[2024-09-08 23:02:58,991] [INFO] [config.py:989:print_user_config]   json = {
    "train_batch_size": 16,
    "steps_per_print": 9.999999e+06
}
[2024-09-08 23:02:58,991] [INFO] [engine.py:105:__init__] CONFIG: micro_batches=1 micro_batch_size=16
[2024-09-08 23:02:58,991] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2024-09-08 23:02:59,124] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2024-09-08 23:02:59,384] [INFO] [engine.py:165:__init__] RANK=0 STAGE=0 LAYERS=4 [0, 4) STAGE_PARAMS=60647424 (60.647M) TOTAL_PARAMS=163037184 (163.037M) UNIQUE_PARAMS=163037184 (163.037M)
[2024-09-08 23:02:59,384] [INFO] [engine.py:165:__init__] RANK=3 STAGE=3 LAYERS=4 [10, 14) STAGE_PARAMS=59862528 (59.863M) TOTAL_PARAMS=163037184 (163.037M) UNIQUE_PARAMS=163037184 (163.037M)
[2024-09-08 23:02:59,384] [INFO] [engine.py:165:__init__] RANK=1 STAGE=1 LAYERS=3 [4, 7) STAGE_PARAMS=21263616 (21.264M) TOTAL_PARAMS=163037184 (163.037M) UNIQUE_PARAMS=163037184 (163.037M)
[2024-09-08 23:02:59,384] [INFO] [engine.py:165:__init__] RANK=2 STAGE=2 LAYERS=3 [7, 10) STAGE_PARAMS=21263616 (21.264M) TOTAL_PARAMS=163037184 (163.037M) UNIQUE_PARAMS=163037184 (163.037M)
  0%|                                                                        | 0/4800 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  0%|                                                                        | 0/4800 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  0%|                                                                        | 0/4800 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
  0%|                                                                        | 0/4800 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4800/4800 [00:03<00:00, 1214.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4800/4800 [00:04<00:00, 1181.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4800/4800 [00:04<00:00, 1145.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4800/4800 [00:04<00:00, 1133.00it/s]
step: 0, loss: 6.572026252746582
step: 10, loss: 1.0279344320297241
step: 20, loss: 1.028913974761963
step: 30, loss: 0.7047958970069885
step: 40, loss: 0.7454834580421448
step: 50, loss: 0.812176525592804
step: 60, loss: 0.8095153570175171
step: 70, loss: 0.7496554255485535
step: 80, loss: 0.6204543113708496
step: 90, loss: 0.5449276566505432
step: 100, loss: 0.6550153493881226
step: 110, loss: 0.5415732264518738
step: 120, loss: 0.6244872808456421
step: 130, loss: 0.6143221855163574
step: 140, loss: 0.5201988816261292
step: 150, loss: 0.46794116497039795
step: 160, loss: 0.46409428119659424
step: 170, loss: 0.5390290021896362
step: 180, loss: 0.48016053438186646
step: 190, loss: 0.4120233952999115
step: 200, loss: 0.4341399371623993
step: 210, loss: 0.39358749985694885
step: 220, loss: 0.36393406987190247
step: 230, loss: 0.3397323787212372
step: 240, loss: 0.30079731345176697
step: 250, loss: 0.2613028287887573
step: 260, loss: 0.29585179686546326
step: 270, loss: 0.2654482126235962
step: 280, loss: 0.2011309415102005
step: 290, loss: 0.18337950110435486
[2024-09-08 23:09:58,105] [INFO] [launch.py:351:main] Process 51269 exits successfully.
[2024-09-08 23:09:58,105] [INFO] [launch.py:351:main] Process 51266 exits successfully.
[2024-09-08 23:09:59,106] [INFO] [launch.py:351:main] Process 51267 exits successfully.
[2024-09-08 23:09:59,107] [INFO] [launch.py:351:main] Process 51268 exits successfully.
```

## 5. Interleaved Scheduling
ì´ì „ì—ëŠ” í•˜ë‚˜ì˜ ìŠ¤í…Œì´ì§€(ì—°ì†ëœ ë ˆì´ì–´ ì§‘í•©)ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ê³„ì‚°í•´ì„œ ê²°ê³¼ ê°’ì„ ì¶œë ¥í–ˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´ 8ê°œì˜ ë ˆì´ì–´ê°€ ìˆê³  2ê°œì˜ ë””ë°”ì´ìŠ¤ê°€ ì£¼ì–´ì¡Œë‹¤ê³  ê°€ì •í•œë‹¤ë©´, ì¼ë°˜ì ìœ¼ë¡œ 1ë²ˆ deviceì— 1-4ë²ˆ ë ˆì´ì–´, 2ë²ˆ deviceì— 5-8ë²ˆ ë ˆì´ì–´ì— í• ë‹¹ë˜ê² ì£ . ê·¸ëŸ¬ë©´ 1ë²ˆ deviceëŠ” 1~4ë²ˆ ë ˆì´ì–´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì§„í–‰í•˜ì—¬ ì¶œë ¥í–ˆìŠµë‹ˆë‹¤. (GPipe, 1F1B ëª¨ë‘ ì´ë ‡ê²Œ ë™ì‘í•¨)

![](../images/interleaved_1.png)
   
ê·¸ëŸ¬ë‚˜ **Interleaved Schedulingì€ Bubble timeì„ ê·¹ë„ë¡œ ì¤„ì´ê¸° ìœ„í•´ í•˜ë‚˜ì˜ ìŠ¤í…Œì´ì§€ë¥¼ ì¤‘ì²©í•´ì„œ ì§„í–‰**í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´  1ë²ˆ deviceê°€ 1-4ë²ˆ ë ˆì´ì–´ì— í• ë‹¹ ë˜ì—ˆë‹¤ë©´, 1-2ë²ˆ ë ˆì´ì–´ì˜ ë™ì‹œì— 3-4ë²ˆ ë ˆì´ì–´ë¥¼ ë™ì‹œì— ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ë˜ë©´ Bubble timeì€ ì¤„ì–´ë“¤ì§€ë§Œ í†µì‹ ë¹„ìš©ì´ ì»¤ì§€ê¸° ë•Œë¬¸ì— ì˜ ì¡°ì ˆí•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. (íŠ¸ë ˆì´ë“œ ì˜¤í”„ ì¡´ì¬)
