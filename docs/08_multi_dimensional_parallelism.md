{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5626dab0",
   "metadata": {},
   "source": [
    "# Multi-dimensional Parallelism\n",
    "\n",
    "이번 세션에서는 Multi-dimensional Parallelism을 위해 사용되는 몇가지 개념과 실습을 진행해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff28d00b",
   "metadata": {},
   "source": [
    "## 1. Multi-dimensional Parallelism\n",
    "\n",
    "Multi-dimensional Parallelism (다차원 병렬화)는 지금까지 공부했던 다양한 병렬화 기법을 함께 사용하는 것입니다. 예를 들면 GPU가 0번부터 7번까지 8대가 있다면 2대는 Data parallelism, 2대는 Pipeline parallelism, 2대는 Tensor parallelism 등을 적용할 수 있습니다. 이때 몇개 차원으로 병렬화가 적용되는지에 따라 N차원 병렬화라고 불리며 방금의 예시는 (2x2x2)로 3차원 병렬화가 되겠죠.\n",
    "\n",
    "![](../images/parallelism.png)\n",
    "\n",
    "이렇게 다양한 병렬처리 기법을 동시에 적용하는 것은 매우 고난도의 기술이 필요합니다. 이번 세션에서는 이러한 병렬처리 기법을 동시에 다루는 방법에 대해 알아봅시다. \n",
    "\n",
    "<br>\n",
    "\n",
    "## 2. MPU (Model Parallel Unit)\n",
    "\n",
    "https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/mpu/initialize.py#L57\n",
    "\n",
    "MPU는 Megatron-LM에서 제안된 개념으로 모델 병렬처리와 관련된 다양한 모듈들을 제공합니다. 특히 **MPU는 다차원 병렬화를 매우 손쉽게 수행할 수 있도록 프로세스 그룹을 자동으로 생성하고 관리해주는 기능**을 가지고 있습니다.\n",
    "\n",
    "다차원 병렬화를 위한 프로세스 그룹 예시를 살펴봅시다. \n",
    "\n",
    "우리가 16개의 GPU를 가지고 있다고 가정해봅시다. 그리고 (Data:2 x Tensor:2 x Pipeline:4)의 차원으로 모델을 병렬화 한다고 해봅시다. 그러면 Tensor parallelism group은 8개, Data parallelism group은 8개, Pipeline parallelism group 4개가 생성됩니다. 즉, 전체 GPU의 수를 해당 병렬화에 할당된 차원의 수로 나눈 값 만큼의 프로세스가 생성됩니다.\n",
    "\n",
    "- Tensor parallelism group은 다음과 같이 생성 될 수 있습니다.\n",
    "  - `[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]`\n",
    "  - 즉, 0번 gpu는 1번 gpu와 tensor parallel 통신을 수행할 수 있으며 2번 gpu는 3번 gpu와 통신 가능합니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Data parallelism group은 다음과 같이 생성 될 수 있습니다.\n",
    "  - `[0, 2], [1, 3], [4, 6], [5, 7], [8, 10], [9, 11], [12, 14], [13, 15]`\n",
    "  - 즉, 0번 gpu는 2번 gpu와 data parallel  통신을 수행할 수 있으며 1번 gpu는 3번 gpu와 통신 가능합니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Pipeline parallelism group은 다음과 같이 생성 될 수 있습니다.\n",
    "  - `[0, 4, 8, 12], [1, 5, 9, 13], [2, 6, 10, 14], [3, 7, 11, 15]`\n",
    "  - Forward 시 0 → 4 → 8 → 12번 방향으로, Backward시 12 → 8 → 4 → 0번 방향으로 통신이 수행됩니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "꽤나 복잡하지만 프로세스가 생성되는 순서를 외우거나 할 필요는 없습니다. MPU 객체가 알아서 이를 처리합니다. 이를 그림으로 나타내면 아래와 같습니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "              +---------+  +---------+  +---------+  +---------+\n",
    "      tensor  |   g00   |  |   g04   |  |   g08   |  |   g12   |\n",
    "data          +---------+  +---------+  +---------+  +---------+ ===> forward\n",
    "      tensor  |   g01   |  |   g05   |  |   g09   |  |   g13   |\n",
    "              +---------+  +---------+  +---------+  +---------+\n",
    "               pipeline     pipeline     pipeline     pipeline\n",
    "\n",
    "              +---------+  +---------+  +---------+  +---------+\n",
    "      tensor  |   g02   |  |   g06   |  |   g10   |  |   g14   |\n",
    "data          +---------+  +---------+  +---------+  +---------+ ===> forward\n",
    "      tensor  |   g03   |  |   g07   |  |   g11   |  |   g15   |\n",
    "              +---------+  +---------+  +---------+  +---------+\n",
    "                pipeline     pipeline     pipeline     pipeline\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "3개의 차원으로 분할되었으니 아래와 같이 3D와 같은 육면체를 구성할 수도 있습니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "                        [g02, g06, g10, g14]\n",
    "                      /  |              /  |\n",
    "                     [g00, g04, g08, g12]  |\n",
    "                     |   |             |   |\n",
    "        3D parallel  |  [g03, g07, g11, g15]\n",
    "                     |  /              |  /\n",
    "                     [g01, g05, g09, g13]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17248e1",
   "metadata": {},
   "source": [
    "MPU를 직접 구현해서 사용해보겠습니다. 이 구현은 Megatron-LM의 코드를 기반으로 제가 변경한 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cf316eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import Tensor\n",
    "from torch.autograd.function import Function\n",
    "\n",
    "\n",
    "class MPU(object):\n",
    "    \"\"\"\n",
    "    MPU: Model Parallel Unit\n",
    "\n",
    "    Notes:\n",
    "        Let's say we have a total of 16 GPUs denoted g0 ... g15 and we use 2 GPUs to parallelize the model tensor,\n",
    "        and 4 GPUs to parallelize the model pipeline. The present method will create 8 tensor model-parallel group,\n",
    "        4 pipeline model parallel groups and 8 data parallel groups as:\n",
    "\n",
    "        - width: 4 pipeline parallel group\n",
    "            [g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]\n",
    "        - height: 8 tensor parallel group\n",
    "            [g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]\n",
    "        - depth: 8 data parallel group\n",
    "            [g0, g2], [g1, g3], [g4, g6], [g5, g7], [g8, g10], [g9, g11], [g12, g14], [g13, g15]\n",
    "\n",
    "                        [g02, g06, g10, g14]\n",
    "                      /  |              /  |\n",
    "                     [g00, g04, g08, g12]  |\n",
    "                     |   |             |   |\n",
    "        3D parallel  |  [g03, g07, g11, g15]\n",
    "                     |  /              |  /\n",
    "                     [g01, g05, g09, g13]\n",
    "\n",
    "                      +---------+  +---------+  +---------+  +---------+\n",
    "              tensor  |   g00   |  |   g04   |  |   g08   |  |   g14   |\n",
    "        data          +---------+  +---------+  +---------+  +---------+ ===> forward\n",
    "              tensor  |   g01   |  |   g05   |  |   g09   |  |   g13   |\n",
    "                      +---------+  +---------+  +---------+  +---------+\n",
    "                        pipeline     pipeline     pipeline     pipeline\n",
    "\n",
    "                      +---------+  +---------+  +---------+  +---------+\n",
    "              tensor  |   g02   |  |   g06   |  |   g10   |  |   g12   |\n",
    "        data          +---------+  +---------+  +---------+  +---------+ ===> forward\n",
    "              tensor  |   g03   |  |   g07   |  |   g11   |  |   g15   |\n",
    "                      +---------+  +---------+  +---------+  +---------+\n",
    "                        pipeline     pipeline     pipeline     pipeline\n",
    "\n",
    "    References:\n",
    "        Original MPU implementation of Megatron-LM.\n",
    "        https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/mpu/initialize.py\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    _tensor_model_parallel_group = None\n",
    "    _pipeline_model_parallel_group = None\n",
    "    _data_parallel_group = None\n",
    "\n",
    "    _tensor_model_parallel_world_size = None\n",
    "    _pipeline_model_parallel_world_size = None\n",
    "    _data_parallel_world_size = None\n",
    "\n",
    "    _tensor_model_parallel_rank = None\n",
    "    _pipeline_model_parallel_rank = None\n",
    "    _pipeline_global_ranks = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tensor_model_parallel_size: int,\n",
    "        pipeline_model_parallel_size: int,\n",
    "        backend: str,\n",
    "        master_port: int,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize MPU object. All process groups are initialized in this method.\n",
    "\n",
    "        Args:\n",
    "            tensor_model_parallel_size (int): tensor model parallel world size\n",
    "            pipeline_model_parallel_size (int): pipeline model parallel world size\n",
    "        \"\"\"\n",
    "\n",
    "        if not dist.is_initialized():\n",
    "            self.initialize_distributed(backend, master_port)\n",
    "\n",
    "        current_rank = dist.get_rank()\n",
    "        global_world_size = dist.get_world_size()\n",
    "\n",
    "        assert (\n",
    "            global_world_size >= tensor_model_parallel_size\n",
    "        ), \"param `tensor_model_parallel_size` must be smaller than global world size.\"\n",
    "\n",
    "        assert (\n",
    "            global_world_size >= pipeline_model_parallel_size\n",
    "        ), \"param `pipeline_model_parallel_size` must be smaller than global world size.\"\n",
    "\n",
    "        total_model_parallel_size = (\n",
    "            tensor_model_parallel_size * pipeline_model_parallel_size\n",
    "        )\n",
    "\n",
    "        assert (\n",
    "            global_world_size % total_model_parallel_size == 0\n",
    "        ), \"global world sizes must be divisible by model parallel world sizes (tp * pp)\"\n",
    "\n",
    "        num_tensor_model_parallel_groups = (\n",
    "            global_world_size // tensor_model_parallel_size\n",
    "        )\n",
    "\n",
    "        num_pipeline_model_parallel_groups = (\n",
    "            global_world_size // pipeline_model_parallel_size\n",
    "        )\n",
    "\n",
    "        # 1. initialize data parallel group\n",
    "        self._initialize_data_parallel_group(\n",
    "            current_rank=current_rank,\n",
    "            tensor_model_parallel_size=tensor_model_parallel_size,\n",
    "            pipeline_model_parallel_size=pipeline_model_parallel_size,\n",
    "            num_pipeline_model_parallel_groups=num_pipeline_model_parallel_groups,\n",
    "        )\n",
    "\n",
    "        # 2. initialize tensor model parallel group\n",
    "        self._initialize_tensor_model_parallel_group(\n",
    "            current_rank=current_rank,\n",
    "            tensor_model_parallel_size=tensor_model_parallel_size,\n",
    "            num_tensor_model_parallel_groups=num_tensor_model_parallel_groups,\n",
    "        )\n",
    "\n",
    "        # 3. initialize pipeline model parallel group\n",
    "        self._initialize_pipeline_model_parallel_group(\n",
    "            current_rank=current_rank,\n",
    "            global_world_size=global_world_size,\n",
    "            num_pipeline_model_parallel_groups=num_pipeline_model_parallel_groups,\n",
    "        )\n",
    "\n",
    "        # 4. create distributed functions\n",
    "        functions = self._initialize_functions()\n",
    "        self._broadcast_fn = functions[\"broadcast\"]\n",
    "        self._reduce_fn = functions[\"reduce\"]\n",
    "        self._scatter_fn = functions[\"scatter\"]\n",
    "        self._gather_fn = functions[\"gather\"]\n",
    "\n",
    "    def _initialize_data_parallel_group(\n",
    "        self,\n",
    "        current_rank: int,\n",
    "        tensor_model_parallel_size: int,\n",
    "        pipeline_model_parallel_size: int,\n",
    "        num_pipeline_model_parallel_groups: int,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize data parallel group\n",
    "\n",
    "        Args:\n",
    "            current_rank (int): current rank\n",
    "            tensor_model_parallel_size (int): tensor model parallel world size\n",
    "            pipeline_model_parallel_size (int): pipeline model parallel world size\n",
    "            num_pipeline_model_parallel_groups (int): the number of pipeline model parallel groups\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self._data_parallel_group is None\n",
    "        ), \"data parallel group is already initialized.\"\n",
    "\n",
    "        for i in range(pipeline_model_parallel_size):\n",
    "            start_rank = i * num_pipeline_model_parallel_groups\n",
    "            end_rank = (i + 1) * num_pipeline_model_parallel_groups\n",
    "\n",
    "            for j in range(tensor_model_parallel_size):\n",
    "                ranks = list(\n",
    "                    range(start_rank + j, end_rank, tensor_model_parallel_size)\n",
    "                )\n",
    "\n",
    "                group = dist.new_group(ranks)\n",
    "                if current_rank in ranks:\n",
    "                    self._data_parallel_group = group\n",
    "\n",
    "    def _initialize_tensor_model_parallel_group(\n",
    "        self,\n",
    "        current_rank: int,\n",
    "        tensor_model_parallel_size: int,\n",
    "        num_tensor_model_parallel_groups: int,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize tensor model parallel group\n",
    "\n",
    "        Args:\n",
    "            current_rank (int): current rank\n",
    "            tensor_model_parallel_size (int): tensor model parallel world size\n",
    "            num_tensor_model_parallel_groups (int): the number of tensor model parallel groups\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self._tensor_model_parallel_group is None\n",
    "        ), \"tensor model parallel group is already initialized.\"\n",
    "\n",
    "        for i in range(num_tensor_model_parallel_groups):\n",
    "            start_rank = i * tensor_model_parallel_size\n",
    "            end_rank = (i + 1) * tensor_model_parallel_size\n",
    "\n",
    "            ranks = list(range(start_rank, end_rank))\n",
    "            group = dist.new_group(ranks)\n",
    "\n",
    "            if current_rank in ranks:\n",
    "                self._tensor_model_parallel_group = group\n",
    "\n",
    "    def _initialize_pipeline_model_parallel_group(\n",
    "        self,\n",
    "        current_rank: int,\n",
    "        global_world_size: int,\n",
    "        num_pipeline_model_parallel_groups: int,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize pipeline model parallel group\n",
    "\n",
    "        Args:\n",
    "            current_rank (int): current rank\n",
    "            global_world_size (int): global world size\n",
    "            num_pipeline_model_parallel_groups (int): the number of model parallel groups\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self._pipeline_model_parallel_group is None\n",
    "        ), \"pipeline model parallel group is already initialized.\"\n",
    "\n",
    "        for i in range(num_pipeline_model_parallel_groups):\n",
    "            ranks = list(\n",
    "                range(i, global_world_size, num_pipeline_model_parallel_groups)\n",
    "            )\n",
    "\n",
    "            group = dist.new_group(ranks)\n",
    "\n",
    "            if current_rank in ranks:\n",
    "                self._pipeline_model_parallel_group = group\n",
    "                self._pipeline_global_ranks = ranks\n",
    "\n",
    "    def model_parallel_is_initialized(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if model and data parallel groups are initialized.\n",
    "\n",
    "        Returns:\n",
    "            bool: whether MPU is initialized\n",
    "        \"\"\"\n",
    "        if (\n",
    "            self._tensor_model_parallel_group is None\n",
    "            or self._pipeline_model_parallel_group is None\n",
    "            or self._data_parallel_group is None\n",
    "        ):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def get_model_parallel_group(self):\n",
    "        \"\"\"\n",
    "        Get the tensor model parallel group.\n",
    "\n",
    "        Notes:\n",
    "            This method existed in the old version of Megatron-LM. It is the same as `get_tensor_model_parallel_group()`,\n",
    "            But we must support backward compatibility because this method is invoked by libraries such as DeepSpeed.\n",
    "\n",
    "        Returns:\n",
    "            ProcessGroup: tensor model parallel group\n",
    "        \"\"\"\n",
    "        return self.get_tensor_model_parallel_group()\n",
    "\n",
    "    def get_model_parallel_world_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the tensor model parallel world size\n",
    "\n",
    "        Notes:\n",
    "            This method existed in the old version of Megatron-LM. It is the same as `get_tensor_model_parallel_world_size()`,\n",
    "            But we must support backward compatibility because this method is invoked by libraries such as DeepSpeed.\n",
    "\n",
    "        Returns:\n",
    "            int: tensor model parallel world size\n",
    "        \"\"\"\n",
    "        return self.get_tensor_model_parallel_world_size()\n",
    "\n",
    "    def get_model_parallel_rank(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the tensor model parallel rank\n",
    "\n",
    "        Notes:\n",
    "            This method existed in the old version of Megatron-LM. It is the same as `get_tensor_model_parallel_rank()`,\n",
    "            But we must support backward compatibility because this method is invoked by libraries such as DeepSpeed.\n",
    "\n",
    "        Returns:\n",
    "            int: tensor model parallel world size\n",
    "        \"\"\"\n",
    "        return self.get_tensor_model_parallel_rank()\n",
    "\n",
    "    def get_tensor_model_parallel_group(self):\n",
    "        \"\"\"\n",
    "        Get tensor model parallel group\n",
    "\n",
    "        Returns:\n",
    "            ProcessGroup: tensor model parallel group\n",
    "        \"\"\"\n",
    "\n",
    "        assert (\n",
    "            self._tensor_model_parallel_group is not None\n",
    "        ), \"tensor model parallel group is not initialized.\"\n",
    "\n",
    "        return self._tensor_model_parallel_group\n",
    "\n",
    "    def get_pipeline_model_parallel_group(self):\n",
    "        \"\"\"\n",
    "        Get pipeline model parallel group\n",
    "\n",
    "        Returns:\n",
    "            ProcessGroup: pipeline model parallel group\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self._pipeline_model_parallel_group is not None\n",
    "        ), \"pipeline model parallel group is not initialized.\"\n",
    "\n",
    "        return self._pipeline_model_parallel_group\n",
    "\n",
    "    def get_data_parallel_group(self):\n",
    "        assert (\n",
    "            self._data_parallel_group is not None\n",
    "        ), \"data parallel group is not initialized.\"\n",
    "\n",
    "        return self._data_parallel_group\n",
    "\n",
    "    def get_tensor_model_parallel_world_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get tensor model parallel world size\n",
    "\n",
    "        Returns:\n",
    "            int: tensor model parallel world size\n",
    "        \"\"\"\n",
    "        if self._tensor_model_parallel_world_size is not None:\n",
    "            return self._tensor_model_parallel_world_size\n",
    "\n",
    "        return dist.get_world_size(self.get_tensor_model_parallel_group())\n",
    "\n",
    "    def set_tensor_model_parallel_world_size(self, world_size: int) -> None:\n",
    "        \"\"\"\n",
    "        Set tensor model parallel world size\n",
    "\n",
    "        Args:\n",
    "            world_size (int): tensor model parallel world size\n",
    "        \"\"\"\n",
    "        self._tensor_model_parallel_world_size = world_size\n",
    "\n",
    "    def get_pipeline_model_parallel_world_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get pipeline model parallel world size\n",
    "\n",
    "        Returns:\n",
    "            int: pipeline model parallel world size\n",
    "        \"\"\"\n",
    "        if self._pipeline_model_parallel_world_size is not None:\n",
    "            return self._pipeline_model_parallel_world_size\n",
    "\n",
    "        return dist.get_world_size(self.get_pipeline_model_parallel_group())\n",
    "\n",
    "    def set_pipeline_model_parallel_world_size(self, world_size: int) -> None:\n",
    "        \"\"\"\n",
    "        Set pipeline model parallel world size\n",
    "\n",
    "        Args:\n",
    "            world_size (int): pipeline model parallel world size\n",
    "        \"\"\"\n",
    "        self._pipeline_model_parallel_world_size = world_size\n",
    "\n",
    "    def get_tensor_model_parallel_rank(self) -> int:\n",
    "        \"\"\"\n",
    "        Get tensor model parallel rank\n",
    "\n",
    "        Returns:\n",
    "            int: tensor model parallel rank\n",
    "        \"\"\"\n",
    "        if self._tensor_model_parallel_rank is not None:\n",
    "            return self._tensor_model_parallel_rank\n",
    "\n",
    "        return dist.get_rank(self.get_tensor_model_parallel_group())\n",
    "\n",
    "    def set_tensor_model_parallel_rank(self, rank: int) -> None:\n",
    "        \"\"\"\n",
    "        Set tensor model parallel rank\n",
    "\n",
    "        Args:\n",
    "            rank (int): tensor model parallel rank\n",
    "        \"\"\"\n",
    "\n",
    "        self._tensor_model_parallel_rank = rank\n",
    "\n",
    "    def get_pipeline_model_parallel_rank(self) -> int:\n",
    "        \"\"\"\n",
    "        Get pipeline model parallel rank\n",
    "\n",
    "        Returns:\n",
    "            int: pipeline model parallel rank\n",
    "        \"\"\"\n",
    "        if self._pipeline_model_parallel_rank is not None:\n",
    "            return self._pipeline_model_parallel_rank\n",
    "\n",
    "        return dist.get_rank(self.get_pipeline_model_parallel_group())\n",
    "\n",
    "    def set_pipeline_model_parallel_rank(self, rank: int) -> None:\n",
    "        \"\"\"\n",
    "        Set pipeline model parallel rank\n",
    "\n",
    "        Args:\n",
    "            rank (int): pipeline model parallel rank\n",
    "        \"\"\"\n",
    "\n",
    "        self._pipeline_model_parallel_rank = rank\n",
    "\n",
    "    def is_pipeline_fist_stage(self) -> bool:\n",
    "        \"\"\"\n",
    "        Return `True` if in the first pipeline model parallel stage, `False` otherwise\n",
    "\n",
    "        Returns:\n",
    "            bool: whether current pipeline model parallel stage is first\n",
    "        \"\"\"\n",
    "        return self.get_pipeline_model_parallel_rank() == 0\n",
    "\n",
    "    def is_pipeline_last_stage(self) -> bool:\n",
    "        \"\"\"\n",
    "        Return `True` if in the last pipeline model parallel stage, `False` otherwise\n",
    "\n",
    "        Returns:\n",
    "            bool: whether current pipeline model parallel stage is last\n",
    "        \"\"\"\n",
    "        return self.get_pipeline_model_parallel_rank() == (\n",
    "            self.get_pipeline_model_parallel_world_size() - 1\n",
    "        )\n",
    "\n",
    "    def get_tensor_model_parallel_src_rank(self) -> int:\n",
    "        \"\"\"\n",
    "        Calculate the global rank corresponding to the first local rank in the tensor model parallel group.\n",
    "\n",
    "        Returns:\n",
    "            int: tensor model parallel source rank\n",
    "        \"\"\"\n",
    "\n",
    "        global_rank = dist.get_rank()\n",
    "        local_world_size = self.get_tensor_model_parallel_world_size()\n",
    "        return (global_rank // local_world_size) * local_world_size\n",
    "\n",
    "    def get_pipeline_model_parallel_fist_rank(self):\n",
    "        \"\"\"\n",
    "        Get the first pipeline model parallel rank\n",
    "\n",
    "        Returns:\n",
    "            int: the first pipeline model parallel rank\n",
    "        \"\"\"\n",
    "        return self._pipeline_global_ranks[0]\n",
    "\n",
    "    def get_pipeline_model_parallel_last_rank(self):\n",
    "        \"\"\"\n",
    "        Get the last pipeline model parallel rank\n",
    "\n",
    "        Returns:\n",
    "            int: the last pipeline model parallel rank\n",
    "        \"\"\"\n",
    "        return self._pipeline_global_ranks[\n",
    "            self.get_pipeline_model_parallel_world_size() - 1\n",
    "        ]\n",
    "\n",
    "    def get_pipeline_model_parallel_next_rank(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the next pipeline model parallel rank comparison with current stage.\n",
    "\n",
    "        Returns:\n",
    "            int: the next pipeline model parallel rank\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self._pipeline_global_ranks is not None\n",
    "        ), \"pipeline model parallel group is not initialized.\"\n",
    "\n",
    "        rank_in_pipe = self.get_pipeline_model_parallel_rank()\n",
    "        world_size = self.get_pipeline_model_parallel_world_size()\n",
    "        return self._pipeline_global_ranks[(rank_in_pipe + 1) % world_size]\n",
    "\n",
    "    def get_pipeline_model_parallel_prev_rank(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the previous pipeline model parallel rank comparison with current stage.\n",
    "\n",
    "        Returns:\n",
    "            int: the previous pipeline model parallel rank\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self._pipeline_global_ranks is not None\n",
    "        ), \"pipeline model parallel group is not initialized.\"\n",
    "\n",
    "        rank_in_pipe = self.get_pipeline_model_parallel_rank()\n",
    "        world_size = self.get_pipeline_model_parallel_world_size()\n",
    "        return self._pipeline_global_ranks[(rank_in_pipe - 1) % world_size]\n",
    "\n",
    "    def get_data_parallel_world_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Get data parallel world size\n",
    "\n",
    "        Returns:\n",
    "            int: data parallel world size\n",
    "        \"\"\"\n",
    "\n",
    "        return dist.get_world_size(self.get_data_parallel_group())\n",
    "\n",
    "    def get_data_parallel_rank(self) -> int:\n",
    "        \"\"\"\n",
    "        Get data parallel rank\n",
    "\n",
    "        Returns:\n",
    "            int: data parallel rank\n",
    "        \"\"\"\n",
    "        return dist.get_rank(self.get_data_parallel_group())\n",
    "\n",
    "    def destroy_model_parallel(self) -> None:\n",
    "        \"\"\"\n",
    "        Destroy all the model parallel groups\n",
    "        \"\"\"\n",
    "\n",
    "        self._tensor_model_parallel_group = None\n",
    "        self._pipeline_model_parallel_group = None\n",
    "        self._data_parallel_group = None\n",
    "\n",
    "    def _broadcast(self, inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Pass the input to the model parallel region.\n",
    "\n",
    "        Args:\n",
    "            inputs (Tensor): input tensor\n",
    "\n",
    "        Returns:\n",
    "            Tensor: broadcast tensor\n",
    "        \"\"\"\n",
    "        return inputs.clone()\n",
    "\n",
    "    def _reduce(self, inputs: Tensor):\n",
    "        \"\"\"\n",
    "        All-reduce the input tensor across tensor model parallel group.\n",
    "\n",
    "        Args:\n",
    "            inputs (Tensor): input tensor\n",
    "\n",
    "        Returns:\n",
    "            Tensor: all-reduced tensor\n",
    "        \"\"\"\n",
    "        if self.get_tensor_model_parallel_world_size() == 1:\n",
    "            return inputs\n",
    "\n",
    "        dist.all_reduce(inputs, group=self.get_tensor_model_parallel_group())\n",
    "        return inputs\n",
    "\n",
    "    def _scatter(self, inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Split the tensor along its last dimension and keep the corresponding slice.\n",
    "\n",
    "        Args:\n",
    "            inputs (Tensor): input tensor\n",
    "\n",
    "        Returns:\n",
    "            Tensor: scattered tensor\n",
    "        \"\"\"\n",
    "        world_size = self.get_tensor_model_parallel_world_size()\n",
    "\n",
    "        if world_size == 1:\n",
    "            return inputs\n",
    "\n",
    "        last_dim = inputs.dim() - 1\n",
    "        last_dim_size = inputs.size()[last_dim] // world_size\n",
    "\n",
    "        inputs_list = torch.split(\n",
    "            tensor=inputs,\n",
    "            split_size_or_sections=last_dim_size,\n",
    "            dim=last_dim,\n",
    "        )\n",
    "\n",
    "        rank = self.get_tensor_model_parallel_rank()\n",
    "        outputs = inputs_list[rank].contiguous()\n",
    "        return outputs\n",
    "\n",
    "    def _gather(self, inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Gather tensors and concatenate along the last dimension\n",
    "\n",
    "        Args:\n",
    "            inputs (Tensor): input tensor\n",
    "\n",
    "        Returns:\n",
    "            Tensor: gathered tensor\n",
    "        \"\"\"\n",
    "        world_size = self.get_tensor_model_parallel_world_size()\n",
    "\n",
    "        if world_size == 1:\n",
    "            return inputs\n",
    "\n",
    "        last_dim = inputs.dim() - 1\n",
    "        rank = self.get_tensor_model_parallel_rank()\n",
    "\n",
    "        tensor_list = [torch.empty_like(inputs) for _ in range(world_size)]\n",
    "        tensor_list[rank] = inputs\n",
    "        torch.distributed.all_gather(\n",
    "            tensor_list, inputs, group=self.get_tensor_model_parallel_group()\n",
    "        )\n",
    "        outputs = torch.cat(tensor_list, dim=last_dim).contiguous()\n",
    "        return outputs\n",
    "\n",
    "    def broadcast(self, inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Pass the input to the model parallel region.\n",
    "\n",
    "        Args:\n",
    "            inputs (Tensor):\n",
    "\n",
    "        Returns:\n",
    "            Tensor: broadcast tensor\n",
    "        \"\"\"\n",
    "\n",
    "        if self._enable_grad(inputs):\n",
    "            outputs = self._broadcast_fn.apply(inputs)\n",
    "        else:\n",
    "            outputs = self._broadcast(inputs)\n",
    "        return outputs\n",
    "\n",
    "    def reduce(self, inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        All-reduce the input tensor across tensor model parallel group.\n",
    "\n",
    "        Args:\n",
    "            inputs (Tensor): input tensor\n",
    "\n",
    "        Returns:\n",
    "            Tensor: all-reduced tensor\n",
    "        \"\"\"\n",
    "\n",
    "        if self._enable_grad(inputs):\n",
    "            outputs = self._reduce_fn.apply(inputs)\n",
    "        else:\n",
    "            outputs = self._reduce(inputs)\n",
    "        return outputs\n",
    "\n",
    "    def scatter(self, inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Split the tensor along its last dimension and keep the corresponding slice.\n",
    "\n",
    "        Args:\n",
    "            inputs (Tensor): input tensor\n",
    "\n",
    "        Returns:\n",
    "            Tensor: scattered tensor\n",
    "        \"\"\"\n",
    "\n",
    "        if self._enable_grad(inputs):\n",
    "            outputs = self._scatter_fn.apply(inputs)\n",
    "        else:\n",
    "            outputs = self._scatter(inputs)\n",
    "        return outputs\n",
    "\n",
    "    def gather(self, inputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Gather tensors and concatenate along the last dimension\n",
    "\n",
    "        Args:\n",
    "            inputs (Tensor): input tensor\n",
    "\n",
    "        Returns:\n",
    "            Tensor: gathered tensor\n",
    "        \"\"\"\n",
    "\n",
    "        if self._enable_grad(inputs):\n",
    "            outputs = self._gather_fn.apply(inputs)\n",
    "        else:\n",
    "            outputs = self._gather(inputs)\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def _enable_grad(inputs: Tensor) -> bool:\n",
    "        \"\"\"\n",
    "        Check current tensor is enabled to pass gradient.\n",
    "\n",
    "        Args:\n",
    "            inputs (Tensor): input tensor\n",
    "\n",
    "        Returns:\n",
    "            bool: whether gradient can be passed or not\n",
    "        \"\"\"\n",
    "        return torch.is_grad_enabled() and inputs.requires_grad\n",
    "\n",
    "    def _initialize_functions(self):\n",
    "        class Broadcast(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, inputs):\n",
    "                return self._broadcast(inputs)\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, inputs):\n",
    "                return self._reduce(inputs)\n",
    "\n",
    "        class Reduce(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, inputs):\n",
    "                return self._reduce(inputs)\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, inputs):\n",
    "                return self._broadcast(inputs)\n",
    "\n",
    "        class Scatter(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, inputs):\n",
    "                return self._scatter(inputs)\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, inputs):\n",
    "                return self._gather(inputs)\n",
    "\n",
    "        class Gather(Function):\n",
    "            @staticmethod\n",
    "            def forward(ctx, inputs):\n",
    "                return self._gather(inputs)\n",
    "\n",
    "            @staticmethod\n",
    "            def backward(ctx, inputs):\n",
    "                return self._scatter(inputs)\n",
    "\n",
    "        return {\n",
    "            \"broadcast\": Broadcast,\n",
    "            \"reduce\": Reduce,\n",
    "            \"scatter\": Scatter,\n",
    "            \"gather\": Gather,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_distributed(backend, master_port):\n",
    "        \"\"\"Initialize torch.distributed and mpu.\"\"\"\n",
    "        if not torch.distributed.is_initialized():\n",
    "            rank = int(os.getenv(\"RANK\", 0))\n",
    "            world_size = int(os.getenv(\"WORLD_SIZE\", 1))\n",
    "            os.environ[\"MASTER_PORT\"] = str(master_port)\n",
    "            device_count = torch.cuda.device_count()\n",
    "\n",
    "            if device_count > 0:\n",
    "                device = rank % device_count\n",
    "                torch.cuda.set_device(device)\n",
    "\n",
    "            torch.distributed.init_process_group(\n",
    "                backend=backend,\n",
    "                world_size=world_size,\n",
    "                rank=rank,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49c1d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/test_mpu.py\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from mpu import MPU\n",
    "\n",
    "mpu = MPU(\n",
    "    tensor_model_parallel_size=2,\n",
    "    pipeline_model_parallel_size=2,\n",
    "    backend=\"nccl\",\n",
    "    master_port=5678,\n",
    ")\n",
    "\n",
    "# 1. MPU는 다음과 같이 프로세스 그룹을 자동으로 생성하고 그들에 접근할 수 있는 메서드를 제공합니다.\n",
    "print(f\"TP group: {mpu.get_tensor_model_parallel_group()}\")\n",
    "print(f\"TP wsz: {mpu.get_tensor_model_parallel_world_size()}\")\n",
    "print(f\"TP rank: {mpu.get_tensor_model_parallel_rank()}\")\n",
    "dist.barrier()\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"PP group: {mpu.get_pipeline_model_parallel_group()}\")\n",
    "print(f\"PP wsz: {mpu.get_pipeline_model_parallel_world_size()}\")\n",
    "print(f\"PP rank: {mpu.get_pipeline_model_parallel_rank()}\")\n",
    "dist.barrier()\n",
    "print(\"\\n\")\n",
    "\n",
    "# 2. Data parallel size는 TP와 PP 사이즈에 맞게 알아서 설정됩니다.\n",
    "# 만약 16대의 GPU에서 TP=4, PP=1을 설정했다면 16 / (4 * 1) = 4로 자동으로 DP size는 4가 됩니다.\n",
    "print(f\"DP group: {mpu.get_data_parallel_group()}\")\n",
    "print(f\"DP wsz: {mpu.get_data_parallel_world_size()}\")\n",
    "print(f\"DP rank: {mpu.get_data_parallel_rank()}\")\n",
    "dist.barrier()\n",
    "print(\"\\n\")\n",
    "\n",
    "# 3. MPU는 reduce, scatter, gather, braodcast 등의 연산을 지원합니다.\n",
    "# 이들은 대부분 Tensor parallel group에서만 사용되기 때문에 Tensor parallel group을 기본으로 설장해뒀습니다.\n",
    "a = torch.tensor([2, 3, 4, 5]).cuda() * dist.get_rank()\n",
    "a = mpu.reduce(a)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd34a5d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-10-28 00:29:04,099] [WARNING] [runner.py:122:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2021-10-28 00:29:04,275] [INFO] [runner.py:360:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 ../src/test_mpu.py\n",
      "[2021-10-28 00:29:05,352] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\n",
      "[2021-10-28 00:29:05,352] [INFO] [launch.py:89:main] nnodes=1, num_local_procs=4, node_rank=0\n",
      "[2021-10-28 00:29:05,352] [INFO] [launch.py:101:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\n",
      "[2021-10-28 00:29:05,352] [INFO] [launch.py:102:main] dist_world_size=4\n",
      "[2021-10-28 00:29:05,352] [INFO] [launch.py:105:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "0: TP group: <torch.distributed.ProcessGroupNCCL object at 0x7f29d30bed50>\n",
      "3: TP group: <torch.distributed.ProcessGroupNCCL object at 0x7fe4dcd0fd50>\n",
      "2: TP group: <torch.distributed.ProcessGroupNCCL object at 0x7fafec84ed50>\n",
      "1: TP group: <torch.distributed.ProcessGroupNCCL object at 0x7fa4dbbeed50>\n",
      "0: TP wsz: 2\n",
      "3: TP wsz: 2\n",
      "2: TP wsz: 2\n",
      "1: TP wsz: 2\n",
      "0: TP rank: 0\n",
      "3: TP rank: 1\n",
      "2: TP rank: 0\n",
      "1: TP rank: 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0: PP group: <torch.distributed.ProcessGroupNCCL object at 0x7f29d30bedc0>\n",
      "3: PP group: <torch.distributed.ProcessGroupNCCL object at 0x7fe4dcd0fdc0>\n",
      "2: PP group: <torch.distributed.ProcessGroupNCCL object at 0x7fafec84edc0>\n",
      "1: PP group: <torch.distributed.ProcessGroupNCCL object at 0x7fa4dbbeedc0>\n",
      "0: PP wsz: 2\n",
      "3: PP wsz: 2\n",
      "2: PP wsz: 2\n",
      "1: PP wsz: 2\n",
      "3: PP rank: 1\n",
      "0: PP rank: 0\n",
      "2: PP rank: 1\n",
      "1: PP rank: 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0: DP group: <torch.distributed.ProcessGroupNCCL object at 0x7f29d30bece0>\n",
      "1: DP group: <torch.distributed.ProcessGroupNCCL object at 0x7fa4dbbeece0>\n",
      "2: DP group: <torch.distributed.ProcessGroupNCCL object at 0x7fafec84ece0>\n",
      "3: DP group: <torch.distributed.ProcessGroupNCCL object at 0x7fe4dcd0fce0>\n",
      "0: DP wsz: 1\n",
      "1: DP wsz: 1\n",
      "2: DP wsz: 1\n",
      "3: DP wsz: 1\n",
      "0: DP rank: 0\n",
      "1: DP rank: 0\n",
      "2: DP rank: 0\n",
      "3: DP rank: 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1: all-reduce => tensor([2, 3, 4, 5], device='cuda:1')\n",
      "0: all-reduce => tensor([2, 3, 4, 5], device='cuda:0')\n",
      "3: all-reduce => tensor([10, 15, 20, 25], device='cuda:3')\n",
      "2: all-reduce => tensor([10, 15, 20, 25], device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "!deepspeed --num_gpus=4 ../src/test_mpu.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef160c02",
   "metadata": {},
   "source": [
    "## 3. Large-scale 프로젝트 소개\n",
    "\n",
    "현재 거의 모든 빅모델 관련 프로젝트는 Megatron-LM을 기반으로 하고 있습니다. Tensor parallelism 세션에서 이미 Megatron-LM의 실습을 진행했기 때문에 이번 세션에서는 따로 실습을 진행하지는 않겠습니다. 지금부터 소개 드릴 3개의 프로젝트 모두 Megatron-LM의 Fork 레포지토리이기 때문에 사용법이 거의 동일하고 Argument 등만 변경된 것이 대부분입니다. 각각 프로젝트 소개를 읽어보시고 적합하다 싶은 프로젝트의 레포를 클론하셔서 사용하시길 바랍니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1) Megatron-DeepSpeed\n",
    "\n",
    "- Maintainer: Shaden, Jeff, ... DeepSpeed Team\n",
    "- [Megatron 최신버전 + DeepSpeed](https://github.com/microsoft/Megatron-DeepSpeed): Upstream 유지됨\n",
    "- [3D parallelism (Megatron 1.1.5) + ZeRO-1](https://github.com/microsoft/DeepSpeedExamples/tree/c1b206c137bb028fc8124fd7d434c2da10efc033/Megatron-LM-v1.1.5-3D_parallelism/megatron): Upstream 유지하지 않음\n",
    "- [2D parallelism (Megatron 1.1.5) + ZeRO-3](https://github.com/microsoft/DeepSpeedExamples/tree/c1b206c137bb028fc8124fd7d434c2da10efc033/Megatron-LM-v1.1.5-ZeRO3): Upstream 유지하지 않음\n",
    "\n",
    "\n",
    "Megatron-LM에 DeepSpeed ZeRO를 추가한 프로젝트입니다. 최근에 NVIDIA-Microsoft가 함께 개발했다고 알려진 530B의 Megatron-Turing의 코드 베이스로 예상됩니다. DeepSpeed **ZeRO 역시 다차원 병렬화의 대상이며 DP 대신 ZeRO-DP를 적용**할 수 있습니다. 그러나 ZeRO 2,3와 Pipeline 병렬화는 호환되지 않습니다. 따라서 **ZeRO 2,3를 사용하려면 Pipeline 병렬화를 적용하지 않아야** 합니다. \n",
    "\n",
    "따라서 (ZeRO-2,3 + TP) 혹은 (ZeRO-1 + TP + PP) 중 한가지를 선택을 해야 합니다. 만약 모델의 크기가 극도로 크다면 ZeRO-3 등을 고려해도 좋지만 현재까지는 3D Parallelism + ZeRO-1의 효율이 좋은 것으로 알려져있습니다. 그 이유는 Tensor parallelism이 노드내에서 통신할 때는 빠르지만 노드간 통신으로 가면 급격하게 느려지기 때문입니다. \n",
    "\n",
    "![](../images/megatron_3d.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "대부분의 경우 위 그림처럼 노드내에서의 병렬화는 TP를 사용하고 노드간 통신은 PP를 사용하며 추가로 ZeRO-1을 적용해서 사용합니다. 그러나 ZeRO 논문에서 언급된 것 처럼 동일한 리소스를 사용할 때 학습 가능한 모델의 최대 크기는 ZeRO가 훨씬 크기 때문에 현재 리소스와 학습하고자 하는 모델 사이즈, 서버의 구성에 맞춰서 병렬화 전략을 잘 선택하시길 바랍니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2) GPT-NeoX\n",
    "\n",
    "![](../images/eleuther_ai.png)\n",
    "\n",
    "- Maintainer: Stella, ... EleutherAI Team\n",
    "- https://github.com/EleutherAI/gpt-neox: Upstream 유지하지 않음\n",
    "- https://github.com/EleutherAI/deeperspeed: Upstream 유지하지 않음\n",
    "\n",
    "GPT-NeoX는 GPT-Neo로 유명한 EleutherAI 팀이 만들고 있는 GPU 버전 코드베이스입니다. GPT-NeoX는 **DeepSpeed Examples에 있는 [3D parallelism (Megatron 1.1.5) + ZeRO-1](https://github.com/microsoft/DeepSpeedExamples/tree/c1b206c137bb028fc8124fd7d434c2da10efc033/Megatron-LM-v1.1.5-3D_parallelism/megatron)에 여러가지 모델링 컴포넌트가 추가되어 있는 프로젝트**입니다. 예를 들어 ScaleNorm, RMSNorm, Rotary Embedding, Alibi Embedding, Shampoo Optimizer, SM3 Optimizer 등이 추가되어 있으며 DeepSpeed Sparse Attention 등을 지원하고 있습니다. 이들은 영어 모델을 개발하고 있으며 GPT-NeoX는 **GPT 모델만 구현되어 있으며** 다른 모델 구조 (Bert, T5, ...) 등은 제거되어 있습니다.\n",
    "\n",
    "한가지 눈 여겨 볼 것은 현재 Megatron-LM과의 업스트림이 유지되지 않고 있으며 서로 꽤나 많이 달라졌습니다. Megatron-LM도 지속적으로 업데이트가 되고 있기 때문에 **업스트림이 유지되는지 유지되지 않는지도 참고할 사항**입니다. 또한 GPT-NeoX는 DeeperSpeed라는 DeepSpeed 포크 레포지토리를 개발해서 사용하고 있는데, 역시 업스트림 유지가 되고있지 않습니다. \n",
    "\n",
    "<br>\n",
    "\n",
    "### 3) Big-Science\n",
    "\n",
    "![](../images/big_science.png)\n",
    "\n",
    "- Maintainer: Stas, Wang, ... Hugging Face Team\n",
    "- https://github.com/bigscience-workshop/Megatron-DeepSpeed: Upstream 유지됨\n",
    "\n",
    "Big Science Workshop은 Huggingface에서 코드베이스 개발을 리드하고 프랑스 정부 산하의 컴퓨팅 장비를 사용하여 **멀티링구얼 빅모델**을 개발하고자 하는 1년간의 워크숍입니다. [Megatron 최신버전 + DeepSpeed](https://github.com/microsoft/Megatron-DeepSpeed)의 Fork 레포지토리이며, 가장 최근 개발되기 시작했지만 참여인원이 많아서 가장 빠른속도로 개발되고 있습니다. 이들도 마찬가지로 ScaleNorm, RMSNorm, Rotary Embedding, Alibi Embedding 등의 **최근 성능이 우수하다고 알려진 컴포넌트들을 지속적으로 추가**하고 있으며 **GPT 모델 이외에 Bert, T5, Prefix-LM 등도 운용**하고 있습니다. Big-Science는 Megatron-LM의 업스트림을 유지하고 있다는 특징도 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
