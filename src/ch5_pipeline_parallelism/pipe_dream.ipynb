{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e7a57e-594a-497e-b723-a3541dae223d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-17 22:55:12,774] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-04-17 22:55:12,882] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n"
     ]
    }
   ],
   "source": [
    "import deepspeed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from deepspeed import PipelineModule\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block as GPT2BlockBase\n",
    "import torch.distributed as dist\n",
    "\n",
    "from transformers import DataCollatorWithPadding, get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3539c4f7-8a87-4508-81ce-e35dcd13a425",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14bae05f-4178-44c9-81f3-9dd47538ec79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/87599 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████| 87599/87599 [00:50<00:00, 1729.26it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size, train_steps = 16, 300\n",
    "train_samples = batch_size * train_steps\n",
    "\n",
    "datasets = load_dataset(\"squad\").data[\"train\"][\"context\"]\n",
    "datasets = [str(sample) for i, sample in enumerate(datasets)]\n",
    "datasets = [\n",
    "        tokenizer(data, return_tensors=\"pt\", max_length=1024).input_ids[0]\n",
    "        for data in tqdm(datasets)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8d8dec7-5ae4-4ede-a2f9-c4f851878069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aee0f5b-c471-47e7-a1c6-f616e6971e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([19895,  5712, 20221,    11,   262,  1524,   468,   257,  7835,  2095,\n",
      "           13,  1629,   404,   262,  8774, 11819,   338,  3869, 29500,   318,\n",
      "          257, 10861, 15207,   286,   262,  5283,  5335,    13, 34528,   287,\n",
      "         2166,   286,   262,  8774, 11819,   290,  6476,   340,    11,   318,\n",
      "          257, 15317, 15207,   286,  1951,   351,  5101,   510, 49309,   351,\n",
      "          262,  8177,   366, 37522,   578,  1215,  2185, 16543,  2516,  1911,\n",
      "         7406,   284,   262,  8774, 11819,   318,   262, 32520,  3970,   286,\n",
      "          262, 17380,  8894,    13, 34528,  2157,   262, 37792,  3970,   318,\n",
      "          262, 10299, 33955,    11,   257, 37919,  1295,   286, 11443,   290,\n",
      "        14580,    13,   632,   318,   257, 30069,   286,   262,  7128, 33955,\n",
      "          379,   406,   454,  8906,    11,  4881,   810,   262,  5283,  5335,\n",
      "         1128,  7241,   306,  4120,   284,  9281,  6206,   324,  5857,   311,\n",
      "        12944,   343,   516,   287,  1248,  3365,    13,  1629,   262,   886,\n",
      "          286,   262,  1388,  3708,   357,   392,   287,   257,  1277,  1627,\n",
      "          326, 20417,   832,   513, 25827,   290,   262,  3561, 31390,   828,\n",
      "          318,   257,  2829,    11,  3660,  7815, 15207,   286,  5335,    13]), tensor([19895,  5712, 20221,    11,   262,  1524,   468,   257,  7835,  2095,\n",
      "           13,  1629,   404,   262,  8774, 11819,   338,  3869, 29500,   318,\n",
      "          257, 10861, 15207,   286,   262,  5283,  5335,    13, 34528,   287,\n",
      "         2166,   286,   262,  8774, 11819,   290,  6476,   340,    11,   318,\n",
      "          257, 15317, 15207,   286,  1951,   351,  5101,   510, 49309,   351,\n",
      "          262,  8177,   366, 37522,   578,  1215,  2185, 16543,  2516,  1911,\n",
      "         7406,   284,   262,  8774, 11819,   318,   262, 32520,  3970,   286,\n",
      "          262, 17380,  8894,    13, 34528,  2157,   262, 37792,  3970,   318,\n",
      "          262, 10299, 33955,    11,   257, 37919,  1295,   286, 11443,   290,\n",
      "        14580,    13,   632,   318,   257, 30069,   286,   262,  7128, 33955,\n",
      "          379,   406,   454,  8906,    11,  4881,   810,   262,  5283,  5335,\n",
      "         1128,  7241,   306,  4120,   284,  9281,  6206,   324,  5857,   311,\n",
      "        12944,   343,   516,   287,  1248,  3365,    13,  1629,   262,   886,\n",
      "          286,   262,  1388,  3708,   357,   392,   287,   257,  1277,  1627,\n",
      "          326, 20417,   832,   513, 25827,   290,   262,  3561, 31390,   828,\n",
      "          318,   257,  2829,    11,  3660,  7815, 15207,   286,  5335,    13])]\n"
     ]
    }
   ],
   "source": [
    "print(datasets[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "685a2d12-6358-4c4d-a81c-45fcd3335a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87599"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "221600ec-0193-41ae-bc21-ae2f44101638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5475"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    batch_encoding = tokenizer.pad(\n",
    "        {\"input_ids\": batch}, padding=\"max_length\", max_length=1024\n",
    "    )\n",
    "    return batch_encoding.input_ids\n",
    "\n",
    "\n",
    "data_loader = iter (\n",
    "         DataLoader(\n",
    "            sorted(datasets, key=len, reverse=True),\n",
    "            # uniform length batching\n",
    "            # https://mccormickml.com/2020/07/29/smart-batching-tutorial/\n",
    "            batch_size=batch_size,\n",
    "            num_workers=8,\n",
    "            collate_fn=collate_fn,\n",
    "            shuffle=False,\n",
    "       )\n",
    "    )\n",
    "\n",
    "len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33557380-d33e-4930-b66d-f3d50762142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_dataset(\"squad\").data[\"train\"][\"context\"]\n",
    "datasets = [str(sample) for i, sample in enumerate(datasets) if i < train_samples]\n",
    "#datasets = [\n",
    "#        tokenizer(data, return_tensors=\"pt\", max_length=1024).input_ids[0]\n",
    "#        for data in tqdm(datasets)\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a49754bb-587f-4a40-a1ef-0f44554dde08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset = datasets[:2]\n",
    "sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b33694f-22ba-47e3-a9b5-f49a8bd55c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19895, 5712, 20221, 11, 262, 1524, 468, 257, 7835, 2095, 13, 1629, 404, 262, 8774, 11819, 338, 3869, 29500, 318, 257, 10861, 15207, 286, 262, 5283, 5335, 13, 34528, 287, 2166, 286, 262, 8774, 11819, 290, 6476, 340, 11, 318, 257, 15317, 15207, 286, 1951, 351, 5101, 510, 49309, 351, 262, 8177, 366, 37522, 578, 1215, 2185, 16543, 2516, 1911, 7406, 284, 262, 8774, 11819, 318, 262, 32520, 3970, 286, 262, 17380, 8894, 13, 34528, 2157, 262, 37792, 3970, 318, 262, 10299, 33955, 11, 257, 37919, 1295, 286, 11443, 290, 14580, 13, 632, 318, 257, 30069, 286, 262, 7128, 33955, 379, 406, 454, 8906, 11, 4881, 810, 262, 5283, 5335, 1128, 7241, 306, 4120, 284, 9281, 6206, 324, 5857, 311, 12944, 343, 516, 287, 1248, 3365, 13, 1629, 262, 886, 286, 262, 1388, 3708, 357, 392, 287, 257, 1277, 1627, 326, 20417, 832, 513, 25827, 290, 262, 3561, 31390, 828, 318, 257, 2829, 11, 3660, 7815, 15207, 286, 5335, 13], [19895, 5712, 20221, 11, 262, 1524, 468, 257, 7835, 2095, 13, 1629, 404, 262, 8774, 11819, 338, 3869, 29500, 318, 257, 10861, 15207, 286, 262, 5283, 5335, 13, 34528, 287, 2166, 286, 262, 8774, 11819, 290, 6476, 340, 11, 318, 257, 15317, 15207, 286, 1951, 351, 5101, 510, 49309, 351, 262, 8177, 366, 37522, 578, 1215, 2185, 16543, 2516, 1911, 7406, 284, 262, 8774, 11819, 318, 262, 32520, 3970, 286, 262, 17380, 8894, 13, 34528, 2157, 262, 37792, 3970, 318, 262, 10299, 33955, 11, 257, 37919, 1295, 286, 11443, 290, 14580, 13, 632, 318, 257, 30069, 286, 262, 7128, 33955, 379, 406, 454, 8906, 11, 4881, 810, 262, 5283, 5335, 1128, 7241, 306, 4120, 284, 9281, 6206, 324, 5857, 311, 12944, 343, 516, 287, 1248, 3365, 13, 1629, 262, 886, 286, 262, 1388, 3708, 357, 392, 287, 257, 1277, 1627, 326, 20417, 832, 513, 25827, 290, 262, 3561, 31390, 828, 318, 257, 2829, 11, 3660, 7815, 15207, 286, 5335, 13]]\n"
     ]
    }
   ],
   "source": [
    "token = [tokenizer(data).input_ids for data in sample_dataset]\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4b043b2-339a-4e54-a5a0-afa52d2f4c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[19895,  5712, 20221,    11,   262,  1524,   468,   257,  7835,  2095,\n",
      "            13,  1629,   404,   262,  8774, 11819,   338,  3869, 29500,   318,\n",
      "           257, 10861, 15207,   286,   262,  5283,  5335,    13, 34528,   287,\n",
      "          2166,   286,   262,  8774, 11819,   290,  6476,   340,    11,   318,\n",
      "           257, 15317, 15207,   286,  1951,   351,  5101,   510, 49309,   351,\n",
      "           262,  8177,   366, 37522,   578,  1215,  2185, 16543,  2516,  1911,\n",
      "          7406,   284,   262,  8774, 11819,   318,   262, 32520,  3970,   286,\n",
      "           262, 17380,  8894,    13, 34528,  2157,   262, 37792,  3970,   318,\n",
      "           262, 10299, 33955,    11,   257, 37919,  1295,   286, 11443,   290,\n",
      "         14580,    13,   632,   318,   257, 30069,   286,   262,  7128, 33955,\n",
      "           379,   406,   454,  8906,    11,  4881,   810,   262,  5283,  5335,\n",
      "          1128,  7241,   306,  4120,   284,  9281,  6206,   324,  5857,   311,\n",
      "         12944,   343,   516,   287,  1248,  3365,    13,  1629,   262,   886,\n",
      "           286,   262,  1388,  3708,   357,   392,   287,   257,  1277,  1627,\n",
      "           326, 20417,   832,   513, 25827,   290,   262,  3561, 31390,   828,\n",
      "           318,   257,  2829,    11,  3660,  7815, 15207,   286,  5335,    13]]), tensor([[19895,  5712, 20221,    11,   262,  1524,   468,   257,  7835,  2095,\n",
      "            13,  1629,   404,   262,  8774, 11819,   338,  3869, 29500,   318,\n",
      "           257, 10861, 15207,   286,   262,  5283,  5335,    13, 34528,   287,\n",
      "          2166,   286,   262,  8774, 11819,   290,  6476,   340,    11,   318,\n",
      "           257, 15317, 15207,   286,  1951,   351,  5101,   510, 49309,   351,\n",
      "           262,  8177,   366, 37522,   578,  1215,  2185, 16543,  2516,  1911,\n",
      "          7406,   284,   262,  8774, 11819,   318,   262, 32520,  3970,   286,\n",
      "           262, 17380,  8894,    13, 34528,  2157,   262, 37792,  3970,   318,\n",
      "           262, 10299, 33955,    11,   257, 37919,  1295,   286, 11443,   290,\n",
      "         14580,    13,   632,   318,   257, 30069,   286,   262,  7128, 33955,\n",
      "           379,   406,   454,  8906,    11,  4881,   810,   262,  5283,  5335,\n",
      "          1128,  7241,   306,  4120,   284,  9281,  6206,   324,  5857,   311,\n",
      "         12944,   343,   516,   287,  1248,  3365,    13,  1629,   262,   886,\n",
      "           286,   262,  1388,  3708,   357,   392,   287,   257,  1277,  1627,\n",
      "           326, 20417,   832,   513, 25827,   290,   262,  3561, 31390,   828,\n",
      "           318,   257,  2829,    11,  3660,  7815, 15207,   286,  5335,    13]])]\n"
     ]
    }
   ],
   "source": [
    "token = [tokenizer(data, return_tensors=\"pt\").input_ids for data in sample_dataset]\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c7691e5-5c1f-492a-8e72-9245050973bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([19895,  5712, 20221,    11,   262,  1524,   468,   257,  7835,  2095,\n",
      "           13,  1629,   404,   262,  8774, 11819,   338,  3869, 29500,   318,\n",
      "          257, 10861, 15207,   286,   262,  5283,  5335,    13, 34528,   287,\n",
      "         2166,   286,   262,  8774, 11819,   290,  6476,   340,    11,   318,\n",
      "          257, 15317, 15207,   286,  1951,   351,  5101,   510, 49309,   351,\n",
      "          262,  8177,   366, 37522,   578,  1215,  2185, 16543,  2516,  1911,\n",
      "         7406,   284,   262,  8774, 11819,   318,   262, 32520,  3970,   286,\n",
      "          262, 17380,  8894,    13, 34528,  2157,   262, 37792,  3970,   318,\n",
      "          262, 10299, 33955,    11,   257, 37919,  1295,   286, 11443,   290,\n",
      "        14580,    13,   632,   318,   257, 30069,   286,   262,  7128, 33955,\n",
      "          379,   406,   454,  8906,    11,  4881,   810,   262,  5283,  5335,\n",
      "         1128,  7241,   306,  4120,   284,  9281,  6206,   324,  5857,   311,\n",
      "        12944,   343,   516,   287,  1248,  3365,    13,  1629,   262,   886,\n",
      "          286,   262,  1388,  3708,   357,   392,   287,   257,  1277,  1627,\n",
      "          326, 20417,   832,   513, 25827,   290,   262,  3561, 31390,   828,\n",
      "          318,   257,  2829,    11,  3660,  7815, 15207,   286,  5335,    13]), tensor([19895,  5712, 20221,    11,   262,  1524,   468,   257,  7835,  2095,\n",
      "           13,  1629,   404,   262,  8774, 11819,   338,  3869, 29500,   318,\n",
      "          257, 10861, 15207,   286,   262,  5283,  5335,    13, 34528,   287,\n",
      "         2166,   286,   262,  8774, 11819,   290,  6476,   340,    11,   318,\n",
      "          257, 15317, 15207,   286,  1951,   351,  5101,   510, 49309,   351,\n",
      "          262,  8177,   366, 37522,   578,  1215,  2185, 16543,  2516,  1911,\n",
      "         7406,   284,   262,  8774, 11819,   318,   262, 32520,  3970,   286,\n",
      "          262, 17380,  8894,    13, 34528,  2157,   262, 37792,  3970,   318,\n",
      "          262, 10299, 33955,    11,   257, 37919,  1295,   286, 11443,   290,\n",
      "        14580,    13,   632,   318,   257, 30069,   286,   262,  7128, 33955,\n",
      "          379,   406,   454,  8906,    11,  4881,   810,   262,  5283,  5335,\n",
      "         1128,  7241,   306,  4120,   284,  9281,  6206,   324,  5857,   311,\n",
      "        12944,   343,   516,   287,  1248,  3365,    13,  1629,   262,   886,\n",
      "          286,   262,  1388,  3708,   357,   392,   287,   257,  1277,  1627,\n",
      "          326, 20417,   832,   513, 25827,   290,   262,  3561, 31390,   828,\n",
      "          318,   257,  2829,    11,  3660,  7815, 15207,   286,  5335,    13])]\n"
     ]
    }
   ],
   "source": [
    "token = [tokenizer(data, return_tensors=\"pt\").input_ids[0] for data in sample_dataset]\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00709bda-9fda-4ee3-8d44-b62f1887a6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4800/4800 [00:02<00:00, 1809.30it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size, train_steps = 16, 300\n",
    "train_samples = batch_size * train_steps\n",
    "\n",
    "datasets = load_dataset(\"squad\").data[\"train\"][\"context\"]\n",
    "datasets = [str(sample) for i, sample in enumerate(datasets) if i < train_samples]\n",
    "datasets = [\n",
    "        tokenizer(data, return_tensors=\"pt\", max_length=1024).input_ids[0]\n",
    "        for data in tqdm(datasets)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5feeae8-b39a-4036-bb39-691b79eae3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([19895,  5712, 20221,    11,   262,  1524,   468,   257,  7835,  2095,\n",
       "            13,  1629,   404,   262,  8774, 11819,   338,  3869, 29500,   318,\n",
       "           257, 10861, 15207,   286,   262,  5283,  5335,    13, 34528,   287,\n",
       "          2166,   286,   262,  8774, 11819,   290,  6476,   340,    11,   318,\n",
       "           257, 15317, 15207,   286,  1951,   351,  5101,   510, 49309,   351,\n",
       "           262,  8177,   366, 37522,   578,  1215,  2185, 16543,  2516,  1911,\n",
       "          7406,   284,   262,  8774, 11819,   318,   262, 32520,  3970,   286,\n",
       "           262, 17380,  8894,    13, 34528,  2157,   262, 37792,  3970,   318,\n",
       "           262, 10299, 33955,    11,   257, 37919,  1295,   286, 11443,   290,\n",
       "         14580,    13,   632,   318,   257, 30069,   286,   262,  7128, 33955,\n",
       "           379,   406,   454,  8906,    11,  4881,   810,   262,  5283,  5335,\n",
       "          1128,  7241,   306,  4120,   284,  9281,  6206,   324,  5857,   311,\n",
       "         12944,   343,   516,   287,  1248,  3365,    13,  1629,   262,   886,\n",
       "           286,   262,  1388,  3708,   357,   392,   287,   257,  1277,  1627,\n",
       "           326, 20417,   832,   513, 25827,   290,   262,  3561, 31390,   828,\n",
       "           318,   257,  2829,    11,  3660,  7815, 15207,   286,  5335,    13]),\n",
       " tensor([19895,  5712, 20221,    11,   262,  1524,   468,   257,  7835,  2095,\n",
       "            13,  1629,   404,   262,  8774, 11819,   338,  3869, 29500,   318,\n",
       "           257, 10861, 15207,   286,   262,  5283,  5335,    13, 34528,   287,\n",
       "          2166,   286,   262,  8774, 11819,   290,  6476,   340,    11,   318,\n",
       "           257, 15317, 15207,   286,  1951,   351,  5101,   510, 49309,   351,\n",
       "           262,  8177,   366, 37522,   578,  1215,  2185, 16543,  2516,  1911,\n",
       "          7406,   284,   262,  8774, 11819,   318,   262, 32520,  3970,   286,\n",
       "           262, 17380,  8894,    13, 34528,  2157,   262, 37792,  3970,   318,\n",
       "           262, 10299, 33955,    11,   257, 37919,  1295,   286, 11443,   290,\n",
       "         14580,    13,   632,   318,   257, 30069,   286,   262,  7128, 33955,\n",
       "           379,   406,   454,  8906,    11,  4881,   810,   262,  5283,  5335,\n",
       "          1128,  7241,   306,  4120,   284,  9281,  6206,   324,  5857,   311,\n",
       "         12944,   343,   516,   287,  1248,  3365,    13,  1629,   262,   886,\n",
       "           286,   262,  1388,  3708,   357,   392,   287,   257,  1277,  1627,\n",
       "           326, 20417,   832,   513, 25827,   290,   262,  3561, 31390,   828,\n",
       "           318,   257,  2829,    11,  3660,  7815, 15207,   286,  5335,    13])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2b99f49-ea68-42a8-ab61-936bea021c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[{'input_ids': tensor([19895,  5712, 20221,    11,   262,  1524,   468,   257,  7835,  2095,\n",
      "           13,  1629,   404,   262,  8774, 11819,   338,  3869, 29500,   318,\n",
      "          257, 10861, 15207,   286,   262,  5283,  5335,    13, 34528,   287,\n",
      "         2166,   286,   262,  8774, 11819,   290,  6476,   340,    11,   318,\n",
      "          257, 15317, 15207,   286,  1951,   351,  5101,   510, 49309,   351,\n",
      "          262,  8177,   366, 37522,   578,  1215,  2185, 16543,  2516,  1911,\n",
      "         7406,   284,   262,  8774, 11819,   318,   262, 32520,  3970,   286,\n",
      "          262, 17380,  8894,    13, 34528,  2157,   262, 37792,  3970,   318,\n",
      "          262, 10299, 33955,    11,   257, 37919,  1295,   286, 11443,   290,\n",
      "        14580,    13,   632,   318,   257, 30069,   286,   262,  7128, 33955,\n",
      "          379,   406,   454,  8906,    11,  4881,   810,   262,  5283,  5335,\n",
      "         1128,  7241,   306,  4120,   284,  9281,  6206,   324,  5857,   311,\n",
      "        12944,   343,   516,   287,  1248,  3365,    13,  1629,   262,   886,\n",
      "          286,   262,  1388,  3708,   357,   392,   287,   257,  1277,  1627,\n",
      "          326, 20417,   832,   513, 25827,   290,   262,  3561, 31390,   828,\n",
      "          318,   257,  2829,    11,  3660,  7815, 15207,   286,  5335,    13])}, {'input_ids': tensor([19895,  5712, 20221,    11,   262,  1524,   468,   257,  7835,  2095,\n",
      "           13,  1629,   404,   262,  8774, 11819,   338,  3869, 29500,   318,\n",
      "          257, 10861, 15207,   286,   262,  5283,  5335,    13, 34528,   287,\n",
      "         2166,   286,   262,  8774, 11819,   290,  6476,   340,    11,   318,\n",
      "          257, 15317, 15207,   286,  1951,   351,  5101,   510, 49309,   351,\n",
      "          262,  8177,   366, 37522,   578,  1215,  2185, 16543,  2516,  1911,\n",
      "         7406,   284,   262,  8774, 11819,   318,   262, 32520,  3970,   286,\n",
      "          262, 17380,  8894,    13, 34528,  2157,   262, 37792,  3970,   318,\n",
      "          262, 10299, 33955,    11,   257, 37919,  1295,   286, 11443,   290,\n",
      "        14580,    13,   632,   318,   257, 30069,   286,   262,  7128, 33955,\n",
      "          379,   406,   454,  8906,    11,  4881,   810,   262,  5283,  5335,\n",
      "         1128,  7241,   306,  4120,   284,  9281,  6206,   324,  5857,   311,\n",
      "        12944,   343,   516,   287,  1248,  3365,    13,  1629,   262,   886,\n",
      "          286,   262,  1388,  3708,   357,   392,   287,   257,  1277,  1627,\n",
      "          326, 20417,   832,   513, 25827,   290,   262,  3561, 31390,   828,\n",
      "          318,   257,  2829,    11,  3660,  7815, 15207,   286,  5335,    13])}, {'input_ids': tensor([19895,  5712, 20221,    11,   262,  1524,   468,   257,  7835,  2095,\n",
      "           13,  1629,   404,   262,  8774, 11819,   338,  3869, 29500,   318,\n",
      "          257, 10861, 15207,   286,   262,  5283,  5335,    13, 34528,   287,\n",
      "         2166,   286,   262,  8774, 11819,   290,  6476,   340,    11,   318,\n",
      "          257, 15317, 15207,   286,  1951,   351,  5101,   510, 49309,   351,\n",
      "          262,  8177,   366, 37522,   578,  1215,  2185, 16543,  2516,  1911,\n",
      "         7406,   284,   262,  8774, 11819,   318,   262, 32520,  3970,   286,\n",
      "          262, 17380,  8894,    13, 34528,  2157,   262, 37792,  3970,   318,\n",
      "          262, 10299, 33955,    11,   257, 37919,  1295,   286, 11443,   290,\n",
      "        14580,    13,   632,   318,   257, 30069,   286,   262,  7128, 33955,\n",
      "          379,   406,   454,  8906,    11,  4881,   810,   262,  5283,  5335,\n",
      "         1128,  7241,   306,  4120,   284,  9281,  6206,   324,  5857,   311,\n",
      "        12944,   343,   516,   287,  1248,  3365,    13,  1629,   262,   886,\n",
      "          286,   262,  1388,  3708,   357,   392,   287,   257,  1277,  1627,\n",
      "          326, 20417,   832,   513, 25827,   290,   262,  3561, 31390,   828,\n",
      "          318,   257,  2829,    11,  3660,  7815, 15207,   286,  5335,    13])}, {'input_ids': tensor([19895,  5712, 20221,    11,   262,  1524,   468,   257,  7835,  2095,\n",
      "           13,  1629,   404,   262,  8774, 11819,   338,  3869, 29500,   318,\n",
      "          257, 10861, 15207,   286,   262,  5283,  5335,    13, 34528,   287,\n",
      "         2166,   286,   262,  8774, 11819,   290,  6476,   340,    11,   318,\n",
      "          257, 15317, 15207,   286,  1951,   351,  5101,   510, 49309,   351,\n",
      "          262,  8177,   366, 37522,   578,  1215,  2185, 16543,  2516,  1911,\n",
      "         7406,   284,   262,  8774, 11819,   318,   262, 32520,  3970,   286,\n",
      "          262, 17380,  8894,    13, 34528,  2157,   262, 37792,  3970,   318,\n",
      "          262, 10299, 33955,    11,   257, 37919,  1295,   286, 11443,   290,\n",
      "        14580,    13,   632,   318,   257, 30069,   286,   262,  7128, 33955,\n",
      "          379,   406,   454,  8906,    11,  4881,   810,   262,  5283,  5335,\n",
      "         1128,  7241,   306,  4120,   284,  9281,  6206,   324,  5857,   311,\n",
      "        12944,   343,   516,   287,  1248,  3365,    13,  1629,   262,   886,\n",
      "          286,   262,  1388,  3708,   357,   392,   287,   257,  1277,  1627,\n",
      "          326, 20417,   832,   513, 25827,   290,   262,  3561, 31390,   828,\n",
      "          318,   257,  2829,    11,  3660,  7815, 15207,   286,  5335,    13])}]\n"
     ]
    }
   ],
   "source": [
    "sample = [{\"input_ids\": data} for data in datasets[:4]]\n",
    "print(type(sample))\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea1235df-be9c-4229-a101-ef516c1558d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[19895,  5712, 20221,    11,   262,  1524,   468,   257,  7835,  2095,\n",
       "            13,  1629,   404,   262,  8774, 11819,   338,  3869, 29500,   318,\n",
       "           257, 10861, 15207,   286,   262,  5283,  5335,    13, 34528,   287,\n",
       "          2166,   286,   262,  8774, 11819,   290,  6476,   340,    11,   318,\n",
       "           257, 15317, 15207,   286,  1951,   351,  5101,   510, 49309,   351,\n",
       "           262,  8177,   366, 37522,   578,  1215,  2185, 16543,  2516,  1911,\n",
       "          7406,   284,   262,  8774, 11819,   318,   262, 32520,  3970,   286,\n",
       "           262, 17380,  8894,    13, 34528,  2157,   262, 37792,  3970,   318,\n",
       "           262, 10299, 33955,    11,   257, 37919,  1295,   286, 11443,   290,\n",
       "         14580,    13,   632,   318,   257, 30069,   286,   262,  7128, 33955,\n",
       "           379,   406,   454,  8906,    11,  4881,   810,   262,  5283,  5335,\n",
       "          1128,  7241,   306,  4120,   284,  9281,  6206,   324,  5857,   311,\n",
       "         12944,   343,   516,   287,  1248,  3365,    13,  1629,   262,   886,\n",
       "           286,   262,  1388,  3708,   357,   392,   287,   257,  1277,  1627,\n",
       "           326, 20417,   832,   513, 25827,   290,   262,  3561, 31390,   828,\n",
       "           318,   257,  2829,    11,  3660,  7815, 15207,   286,  5335,    13],\n",
       "        [19895,  5712, 20221,    11,   262,  1524,   468,   257,  7835,  2095,\n",
       "            13,  1629,   404,   262,  8774, 11819,   338,  3869, 29500,   318,\n",
       "           257, 10861, 15207,   286,   262,  5283,  5335,    13, 34528,   287,\n",
       "          2166,   286,   262,  8774, 11819,   290,  6476,   340,    11,   318,\n",
       "           257, 15317, 15207,   286,  1951,   351,  5101,   510, 49309,   351,\n",
       "           262,  8177,   366, 37522,   578,  1215,  2185, 16543,  2516,  1911,\n",
       "          7406,   284,   262,  8774, 11819,   318,   262, 32520,  3970,   286,\n",
       "           262, 17380,  8894,    13, 34528,  2157,   262, 37792,  3970,   318,\n",
       "           262, 10299, 33955,    11,   257, 37919,  1295,   286, 11443,   290,\n",
       "         14580,    13,   632,   318,   257, 30069,   286,   262,  7128, 33955,\n",
       "           379,   406,   454,  8906,    11,  4881,   810,   262,  5283,  5335,\n",
       "          1128,  7241,   306,  4120,   284,  9281,  6206,   324,  5857,   311,\n",
       "         12944,   343,   516,   287,  1248,  3365,    13,  1629,   262,   886,\n",
       "           286,   262,  1388,  3708,   357,   392,   287,   257,  1277,  1627,\n",
       "           326, 20417,   832,   513, 25827,   290,   262,  3561, 31390,   828,\n",
       "           318,   257,  2829,    11,  3660,  7815, 15207,   286,  5335,    13],\n",
       "        [19895,  5712, 20221,    11,   262,  1524,   468,   257,  7835,  2095,\n",
       "            13,  1629,   404,   262,  8774, 11819,   338,  3869, 29500,   318,\n",
       "           257, 10861, 15207,   286,   262,  5283,  5335,    13, 34528,   287,\n",
       "          2166,   286,   262,  8774, 11819,   290,  6476,   340,    11,   318,\n",
       "           257, 15317, 15207,   286,  1951,   351,  5101,   510, 49309,   351,\n",
       "           262,  8177,   366, 37522,   578,  1215,  2185, 16543,  2516,  1911,\n",
       "          7406,   284,   262,  8774, 11819,   318,   262, 32520,  3970,   286,\n",
       "           262, 17380,  8894,    13, 34528,  2157,   262, 37792,  3970,   318,\n",
       "           262, 10299, 33955,    11,   257, 37919,  1295,   286, 11443,   290,\n",
       "         14580,    13,   632,   318,   257, 30069,   286,   262,  7128, 33955,\n",
       "           379,   406,   454,  8906,    11,  4881,   810,   262,  5283,  5335,\n",
       "          1128,  7241,   306,  4120,   284,  9281,  6206,   324,  5857,   311,\n",
       "         12944,   343,   516,   287,  1248,  3365,    13,  1629,   262,   886,\n",
       "           286,   262,  1388,  3708,   357,   392,   287,   257,  1277,  1627,\n",
       "           326, 20417,   832,   513, 25827,   290,   262,  3561, 31390,   828,\n",
       "           318,   257,  2829,    11,  3660,  7815, 15207,   286,  5335,    13],\n",
       "        [19895,  5712, 20221,    11,   262,  1524,   468,   257,  7835,  2095,\n",
       "            13,  1629,   404,   262,  8774, 11819,   338,  3869, 29500,   318,\n",
       "           257, 10861, 15207,   286,   262,  5283,  5335,    13, 34528,   287,\n",
       "          2166,   286,   262,  8774, 11819,   290,  6476,   340,    11,   318,\n",
       "           257, 15317, 15207,   286,  1951,   351,  5101,   510, 49309,   351,\n",
       "           262,  8177,   366, 37522,   578,  1215,  2185, 16543,  2516,  1911,\n",
       "          7406,   284,   262,  8774, 11819,   318,   262, 32520,  3970,   286,\n",
       "           262, 17380,  8894,    13, 34528,  2157,   262, 37792,  3970,   318,\n",
       "           262, 10299, 33955,    11,   257, 37919,  1295,   286, 11443,   290,\n",
       "         14580,    13,   632,   318,   257, 30069,   286,   262,  7128, 33955,\n",
       "           379,   406,   454,  8906,    11,  4881,   810,   262,  5283,  5335,\n",
       "          1128,  7241,   306,  4120,   284,  9281,  6206,   324,  5857,   311,\n",
       "         12944,   343,   516,   287,  1248,  3365,    13,  1629,   262,   886,\n",
       "           286,   262,  1388,  3708,   357,   392,   287,   257,  1277,  1627,\n",
       "           326, 20417,   832,   513, 25827,   290,   262,  3561, 31390,   828,\n",
       "           318,   257,  2829,    11,  3660,  7815, 15207,   286,  5335,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba299df8-64d3-42bd-b0a5-106cbd939ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[19895,  5712, 20221,  ..., 50256, 50256, 50256],\n",
       "        [19895,  5712, 20221,  ..., 50256, 50256, 50256],\n",
       "        [19895,  5712, 20221,  ..., 50256, 50256, 50256],\n",
       "        [19895,  5712, 20221,  ..., 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad(sample, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "192e3b4f-f5ea-4cda-8d4a-af225c47a88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4800/4800 [00:02<00:00, 1807.84it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size, train_steps = 16, 300\n",
    "train_samples = batch_size * train_steps\n",
    "\n",
    "datasets = load_dataset(\"squad\").data[\"train\"][\"context\"]\n",
    "datasets = [str(sample) for i, sample in enumerate(datasets) if i < train_samples]\n",
    "datasets = [\n",
    "        tokenizer(data, return_tensors=\"pt\", max_length=1024).input_ids[0]\n",
    "        for data in tqdm(datasets)\n",
    "]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_encoding = tokenizer.pad(\n",
    "        {\"input_ids\": batch}, padding=\"max_length\", max_length=1024\n",
    "    )\n",
    "    return batch_encoding.input_ids\n",
    "\n",
    "\n",
    "data_loader = DataLoader(\n",
    "            #datasets,\n",
    "            sorted(datasets, key=len, reverse=True),\n",
    "            # uniform length batching\n",
    "            # https://mccormickml.com/2020/07/29/smart-batching-tutorial/\n",
    "            batch_size=batch_size,\n",
    "            num_workers=8,\n",
    "            collate_fn=collate_fn,\n",
    "            shuffle=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "331d43d8-857d-4004-9dfd-e1cd2a605ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in data_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1a03a3c-607d-4d13-b715-ee735358b194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1024,\n",
       " 1024,\n",
       " 1024,\n",
       " 1024,\n",
       " 1024,\n",
       " 1024,\n",
       " 1024,\n",
       " 1024,\n",
       " 1024,\n",
       " 1024,\n",
       " 1024,\n",
       " 1024,\n",
       " 1024,\n",
       " 1024,\n",
       " 1024,\n",
       " 1024]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "033cad2b-f2c1-40dc-9071-9d121562d4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[41000, 45436,   286,  ..., 50256, 50256, 50256],\n",
       "        [41000, 45436,   286,  ..., 50256, 50256, 50256]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4200da3d-830e-495b-97f2-3dc4f200a409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding, get_scheduler\n",
    "\n",
    "batch_size = 2\n",
    "datasets = load_dataset(\"squad\")\n",
    "      \n",
    "tokenized_datasets = datasets.map(lambda x: tokenizer(x[\"context\"], truncation=True, padding=\"max_length\"), batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(datasets[\"train\"].column_names)\n",
    "#data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def data_collator(batch):\n",
    "    print(type(batch))\n",
    "    a = [x[\"input_ids\"] for x in batch]\n",
    "    return torch.tensor(a)\n",
    "\n",
    "data_loader = DataLoader(\n",
    "         tokenized_datasets[\"train\"], shuffle=False, batch_size=batch_size, collate_fn=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f63c80d6-013a-4328-8c5c-201571efd8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "tensor([[19895,  5712, 20221,  ..., 50256, 50256, 50256],\n",
      "        [19895,  5712, 20221,  ..., 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "for batch in data_loader:\n",
    "    break\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d1bc7e39-68f2-497e-9b0a-c7520e5ed24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c908751c5ee4a5fa99d9e72e8e0b45a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e35a155611e4214ba8ef67a9489393f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding, get_scheduler\n",
    "\n",
    "batch_size = 16\n",
    "datasets = load_dataset(\"squad\")\n",
    "      \n",
    "tokenized_datasets = datasets.map(lambda x: tokenizer(x[\"context\"], truncation=True), batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(datasets[\"train\"].column_names)\n",
    "\n",
    "#data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def data_collator(batch):\n",
    "    batch_encoding = tokenizer.pad(\n",
    "        #{\"input_ids\": batch}, return_tensors=\"pt\", padding=\"max_length\", max_length=1024\n",
    "        {\"input_ids\": batch}, return_tensors=\"pt\", padding=True\n",
    "    )\n",
    "    return batch_encoding.input_ids\n",
    "\n",
    "    \n",
    "data_loader = DataLoader(\n",
    "         tokenized_datasets[\"train\"][\"input_ids\"], shuffle=False, batch_size=batch_size, collate_fn=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "36de05ae-6116-45df-9af4-ad1c59480cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 261])\n"
     ]
    }
   ],
   "source": [
    "for batch in data_loader:\n",
    "    break\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a0cff9df-3a43-44b9-9309-8fb3f2c0ff47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 87599\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2961c655-86a6-4f4b-b243-453b68c22567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding, get_scheduler\n",
    "\n",
    "batch_size = 16\n",
    "datasets = load_dataset(\"squad\")\n",
    "      \n",
    "tokenized_datasets = datasets.map(lambda x: tokenizer(x[\"context\"], truncation=True, padding=\"max_length\"), batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(datasets[\"train\"].column_names)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "data_loader = DataLoader(\n",
    "         tokenized_datasets[\"train\"], shuffle=False, batch_size=batch_size, collate_fn=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23b73bb5-ce09-4a90-a004-427e63c09142",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_datasets = sorted(tokenized_datasets[\"train\"][\"input_ids\"], key=len, reverse=True)\n",
    "#print([len(x) for x in sorted_datasets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1aefc84d-cce2-4b8e-a48f-2c72a4c6e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding, get_scheduler\n",
    "\n",
    "batch_size = 16\n",
    "train_datasets = load_dataset(\"squad\", split=\"train\")\n",
    "tokenized_datasets = tokenizer(train_datasets[\"context\"], truncation=True)\n",
    "#sorted_datasets = sorted(tokenized_datasets[\"input_ids\"], key=len, reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3bce47d6-da23-4467-b8bb-41c9ddf20d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{'input_ids': [[19895, 5712, 20221, 11, 262, 1524, 468, 257, 7835, 2095, 13, 1629, 404, 262, 8774, 11819, 338, 3869, 29500, 318, 257, 10861, 15207, 286, 262, 5283, 5335, 13, 34528, 287, 2166, 286, 262, 8774, 11819, 290, 6476, 340, 11, 318, 257, 15317, 15207, 286, 1951, 351, 5101, 510, 49309, 351, 262, 8177, 366, 37522, 578, 1215, 2185, 16543, 2516, 1911, 7406, 284, 262, 8774, 11819, 318, 262, 32520, 3970, 286, 262, 17380, 8894, 13, 34528, 2157, 262, 37792, 3970, 318, 262, 10299, 33955, 11, 257, 37919, 1295, 286, 11443, 290, 14580, 13, 632, 318, 257, 30069, 286, 262, 7128, 33955, 379, 406, 454, 8906, 11, 4881, 810, 262, 5283, 5335, 1128, 7241, 306, 4120, 284, 9281, 6206, 324, 5857, 311, 12944, 343, 516, 287, 1248, 3365, 13, 1629, 262, 886, 286, 262, 1388, 3708, 357, 392, 287, 257, 1277, 1627, 326, 20417, 832, 513, 25827, 290, 262, 3561, 31390, 828, 318, 257, 2829, 11, 3660, 7815, 15207, 286, 5335, 13], [19895, 5712, 20221, 11, 262, 1524, 468, 257, 7835, 2095, 13, 1629, 404, 262, 8774, 11819, 338, 3869, 29500, 318, 257, 10861, 15207, 286, 262, 5283, 5335, 13, 34528, 287, 2166, 286, 262, 8774, 11819, 290, 6476, 340, 11, 318, 257, 15317, 15207, 286, 1951, 351, 5101, 510, 49309, 351, 262, 8177, 366, 37522, 578, 1215, 2185, 16543, 2516, 1911, 7406, 284, 262, 8774, 11819, 318, 262, 32520, 3970, 286, 262, 17380, 8894, 13, 34528, 2157, 262, 37792, 3970, 318, 262, 10299, 33955, 11, 257, 37919, 1295, 286, 11443, 290, 14580, 13, 632, 318, 257, 30069, 286, 262, 7128, 33955, 379, 406, 454, 8906, 11, 4881, 810, 262, 5283, 5335, 1128, 7241, 306, 4120, 284, 9281, 6206, 324, 5857, 311, 12944, 343, 516, 287, 1248, 3365, 13, 1629, 262, 886, 286, 262, 1388, 3708, 357, 392, 287, 257, 1277, 1627, 326, 20417, 832, 513, 25827, 290, 262, 3561, 31390, 828, 318, 257, 2829, 11, 3660, 7815, 15207, 286, 5335, 13]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_datasets))\n",
    "print(tokenized_datasets[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03b4c754-4fcc-4490-88df-204060a0cce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "datasets = load_dataset(\"squad\")\n",
    "def tokenize_function(element):\n",
    "        return tokenizer(element[\"context\"], truncation=True)\n",
    "\n",
    "#tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = datasets.map(lambda x: tokenizer(x[\"context\"], truncation=True), batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(datasets[\"train\"].column_names)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=False, batch_size=batch_size, collate_fn=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17c028d7-0662-4e26-abe4-7bbc140f9137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 261])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in data_loader:\n",
    "    break\n",
    "batch[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ec9821-cdb7-45d1-a4fa-bea22bef9e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp",
   "language": "python",
   "name": "dp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
