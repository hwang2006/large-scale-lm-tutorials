{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee8a5fce-dfea-4600-a60e-290cbfd9efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size, hidden_size =4, 512\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.z1 = None\n",
    "        self.w1 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_size, 1, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = self.w1(x)\n",
    "        print(z1)\n",
    "        self.z1 = z1.clone().detach()\n",
    "        z2 = self.w2(z1)\n",
    "        return z2\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "916e0d62-1109-4e3a-a7a3-739362e32fdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 512])\n",
      "torch.Size([1024, 100])\n",
      "torch.Size([10, 1024])\n",
      "torch.Size([4, 512])\n",
      "tensor([[ 1.2053e-02, -2.8138e-01,  1.1684e+00,  2.8869e-01, -2.3495e-01,\n",
      "          1.7711e-02, -9.4961e-01, -4.5591e-01,  4.5444e-01, -1.0913e+00,\n",
      "          2.5712e-01,  6.3148e-01, -3.0727e-01, -2.2693e-01, -7.2851e-01,\n",
      "          4.4833e-01, -7.1848e-01, -5.5857e-01,  2.7255e-01,  4.4720e-01,\n",
      "          4.0542e-01,  8.0266e-01, -2.6425e-01,  9.6507e-01, -8.0444e-01,\n",
      "          9.4456e-01,  1.8210e-01,  2.4276e-01, -4.8592e-01,  1.8035e-01,\n",
      "          7.0945e-01,  2.7731e-01,  6.1586e-01,  7.2934e-01,  2.7688e-01,\n",
      "         -1.2357e-01, -2.0440e-01,  2.0607e+00,  1.8939e-01,  1.2655e+00,\n",
      "         -5.0912e-01,  2.0741e-01,  5.9439e-01,  5.5814e-01,  7.1868e-04,\n",
      "         -1.7160e-01, -3.5857e-01,  7.2398e-01,  7.5328e-01, -1.1581e-02,\n",
      "          3.5739e-01,  4.9360e-01, -9.1885e-01, -9.0178e-01,  7.6294e-01,\n",
      "         -1.2743e+00, -3.4529e-01, -2.0674e-01,  1.3492e-01, -4.7945e-01,\n",
      "          1.2097e+00, -5.4454e-02, -2.8728e-01, -2.2626e-01,  7.3690e-01,\n",
      "          3.8658e-01,  2.8108e-01, -1.2133e+00,  1.0740e+00, -5.8895e-02,\n",
      "          3.5479e-01, -1.4817e-01,  1.0048e+00, -7.9871e-02, -9.9070e-03,\n",
      "          1.1519e+00, -8.3207e-01, -1.7487e-01, -2.6487e-01, -1.9291e-01,\n",
      "         -9.1619e-01, -1.5880e-01,  4.3877e-01, -7.2883e-01,  2.9637e-01,\n",
      "         -1.6383e-01,  8.9418e-01, -4.3462e-01, -3.0884e-01, -4.2228e-01,\n",
      "         -1.1953e-01, -8.8652e-01,  8.3880e-01,  4.2702e-01, -6.4946e-01,\n",
      "         -3.5939e-01, -1.3322e-01, -5.3490e-01,  2.8928e-01,  1.1680e-02],\n",
      "        [ 6.2847e-01,  2.1342e-02, -2.4339e-02, -4.4596e-01, -3.8069e-01,\n",
      "          1.3342e-01,  2.8801e-01, -3.4224e-01,  2.9991e-01,  3.7675e-02,\n",
      "          1.0578e+00,  4.1042e-01, -5.7585e-01,  4.3164e-01,  5.3331e-01,\n",
      "         -1.0173e-01, -1.0240e+00,  4.2725e-01,  6.5404e-01,  1.3019e-01,\n",
      "          6.7453e-01, -7.7669e-01,  5.5434e-02,  1.3502e+00, -8.4861e-01,\n",
      "          6.9778e-01,  3.6529e-01,  5.4387e-02,  5.2688e-02,  7.3694e-01,\n",
      "         -4.0383e-01,  2.3780e-01,  3.2481e-01,  2.0069e-01, -2.8114e-01,\n",
      "         -7.2122e-01,  5.8485e-01,  8.2925e-01,  2.9220e-01, -5.8888e-02,\n",
      "          5.2178e-01,  5.9831e-02,  4.7525e-01, -4.4363e-01,  4.6829e-01,\n",
      "         -6.1025e-01, -2.4494e-01,  5.2708e-01, -1.3054e+00, -6.0344e-01,\n",
      "         -8.0512e-01,  3.5543e-01,  9.0971e-01, -3.4929e-01,  1.7965e-01,\n",
      "          7.8281e-01, -1.0091e+00,  1.3088e-01,  4.9043e-01, -9.7574e-02,\n",
      "         -1.2365e+00, -3.3480e-01,  8.1578e-01, -6.6180e-02,  2.4948e-01,\n",
      "          5.6792e-01,  1.2660e-01, -1.4527e-01, -1.5916e-01, -5.4581e-01,\n",
      "          2.3636e-01,  4.2397e-01,  8.2671e-01, -1.8254e-01, -2.3318e-01,\n",
      "         -3.7214e-03, -4.9919e-01, -1.3203e+00, -7.7062e-02, -4.6717e-01,\n",
      "          1.1191e+00,  7.4381e-01, -1.2759e-01, -1.5836e-01, -1.0146e-01,\n",
      "          1.2385e-01, -2.3368e-01,  1.8217e-01, -3.7922e-01, -7.1944e-01,\n",
      "          3.2393e-01,  1.1132e-01, -5.6166e-01, -1.0513e-01,  2.7995e-01,\n",
      "          2.3234e-01,  5.8920e-01, -9.4429e-01,  1.0281e-01,  3.2923e-01],\n",
      "        [ 3.6145e-01, -2.4446e-01, -4.4558e-01, -4.8813e-02,  4.8243e-01,\n",
      "          5.5425e-01,  6.0893e-01,  2.5092e-01, -3.3689e-01, -3.7738e-01,\n",
      "         -6.7806e-01,  1.4453e-01, -1.2436e-02, -4.6803e-01,  7.1406e-01,\n",
      "          1.7129e+00, -6.3176e-01,  4.7978e-01,  6.6457e-01, -2.9246e-01,\n",
      "          6.4464e-01, -1.0695e+00, -4.9395e-01,  6.4829e-01, -3.3603e-01,\n",
      "         -3.6682e-01, -3.9156e-01, -9.8353e-01, -1.7249e-01, -3.4344e-01,\n",
      "         -3.6127e-01,  4.4305e-01,  1.2453e-01, -5.0592e-01,  5.6466e-02,\n",
      "          2.8239e-01,  8.2086e-01,  9.5022e-01, -1.8118e-01, -2.4522e-01,\n",
      "         -4.8420e-02, -2.2190e-01,  1.6000e-01, -9.7376e-02,  1.0117e+00,\n",
      "         -3.9742e-01, -4.8836e-01,  9.1322e-01,  1.3144e-01,  1.9713e-01,\n",
      "          5.1304e-01,  7.5744e-01,  4.0816e-01, -7.2953e-01,  1.7742e-01,\n",
      "          7.7522e-01, -2.4927e-01,  9.5101e-01, -3.9577e-01,  6.8745e-01,\n",
      "          5.9561e-01,  1.1398e-01,  1.8362e-01,  3.2029e-01, -1.6300e-01,\n",
      "         -2.5293e-01, -7.7356e-03, -8.9537e-01,  1.5126e+00,  1.2696e-01,\n",
      "         -1.0278e+00,  5.8179e-01, -1.0501e+00, -1.2403e+00,  2.9386e-01,\n",
      "         -4.9497e-01, -6.4527e-01, -8.8282e-01, -7.3856e-01,  4.2991e-01,\n",
      "          7.0203e-01, -6.9036e-01,  5.6659e-01,  4.6589e-01, -4.2434e-01,\n",
      "         -2.7606e-01, -4.3916e-01,  4.8237e-01, -2.8669e-03, -9.1885e-01,\n",
      "         -1.4613e-02, -2.6189e-01, -1.6956e-03, -2.3057e-01,  4.2279e-01,\n",
      "         -8.4924e-02,  7.4391e-01,  3.4571e-01, -1.0947e-01,  5.1567e-01],\n",
      "        [-9.0300e-01,  2.2432e-01,  1.3415e-01,  8.7782e-03, -3.2175e-01,\n",
      "         -7.9723e-02,  1.0204e-01, -1.8833e-02, -9.9115e-02, -4.4861e-01,\n",
      "          1.9723e-01,  2.8849e-02, -1.7032e-01,  2.1486e-02, -8.1211e-01,\n",
      "          8.2795e-01, -1.2401e+00, -3.7315e-01, -1.0851e+00,  8.9658e-01,\n",
      "         -5.9821e-01,  3.8774e-01, -9.8856e-03, -1.7308e-01, -4.4794e-01,\n",
      "         -1.4270e-01, -1.1967e+00,  4.9154e-01,  7.9405e-01,  5.0812e-01,\n",
      "          6.7584e-01, -1.2747e-01,  3.5748e-01,  3.7511e-01, -2.1074e-01,\n",
      "          1.2329e-01, -8.5022e-01, -8.6491e-01, -1.3487e-01,  1.0638e-01,\n",
      "          1.2510e-01, -5.5990e-01, -4.1868e-01,  4.4211e-01, -1.2478e-01,\n",
      "         -3.9582e-02, -2.7954e-01, -3.3638e-01,  1.1930e-01, -3.9843e-01,\n",
      "         -1.3790e-01, -2.4855e-01, -1.6754e-01, -2.3299e-01, -1.3026e+00,\n",
      "          1.8793e-01, -1.8520e-01,  7.0453e-01, -4.1890e-01, -1.0481e-01,\n",
      "          5.9191e-01,  5.1066e-02,  1.0185e+00,  5.3031e-01, -4.7788e-01,\n",
      "          6.8031e-02,  1.1112e+00, -1.0170e-01, -7.3899e-01,  9.7201e-01,\n",
      "          2.3036e-02, -6.8748e-01, -6.7786e-01,  4.4005e-01,  3.8568e-01,\n",
      "         -6.7011e-01, -2.4884e-01, -6.8408e-02,  2.5009e-01, -7.0245e-01,\n",
      "         -9.8595e-01,  2.2939e-01, -4.3335e-01,  3.9416e-01, -2.6282e-01,\n",
      "         -2.3853e-01,  1.0250e-01, -2.5861e-02,  5.9670e-01,  2.7102e-01,\n",
      "          4.3799e-01,  5.0161e-01,  6.9514e-01,  2.3492e-01,  4.7822e-01,\n",
      "          3.6212e-01, -2.4306e-01,  6.5389e-01,  1.4347e-01,  3.4870e-01]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size, hidden_size =4, 512\n",
    "\n",
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.z1 = None\n",
    "        self.w1 = nn.Linear(512, 100, bias=False)\n",
    "        self.w2 = nn.Linear(100, 1024, bias=False)\n",
    "        self.w3 = nn.Linear(1024, 10, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = self.w1(x)\n",
    "        print(z1)\n",
    "        self.z1 = z1.clone().detach()\n",
    "        z2 = self.w2(z1)\n",
    "        z3 = self.w3(z2)\n",
    "        return z3\n",
    "\n",
    "model1 = Net1()\n",
    "print(model1.w1.weight.shape)\n",
    "print(model1.w2.weight.shape)\n",
    "print(model1.w3.weight.shape)\n",
    "\n",
    "x = torch.randn(4, 512)\n",
    "print(x.shape)\n",
    "out = model1(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e9c3cb6-8898-4e58-8804-bc9b2c6cb919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 100])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(x, model1.w1.weight.T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a05b437-cb2d-46c3-9609-04cf741f578f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 100])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x@model1.w1.weight.T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0a95fd4-abcc-4f4f-9d66-c5d5bd4d8723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3885, -0.3268,  0.7740,  ...,  0.3769, -0.9833, -0.0972],\n",
      "        [ 1.8091,  0.0673,  0.8450,  ..., -0.3477, -0.6267, -0.7226],\n",
      "        [ 0.4881,  0.0083,  0.8602,  ..., -0.6376,  0.0209, -0.4623],\n",
      "        [-0.4124, -0.4325,  1.5413,  ...,  0.0395, -0.1859, -0.2388]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "tensor([[ 0.1055],\n",
      "        [ 0.1254],\n",
      "        [-0.5648],\n",
      "        [ 0.0886]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# example input sizes\n",
    "batch_size, hidden_size = 4, 512\n",
    "\n",
    "# create dummy data (bsz=4, hid=256)\n",
    "x = torch.randn(batch_size,hidden_size) \n",
    "out = model(x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1788e30c-cd24-49f3-8365-b0516bcb2721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7946df28-bad0-484b-ac20-38a509180c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loss type = torch.float32'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# craete dummy data (bsz=4)\n",
    "y = torch.tensor([[1.9], [9.5], [0.9], [1.2]])\n",
    "\n",
    "# compute mean square error loss\n",
    "L = torch.nn.functional.mse_loss(out, y)\n",
    "\n",
    "# check dtype of loss\n",
    "f\"loss type = {L.dtype}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bf92aa4-b9da-4e95-8ea9-67ea42ddd425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(23.6211, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b27397e-7d51-4203-a190-099f7c865803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(23.6211, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.sum((out-y)**2)/batch_size\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "857c1e32-df6e-4329-b53a-41f6b23a4abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8972],\n",
      "        [-4.6873],\n",
      "        [-0.7324],\n",
      "        [-0.5557]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "DL_Dout = 2*(out - y) / batch_size # Dout/DL\n",
    "print(DL_Dout) # Backward Activation [1,4] [output_size, batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57a3f904-d25a-45d4-9742-b2fddca59d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Dout_Dw2 = model.z1.clone().detach() # Backward Activation: w2 layer\n",
    "Dout_Dw2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0cd4b03-e9af-4fda-856c-a972f0a857b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DL_Dw2 = torch.matmul(DL_Dout.T, Dout_Dw2) # w2 gradient \n",
    "DL_Dw2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92d3aae2-4a9d-477b-82e8-03f2981a93cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DL_Dz1 = torch.matmul(DL_Dout, model.w2.weight) # Dl/Dout * w2\n",
    "DL_Dz1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2835f654-2db3-49e6-8670-d0180fff364d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x.shape [batch_size=4, hidden_size=512]\n",
    "DL_Dw1 = torch.matmul(DL_Dz1.T,x) # w1 gradient\n",
    "DL_Dw1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48a219f5-eed3-43a6-9000-f459034852db",
   "metadata": {},
   "outputs": [],
   "source": [
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef5ab0cf-f22d-4a1e-a1c9-21e30a8a12e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0856,  0.0118, -0.0254,  ..., -0.0425,  0.0192, -0.0561],\n",
      "        [ 0.1334, -0.0184,  0.0396,  ...,  0.0661, -0.0298,  0.0874],\n",
      "        [ 0.0753, -0.0104,  0.0224,  ...,  0.0373, -0.0169,  0.0493],\n",
      "        ...,\n",
      "        [ 0.2326, -0.0321,  0.0691,  ...,  0.1153, -0.0521,  0.1524],\n",
      "        [ 0.2156, -0.0297,  0.0640,  ...,  0.1069, -0.0483,  0.1413],\n",
      "        [ 0.0517, -0.0071,  0.0154,  ...,  0.0257, -0.0116,  0.0339]])\n",
      "tensor([[-0.0856,  0.0118, -0.0254,  ..., -0.0425,  0.0192, -0.0561],\n",
      "        [ 0.1334, -0.0184,  0.0396,  ...,  0.0661, -0.0298,  0.0874],\n",
      "        [ 0.0753, -0.0104,  0.0224,  ...,  0.0373, -0.0169,  0.0493],\n",
      "        ...,\n",
      "        [ 0.2326, -0.0321,  0.0691,  ...,  0.1153, -0.0521,  0.1524],\n",
      "        [ 0.2156, -0.0297,  0.0640,  ...,  0.1069, -0.0483,  0.1413],\n",
      "        [ 0.0517, -0.0071,  0.0154,  ...,  0.0257, -0.0116,  0.0339]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.w1.weight.grad)\n",
    "print(DL_Dw1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96f17733-144d-423f-8491-654a7202fde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: Parameter containing:\n",
      "tensor([[-0.0021,  0.0054, -0.0359,  ...,  0.0298,  0.0162, -0.0093],\n",
      "        [-0.0225,  0.0289,  0.0428,  ...,  0.0317,  0.0185,  0.0310],\n",
      "        [ 0.0394,  0.0035,  0.0282,  ..., -0.0349, -0.0162, -0.0333],\n",
      "        ...,\n",
      "        [ 0.0064, -0.0342,  0.0275,  ..., -0.0055, -0.0226, -0.0247],\n",
      "        [-0.0439,  0.0092, -0.0210,  ...,  0.0023, -0.0396,  0.0100],\n",
      "        [-0.0334,  0.0217, -0.0039,  ..., -0.0278,  0.0370,  0.0168]],\n",
      "       requires_grad=True)\n",
      "\n",
      "after: Parameter containing:\n",
      "tensor([[ 0.0064,  0.0042, -0.0333,  ...,  0.0341,  0.0142, -0.0037],\n",
      "        [-0.0358,  0.0307,  0.0388,  ...,  0.0251,  0.0214,  0.0223],\n",
      "        [ 0.0319,  0.0045,  0.0260,  ..., -0.0386, -0.0145, -0.0382],\n",
      "        ...,\n",
      "        [-0.0169, -0.0310,  0.0206,  ..., -0.0170, -0.0174, -0.0400],\n",
      "        [-0.0654,  0.0122, -0.0274,  ..., -0.0084, -0.0347, -0.0041],\n",
      "        [-0.0386,  0.0224, -0.0055,  ..., -0.0303,  0.0381,  0.0135]],\n",
      "       requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'before: {model.w1.weight}\\n')\n",
    "optimizer.step()\n",
    "#print(model.w1.weight.grad)\n",
    "print(f'after: {model.w1.weight}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da8e6f6a-f4f2-40a8-a562-1858ad019045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.1\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e82825c7-bf3f-4951-9336-d08a74f1220f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0064,  0.0042, -0.0333,  ...,  0.0341,  0.0142, -0.0037],\n",
       "        [-0.0358,  0.0307,  0.0388,  ...,  0.0251,  0.0214,  0.0223],\n",
       "        [ 0.0319,  0.0045,  0.0260,  ..., -0.0386, -0.0145, -0.0382],\n",
       "        ...,\n",
       "        [-0.0169, -0.0310,  0.0206,  ..., -0.0170, -0.0174, -0.0400],\n",
       "        [-0.0654,  0.0122, -0.0274,  ..., -0.0084, -0.0347, -0.0041],\n",
       "        [-0.0386,  0.0224, -0.0055,  ..., -0.0303,  0.0381,  0.0135]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups[0]['params'][0] # w1 weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fe28a0a-d8be-4b07-b321-304507cabfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer.param_groups[0]['params'][1] #w2 weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f28d4114-e78b-4b6d-a7dc-532d9a922153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6451, -0.5995,  0.1066,  ..., -0.7793,  1.1848,  0.0143],\n",
      "        [ 0.3115,  1.2819,  1.0466,  ..., -0.2717,  0.3988,  0.1265],\n",
      "        [-0.4989,  0.0384, -0.2930,  ..., -0.7682,  0.4926,  0.3768],\n",
      "        [ 0.4781,  0.1196, -0.4912,  ...,  0.0387, -0.3887, -0.2607]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "before: Parameter containing:\n",
      "tensor([[-0.0428,  0.0275, -0.0161,  ..., -0.0036,  0.0251,  0.0356],\n",
      "        [-0.0376,  0.0034,  0.0349,  ..., -0.0024,  0.0207,  0.0341],\n",
      "        [ 0.0212,  0.0041, -0.0154,  ...,  0.0037,  0.0003,  0.0075],\n",
      "        ...,\n",
      "        [-0.0059,  0.0205,  0.0327,  ...,  0.0228, -0.0251, -0.0077],\n",
      "        [ 0.0258, -0.0422,  0.0310,  ..., -0.0296, -0.0283, -0.0155],\n",
      "        [ 0.0249, -0.0296,  0.0196,  ..., -0.0062,  0.0433, -0.0263]],\n",
      "       requires_grad=True)\n",
      "\n",
      "after: Parameter containing:\n",
      "tensor([[-1.0428, -0.9725, -1.0161,  ..., -1.0036,  1.0251, -0.9644],\n",
      "        [ 0.9624,  1.0034,  1.0349,  ...,  0.9976, -0.9793,  1.0341],\n",
      "        [ 1.0212,  1.0041,  0.9846,  ...,  1.0037, -0.9997,  1.0075],\n",
      "        ...,\n",
      "        [-1.0059, -0.9795, -0.9673,  ..., -0.9772,  0.9749, -1.0077],\n",
      "        [ 1.0258,  0.9578,  1.0310,  ...,  0.9704, -1.0283,  0.9845],\n",
      "        [-0.9751, -1.0296, -0.9804,  ..., -1.0062,  1.0433, -1.0263]],\n",
      "       requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size, hidden_size =4, 512\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.z1 = None\n",
    "        self.w1 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_size, 1, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = self.w1(x)\n",
    "        print(z1)\n",
    "        self.z1 = z1.clone().detach()\n",
    "        z2 = self.w2(z1)\n",
    "        return z2\n",
    "\n",
    "model = Net()\n",
    "\n",
    "# example input sizes\n",
    "batch_size, hidden_size = 4, 512\n",
    "\n",
    "# create dummy data (bsz=4, hid=256)\n",
    "x = torch.randn(batch_size,hidden_size) \n",
    "out = model(x)\n",
    "#print(out)\n",
    "\n",
    "from torch.optim import RMSprop\n",
    "\n",
    "optimizer = RMSprop(model.parameters(), lr=1e-1)\n",
    "\n",
    "# craete dummy data (bsz=4)\n",
    "y = torch.tensor([[1.9], [9.5], [0.9], [1.2]])\n",
    "\n",
    "# compute mean square error loss\n",
    "L = torch.nn.functional.mse_loss(out, y)\n",
    "\n",
    "# check dtype of loss\n",
    "#f\"loss type = {L.dtype}\"\n",
    "L.backward()\n",
    "\n",
    "print(f'before: {model.w1.weight}\\n')\n",
    "optimizer.step()\n",
    "#print(model.w1.weight.grad)\n",
    "print(f'after: {model.w1.weight}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa20c9ed-23b8-4439-aedc-e54379f60e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9d2c959-da67-476a-bcf0-660546eb0ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "fp32_model= Net().to(\"cuda\")\n",
    "lr = 1e-0\n",
    "optimizer = SGD(fp32_model.parameters(), lr=lr)\n",
    "# print(lr)  #1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "282f56d0-7201-4eef-9a1e-232fa2421fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPU = 1.001953125 MiB'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"GPU = {torch.cuda.memory_allocated(0) / (1024 ** 2)} MiB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82a695d1-dba8-43eb-80bf-712b3f357c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0090,  0.0216, -0.0142,  ...,  0.0296,  0.0021, -0.0142],\n",
       "        [-0.0063, -0.0333,  0.0339,  ..., -0.0066, -0.0135, -0.0414],\n",
       "        [ 0.0342,  0.0385, -0.0415,  ..., -0.0416, -0.0054, -0.0287],\n",
       "        ...,\n",
       "        [ 0.0410,  0.0120,  0.0190,  ...,  0.0098, -0.0418,  0.0419],\n",
       "        [-0.0006,  0.0331,  0.0249,  ...,  0.0122,  0.0266, -0.0083],\n",
       "        [-0.0066, -0.0123, -0.0396,  ..., -0.0416, -0.0441,  0.0115]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp32_model.w1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9602758d-c346-480e-aef1-f2ba311e870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fp32_model.w2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22649bc8-db08-49c6-83d4-44d85a979aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0027,  0.1247,  0.6845,  ...,  0.2222,  0.3790,  0.0798],\n",
      "        [-0.5664, -0.8630, -0.5983,  ..., -0.0732,  0.0292, -1.0817],\n",
      "        [ 0.1397,  0.5587, -0.2522,  ..., -1.0445,  0.6003,  0.7344],\n",
      "        [ 0.4070, -0.2195,  0.7548,  ...,  0.2692,  0.7560,  0.6452]],\n",
      "       device='cuda:0', grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'logits type = torch.float32'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# example input sizes\n",
    "#batch_size, hidden_size =4, 8\n",
    "\n",
    "# create dummy data (bsz=4, hid=256)\n",
    "x = torch.randn(batch_size,hidden_size, dtype=torch.float, device=\"cuda\") \n",
    "\n",
    "# do forward\n",
    "z2 = fp32_model(x)\n",
    "\n",
    "# check dtypr of output logits\n",
    "f\"logits type = {z2.dtype}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d02965d-c80d-4889-8215-97cdacfbca92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loss type = torch.float32'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# craete dummy data (bsz=4)\n",
    "#y = torch.tensor([[1.9], [9.5], [0.9], [1.2]], dtype=torch.half, device=\"cuda\") #batch_size =4\n",
    "y = torch.tensor([[1.9], [9.5], [0.9], [1.2]], dtype=torch.float32, device=\"cuda\") #batch_size =4\n",
    "#y = torch.tensor([[1.9]], dtype=torch.float32, device=\"cuda\")\n",
    "#y = torch.tensor([[1.9], [0.5]], dtype=torch.float32, device=\"cuda\")\n",
    "# compute mean square error loss\n",
    "L = torch.nn.functional.mse_loss(z2, y)\n",
    "\n",
    "# check dtype of loss\n",
    "f\"loss type = {L.dtype}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae67a22d-da80-4ae1-9e21-a42fb1205b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(23.4768, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor([[ 0.2409],\n",
      "        [ 0.1095],\n",
      "        [-0.2919],\n",
      "        [-0.0456]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "tensor([[1.9000],\n",
      "        [9.5000],\n",
      "        [0.9000],\n",
      "        [1.2000]], device='cuda:0')\n",
      "tensor(23.4768, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(L)\n",
    "print(z2)\n",
    "print(y)\n",
    "loss = torch.sum((z2-y)**2/batch_size)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5aef694d-05a1-4e10-9161-bf8604f21e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: Parameter containing:\n",
      "tensor([[ 1.7143e-02, -4.8008e-03,  1.2542e-02,  2.2025e-02,  2.8882e-02,\n",
      "          1.8568e-02, -2.8732e-02,  1.5700e-02,  9.7479e-03, -2.9827e-02,\n",
      "          2.0754e-02, -3.6667e-02, -1.5246e-02, -1.4673e-02, -3.4143e-02,\n",
      "          1.8870e-02, -3.3328e-02,  1.1338e-02,  2.4312e-03, -4.0811e-03,\n",
      "         -1.4395e-02, -4.1016e-02, -4.3679e-02, -4.2365e-02, -2.3039e-02,\n",
      "          1.7028e-02,  9.3678e-03, -2.9893e-02, -2.7834e-02, -3.3187e-02,\n",
      "         -4.3348e-02,  2.3753e-03, -2.2288e-03,  3.7972e-02,  1.2616e-02,\n",
      "          3.8895e-02,  2.7593e-02,  1.4104e-02, -3.0829e-02,  1.9050e-02,\n",
      "          5.9025e-03,  7.7837e-03,  2.6884e-02,  4.1996e-02, -1.4450e-02,\n",
      "         -2.2397e-02, -5.9393e-03, -2.3913e-05,  3.6137e-02,  3.0431e-02,\n",
      "          4.0670e-02,  3.6752e-02, -3.9020e-02,  1.5637e-02, -2.7268e-02,\n",
      "         -1.2595e-02,  2.1426e-02,  9.0879e-05,  2.4869e-02, -4.1285e-02,\n",
      "         -3.9356e-02,  6.0615e-03,  1.8779e-03,  1.7123e-02,  1.0488e-02,\n",
      "         -3.3661e-03,  4.7980e-03, -4.1895e-02,  3.6435e-02,  2.4002e-02,\n",
      "          3.5289e-02,  3.4804e-02,  3.2696e-02, -1.3456e-03, -3.0258e-02,\n",
      "          5.4317e-03, -2.7693e-02, -3.1796e-02,  2.7278e-02, -2.3428e-02,\n",
      "         -1.7254e-02,  2.9608e-02,  1.5286e-02, -2.8931e-02, -1.4977e-02,\n",
      "          1.2329e-02,  9.9679e-03,  3.6776e-02,  5.8517e-03,  4.4856e-03,\n",
      "          1.1811e-02,  3.9531e-02,  1.8712e-02, -1.7365e-05,  2.6036e-02,\n",
      "          3.1895e-02,  2.5295e-02,  3.0273e-02,  4.1518e-02,  2.9619e-02,\n",
      "          7.9831e-03, -2.9561e-02, -8.5771e-03, -7.8231e-03, -3.3948e-02,\n",
      "          1.9572e-02,  4.0240e-02,  2.1530e-02,  3.0995e-02,  3.3108e-02,\n",
      "          5.3514e-03,  1.0953e-02, -1.3087e-02,  7.1965e-03,  8.8625e-03,\n",
      "         -2.6977e-02, -3.8092e-02,  3.7539e-02,  5.3151e-03,  1.1483e-02,\n",
      "         -3.3649e-03,  2.6912e-02,  1.5048e-02,  1.2880e-02,  2.6680e-02,\n",
      "          3.0863e-02,  3.3613e-02,  2.7062e-02,  4.0745e-02, -2.4930e-03,\n",
      "         -1.5409e-02,  2.1165e-02,  7.8844e-03,  8.3010e-03, -1.1422e-03,\n",
      "         -3.9921e-02,  4.1881e-03, -2.9853e-02,  1.5862e-02, -2.0140e-02,\n",
      "         -2.8434e-02,  3.9181e-02, -9.3265e-03, -1.3445e-03, -1.4142e-02,\n",
      "          1.5670e-03,  3.7501e-02,  3.8107e-02,  4.5386e-03, -5.9532e-03,\n",
      "          8.2170e-03,  3.7458e-02, -1.7530e-02,  3.3571e-02, -1.7323e-02,\n",
      "          6.3740e-04, -3.1916e-02, -4.1159e-02, -2.4103e-02, -3.0367e-02,\n",
      "          1.6460e-02, -2.5722e-02, -3.8696e-02,  8.8091e-03, -1.3947e-02,\n",
      "         -3.5258e-02,  3.1360e-02, -4.4272e-03, -3.9442e-02, -1.2709e-02,\n",
      "          1.2455e-02, -3.4951e-02, -2.6085e-02,  3.3874e-02, -3.0498e-02,\n",
      "         -2.8690e-02, -3.4675e-02,  4.3055e-02,  1.4655e-02, -3.9081e-03,\n",
      "         -3.2349e-02,  5.9025e-04, -3.2804e-02, -2.5039e-03,  4.1177e-02,\n",
      "          2.9130e-02,  2.4130e-02,  2.2309e-02,  2.9359e-02,  3.2981e-02,\n",
      "         -2.0086e-02,  3.2176e-02,  4.0411e-02,  2.6343e-02,  2.4450e-02,\n",
      "         -1.0426e-02,  2.9152e-02,  2.8588e-02, -4.8803e-03, -6.4112e-03,\n",
      "         -1.1253e-02,  2.8246e-02,  3.1732e-02,  2.3610e-02, -4.3166e-02,\n",
      "         -2.8819e-02,  3.1303e-02, -6.3709e-03, -3.5317e-02, -3.9971e-02,\n",
      "         -4.2584e-02,  1.6160e-02, -6.1430e-03,  4.2404e-02, -3.8547e-03,\n",
      "         -2.8233e-03, -3.9575e-02, -5.0029e-03, -3.9601e-02, -1.5022e-02,\n",
      "          3.4279e-02, -3.6830e-02,  2.9105e-02,  9.3017e-03, -3.1541e-02,\n",
      "         -3.3436e-02, -1.8493e-02,  1.2734e-02, -5.9732e-03, -1.0219e-02,\n",
      "          2.4501e-02,  2.0593e-03,  4.6684e-04, -3.7978e-02, -3.3512e-02,\n",
      "          4.0635e-02, -3.4573e-02, -3.8983e-02, -1.4969e-02, -1.2839e-02,\n",
      "         -1.1664e-02, -1.9651e-02, -3.3197e-02,  1.1866e-02,  3.9901e-02,\n",
      "          2.9781e-02, -4.0848e-02,  7.5262e-04,  9.9190e-03,  2.1180e-02,\n",
      "         -3.9393e-02, -4.8172e-03, -2.4060e-02,  1.5091e-02, -8.4752e-03,\n",
      "          2.7169e-02,  2.5076e-04, -3.6256e-02, -2.3785e-02, -4.0653e-02,\n",
      "         -3.1949e-02, -3.8159e-02,  4.3427e-02, -3.5188e-02, -2.5039e-02,\n",
      "         -1.3696e-02,  2.5103e-02,  8.3490e-03,  1.2443e-02,  1.5144e-02,\n",
      "          4.9168e-03, -3.1629e-02,  4.3267e-03,  6.6938e-03, -1.5077e-02,\n",
      "          3.7286e-02,  1.7427e-03, -3.6780e-02,  3.4056e-02,  3.4018e-02,\n",
      "         -1.5154e-02, -1.7995e-02, -1.6400e-02,  1.3118e-02,  2.0007e-02,\n",
      "         -1.1411e-02, -1.0369e-02, -8.0412e-03, -4.5789e-03, -2.8092e-02,\n",
      "         -1.1609e-02, -1.6563e-02, -4.2097e-02, -1.4906e-02, -1.8203e-02,\n",
      "          3.0573e-02, -2.4039e-02, -4.1078e-02,  6.5312e-04,  3.1293e-02,\n",
      "         -4.3136e-02,  3.2283e-02,  2.1260e-02, -2.1474e-02, -6.3562e-03,\n",
      "          1.1224e-02,  3.3739e-02,  4.3517e-02, -2.4970e-03, -4.2046e-02,\n",
      "          2.1198e-02, -2.8050e-02, -4.4662e-03,  4.3465e-02,  3.2508e-02,\n",
      "          1.7689e-03,  2.7826e-02, -1.7193e-02, -3.3569e-02, -1.4050e-02,\n",
      "          2.4110e-02, -3.8975e-02, -2.6146e-02, -6.5121e-03, -3.8692e-02,\n",
      "         -1.7287e-02, -2.2409e-02, -2.3010e-02, -3.3212e-02,  3.0569e-02,\n",
      "         -2.8225e-03, -3.1657e-02,  1.7243e-03, -1.0659e-02, -5.4001e-03,\n",
      "          7.9600e-03,  2.9426e-02,  1.5383e-02,  5.6250e-03,  4.3423e-02,\n",
      "         -6.8248e-03,  2.6164e-02,  7.9573e-03, -2.6748e-02,  1.2547e-02,\n",
      "         -5.9601e-03,  3.2798e-03,  3.9224e-02,  1.8425e-02,  7.4102e-03,\n",
      "         -1.5572e-02, -2.1950e-02, -2.3362e-02, -3.4824e-02,  8.3451e-03,\n",
      "         -6.9866e-03,  1.1875e-02, -3.0210e-02, -9.6349e-03, -8.4229e-04,\n",
      "         -2.9128e-02, -2.5504e-02, -3.7553e-02, -2.7728e-02, -5.8348e-03,\n",
      "          2.1707e-02,  3.2112e-02, -3.0273e-02, -3.1172e-02, -1.3451e-02,\n",
      "          8.1136e-03,  3.8987e-02,  7.4736e-03, -4.6450e-03,  4.0225e-02,\n",
      "         -8.8219e-03,  4.1232e-02, -1.5160e-02,  6.7792e-03,  3.9447e-02,\n",
      "          1.7789e-02,  2.5526e-02,  1.8578e-02,  1.2228e-02, -7.3306e-03,\n",
      "         -4.0117e-02,  3.8037e-02,  1.5079e-02,  7.3825e-03, -2.6144e-02,\n",
      "          1.7139e-02,  2.2047e-02,  1.8727e-02, -2.6153e-02, -9.8429e-03,\n",
      "          3.7312e-02, -1.5792e-02, -4.1770e-02,  1.7977e-03, -3.7472e-02,\n",
      "          5.8751e-04,  5.8627e-03,  3.8295e-02, -3.3624e-02, -1.9909e-02,\n",
      "         -2.6630e-03, -3.7542e-02, -1.5417e-03, -1.5010e-02,  3.0157e-03,\n",
      "         -1.9697e-02, -3.3875e-02, -2.4889e-02,  9.6520e-03, -1.7678e-02,\n",
      "         -7.3464e-03, -3.4555e-02, -2.9809e-02, -6.1817e-03,  2.7300e-02,\n",
      "         -4.3243e-02, -3.1099e-02,  1.1867e-02, -1.6684e-02, -5.3890e-05,\n",
      "         -2.4414e-02,  2.5968e-02,  3.9038e-02,  3.0146e-02, -3.3846e-02,\n",
      "          1.9037e-02,  1.9714e-02, -4.3761e-02, -1.1756e-02,  1.2097e-02,\n",
      "         -3.1893e-02, -4.2764e-02, -2.3784e-02, -1.1656e-02, -6.2409e-03,\n",
      "         -1.7526e-02,  1.3487e-02,  3.9920e-02, -3.5019e-02,  3.7840e-02,\n",
      "         -3.8855e-02, -5.1863e-03,  4.0255e-02, -3.0750e-02, -2.2179e-02,\n",
      "          1.4517e-02,  3.4002e-02,  3.7098e-02, -3.8098e-02,  2.2658e-02,\n",
      "         -2.5232e-02,  2.9611e-02, -1.7546e-02, -1.2742e-02,  2.0501e-02,\n",
      "         -1.9989e-02, -2.2746e-02, -4.1670e-02, -5.0501e-03, -1.1240e-02,\n",
      "          4.0702e-02,  2.7664e-02,  2.8416e-02,  9.5393e-04,  3.4508e-04,\n",
      "         -2.7185e-02, -1.2148e-02,  4.0239e-02,  2.1241e-03, -7.6657e-03,\n",
      "         -4.3718e-03, -1.7629e-02, -2.0631e-04,  3.1228e-02, -2.3924e-02,\n",
      "          4.0987e-02,  8.9472e-03, -3.4829e-03, -3.6761e-02, -1.4699e-03,\n",
      "         -3.7296e-02, -2.8943e-03, -3.3899e-03,  3.6608e-02,  3.4958e-02,\n",
      "          4.2940e-02,  1.1229e-02,  9.9369e-03,  5.3086e-04, -2.7646e-02,\n",
      "         -1.8458e-02,  1.8723e-02,  1.4443e-02, -9.7189e-03, -2.1879e-03,\n",
      "          3.4343e-02, -1.3096e-02, -1.9260e-02, -4.2515e-02,  3.6733e-02,\n",
      "          2.8999e-02, -1.3370e-02, -3.5628e-02,  2.6084e-02, -3.4787e-02,\n",
      "         -1.0392e-03, -2.7032e-02]], device='cuda:0', requires_grad=True)\n",
      "\n",
      "after: Parameter containing:\n",
      "tensor([[-2.3031e+00, -3.7569e+00, -1.9093e+00, -1.1558e+00,  1.5144e+00,\n",
      "         -2.1376e+00,  3.1573e+00, -6.6610e+00, -2.1626e+00,  1.1914e+00,\n",
      "          2.8083e-01, -4.3198e+00, -2.6501e-01, -5.7550e-01,  2.2060e+00,\n",
      "         -3.8633e+00, -8.9779e-01, -2.0973e+00, -1.0097e+00,  7.7415e-01,\n",
      "          1.3739e+00, -2.5509e+00,  1.0893e-01,  1.3025e+00,  3.4293e+00,\n",
      "         -3.2434e+00,  1.1550e+00,  3.0192e+00,  1.7659e+00, -5.7351e+00,\n",
      "         -7.6913e-01,  1.4812e+00,  2.4920e-02, -1.1006e+00,  3.9281e+00,\n",
      "         -3.2916e+00, -6.8308e+00, -7.3239e+00, -2.0314e+00,  5.3772e-01,\n",
      "          2.1186e+00, -7.2837e-01, -1.3848e+00,  1.4872e+00, -1.3750e+00,\n",
      "          7.4367e-01,  6.3773e+00, -2.4708e+00,  2.8928e+00, -9.5415e-01,\n",
      "         -3.9491e-01,  3.9481e+00, -3.7265e+00,  2.6161e+00, -3.1742e+00,\n",
      "          1.4817e+00,  1.3296e+00, -1.6648e+00,  3.7652e+00,  1.9157e-01,\n",
      "         -5.6816e+00, -3.8823e+00, -1.4313e-01,  3.9382e-01, -2.0690e-02,\n",
      "         -5.6638e+00, -1.0713e+00,  5.1236e+00, -1.6881e+00,  1.0509e+00,\n",
      "         -9.8233e-01,  1.8645e-01,  1.8945e+00,  1.0883e+00,  7.0031e-01,\n",
      "         -1.7013e+00, -3.3818e+00,  1.2905e+00, -2.0528e+00,  4.3697e+00,\n",
      "         -1.9063e-01,  4.2319e+00, -8.0757e-01,  1.9437e+00,  1.2101e+00,\n",
      "         -3.0945e+00, -1.5994e+00,  4.0292e-01,  3.1164e+00,  4.6536e+00,\n",
      "          3.2594e+00,  2.6203e+00,  4.2510e+00,  4.7069e+00, -1.4029e+00,\n",
      "          2.8930e+00, -2.6160e+00,  1.8336e+00, -2.5180e+00,  3.2804e+00,\n",
      "         -3.9131e+00, -1.6511e-01,  9.4661e-01,  1.3927e+00, -5.8917e-01,\n",
      "         -3.9069e+00,  3.0803e+00, -1.3525e+00, -3.9631e+00, -5.8386e-02,\n",
      "         -2.6997e+00, -1.3833e+00,  3.1982e+00, -3.2058e+00,  5.2838e+00,\n",
      "         -2.8750e+00,  2.5692e+00,  3.5508e+00,  3.6476e-01, -1.9257e+00,\n",
      "          7.2259e+00, -3.3399e+00,  4.9158e-01,  2.4110e+00,  1.0789e+00,\n",
      "         -6.7739e-01,  2.8137e+00,  1.0583e-01, -3.5134e-02, -2.9189e+00,\n",
      "          5.7069e-01, -2.5142e+00,  3.1994e+00,  7.6919e-01, -9.7537e-01,\n",
      "          2.9031e-02,  2.1381e+00, -1.3686e+00,  4.1146e+00, -2.7671e+00,\n",
      "          1.7538e-01,  2.6110e+00, -2.4287e+00,  1.0206e+00,  4.1750e+00,\n",
      "         -7.4956e-01,  2.7226e+00,  4.2884e+00,  2.8734e+00, -1.6092e+00,\n",
      "         -5.2728e+00, -5.3946e-01,  3.6161e-01, -4.9384e-01,  4.6369e+00,\n",
      "          5.5412e+00, -3.5895e+00,  1.3406e+00, -4.3792e-01,  2.9855e+00,\n",
      "         -2.4284e+00,  1.9462e+00,  1.1530e-01,  3.7563e+00, -3.4715e+00,\n",
      "          4.1752e+00, -3.4608e+00, -3.6967e+00,  1.2065e+00, -6.7734e-01,\n",
      "          2.6822e+00, -1.1111e+00,  1.5890e-01,  6.1449e-01, -1.7645e+00,\n",
      "         -5.0179e+00, -3.1858e+00,  8.0715e-01, -9.4051e-01,  2.6052e+00,\n",
      "          2.2872e+00,  3.5677e+00, -4.8513e+00, -7.7267e-01, -1.7511e+00,\n",
      "          9.4918e-01, -7.8331e-01,  1.6570e+00, -3.9760e-01,  2.8098e-01,\n",
      "          2.0799e+00, -6.1651e-01, -1.5115e+00,  7.3336e-01, -1.8306e+00,\n",
      "         -4.7764e+00,  1.5020e+00,  1.5723e+00,  1.5879e+00,  4.5484e+00,\n",
      "         -5.6868e+00,  2.3914e+00, -4.3250e+00,  1.4038e+00,  3.4216e+00,\n",
      "          4.7998e-01, -1.8060e+00,  7.4316e-01, -5.1016e-01, -2.9249e-01,\n",
      "          1.2767e+00,  6.5528e-01, -4.4065e+00,  4.2315e+00, -3.5866e-01,\n",
      "         -2.6173e+00,  2.4704e+00, -4.8703e+00,  1.3257e+00, -3.1875e+00,\n",
      "         -6.8197e-01, -9.6794e-01,  1.3464e-02,  2.2816e+00,  4.1877e-02,\n",
      "         -4.0400e+00, -5.8066e+00, -3.7681e+00,  3.8948e+00, -3.3976e+00,\n",
      "         -8.4231e-01, -4.8117e+00,  2.1314e+00,  2.6944e-01, -3.0646e+00,\n",
      "         -3.8099e+00, -3.3147e+00,  1.5253e+00, -4.0449e-01, -4.4496e+00,\n",
      "          2.0391e+00, -3.9844e+00,  5.4379e+00,  4.9860e-01,  4.3581e+00,\n",
      "          2.5057e+00,  1.8219e+00, -1.4833e-01,  1.6175e+00, -2.3805e+00,\n",
      "          4.1886e-01,  1.4510e+00,  3.4638e+00, -1.3039e+00,  2.2385e+00,\n",
      "         -3.5249e+00,  2.1608e+00,  6.5820e+00, -4.6916e+00, -2.4200e+00,\n",
      "          1.7168e+00, -3.1500e+00,  5.4599e-01,  2.7975e-01,  6.1955e+00,\n",
      "         -3.5689e+00,  2.2668e-01, -3.3918e+00, -1.8375e+00, -8.7817e-01,\n",
      "          2.2768e+00, -1.7089e+00,  6.2291e+00,  1.1044e+00, -8.1016e-01,\n",
      "          1.0113e+00, -8.5216e-01, -3.2590e+00, -7.9505e-01,  1.6390e-01,\n",
      "          3.5999e-01,  1.1336e+00, -1.8958e+00, -5.6655e-01,  3.3562e+00,\n",
      "          3.5219e+00, -2.2874e+00,  3.8828e-02, -1.9978e+00,  2.6347e+00,\n",
      "          4.9901e+00, -2.0748e+00, -1.8749e+00, -5.1303e-01,  1.3986e+00,\n",
      "         -6.6534e+00, -4.9396e-02, -1.0863e+00,  3.8282e+00, -8.6293e-01,\n",
      "          2.4837e+00,  2.3913e+00, -3.2745e+00, -1.1086e+00, -3.1949e+00,\n",
      "          6.0199e-01,  2.6464e+00, -4.5696e-02,  1.6077e+00, -5.9789e-01,\n",
      "          4.8501e-01, -2.8424e+00, -1.1471e+00,  5.7230e+00, -6.8214e-01,\n",
      "         -6.3622e+00,  4.8816e-01, -3.4025e+00,  5.8339e-01, -7.6423e-01,\n",
      "         -2.8072e+00, -1.8308e+00, -2.8642e+00, -5.2752e-01,  3.5285e+00,\n",
      "         -7.4184e-01,  7.0245e-01,  3.4509e+00,  1.3042e+00,  4.7622e+00,\n",
      "          5.0677e+00,  3.1559e+00,  3.4289e+00, -4.9739e-01, -9.0735e-01,\n",
      "         -2.0676e+00,  8.5162e-01,  2.4832e+00,  6.7721e+00, -6.7196e-01,\n",
      "          1.7180e+00, -4.3840e-01,  2.3630e+00,  2.3121e+00,  4.2822e+00,\n",
      "         -2.6984e+00,  5.0694e-01, -3.0450e+00, -3.5073e+00,  1.6227e+00,\n",
      "         -3.1408e+00,  2.1910e+00, -1.0359e-01,  3.4514e-01,  2.8727e+00,\n",
      "          5.9501e+00,  1.0270e+00, -2.0201e+00, -3.0608e+00,  8.8617e-01,\n",
      "         -7.9334e-03,  1.5095e-01,  3.4274e+00, -5.4805e+00, -2.5185e+00,\n",
      "          1.7526e+00, -4.2764e+00, -1.1697e+00, -4.7432e+00,  1.5719e+00,\n",
      "         -1.8283e+00, -3.1609e+00, -1.1745e+00,  4.7987e-01, -9.2145e-01,\n",
      "          3.3710e+00, -3.8891e-01, -2.4770e+00,  4.0699e-01, -5.1513e+00,\n",
      "         -3.4177e+00,  3.2686e+00, -1.9375e+00,  2.1306e+00,  2.2255e+00,\n",
      "          1.4769e-01,  1.0498e+00,  1.9935e+00,  7.0345e-01, -5.9847e+00,\n",
      "          1.9150e+00, -2.0959e+00,  4.6287e+00, -5.3559e+00,  4.8492e-01,\n",
      "          6.7454e-01,  7.7876e+00, -2.5456e+00,  7.3759e-01,  3.5961e+00,\n",
      "         -3.3119e+00,  6.6007e-01, -1.7576e+00,  5.3216e-01, -1.9483e+00,\n",
      "         -4.6949e-02,  4.3180e+00, -5.0404e+00, -3.1417e+00,  3.3560e+00,\n",
      "         -2.7452e+00,  2.0954e+00, -1.6542e+00,  1.4604e+00,  1.7953e+00,\n",
      "          1.9013e+00, -4.1551e+00, -2.7604e+00,  5.4716e-01, -6.8403e-01,\n",
      "          3.9483e+00,  6.6335e-01, -2.4057e+00,  2.4841e+00,  8.4136e-01,\n",
      "          4.2926e+00, -1.6241e+00,  1.5130e-01,  3.5336e+00,  5.1890e+00,\n",
      "         -2.3278e+00,  4.6526e+00,  1.0012e+00, -8.4104e-01,  2.1108e+00,\n",
      "          1.6734e+00, -2.3240e+00, -2.2559e+00,  1.3360e-01, -5.2414e+00,\n",
      "          1.8500e-01,  1.0675e+00,  3.4070e+00, -1.7279e+00, -1.6514e+00,\n",
      "          5.1955e+00,  3.9347e+00,  1.1157e+00, -3.3320e+00,  6.2443e+00,\n",
      "         -3.8438e+00,  4.4848e+00, -2.3239e-01,  1.3810e+00, -2.7562e+00,\n",
      "          5.3396e+00, -3.8726e-03, -1.5809e+00,  2.5808e+00,  6.1642e-01,\n",
      "         -6.6456e+00,  6.1743e-01,  5.3206e-01,  1.7151e+00,  1.6910e+00,\n",
      "          2.9789e+00, -2.8780e+00,  2.3873e+00, -2.3014e+00,  3.1483e+00,\n",
      "         -1.5021e+00,  9.2358e-01,  2.4984e+00,  2.1803e-01, -4.9274e-01,\n",
      "         -3.0967e+00, -4.9556e+00,  2.7271e-01, -8.7142e-01, -8.7379e-01,\n",
      "          1.1364e+00, -2.3410e+00, -2.7269e+00, -2.0312e+00,  4.7785e+00,\n",
      "         -1.7783e+00,  9.9531e-02,  1.3437e+00, -4.1508e-01, -1.0284e+00,\n",
      "          3.5747e+00, -1.7753e+00, -2.4274e+00, -3.6222e-01, -6.3615e+00,\n",
      "         -3.3863e+00, -2.5767e+00, -1.9380e+00, -2.7704e+00,  4.0397e-01,\n",
      "          2.6291e+00, -5.1624e-01, -4.1965e+00,  7.3239e+00, -2.8083e+00,\n",
      "         -3.1468e+00,  3.6915e+00, -1.2975e+00, -2.3316e-01, -6.4919e-01,\n",
      "          1.2789e+00, -4.2002e+00]], device='cuda:0', requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "L.backward()\n",
    "w2_weight = fp32_model.w2.weight.clone().detach()\n",
    "w1_weight = fp32_model.w1.weight.clone().detach()\n",
    "print(f'before: {fp32_model.w2.weight}\\n')\n",
    "optimizer.step()\n",
    "print(f'after: {fp32_model.w2.weight}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "781fba10-4815-4e48-a771-bcd19292e7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8296],\n",
      "        [-4.6953],\n",
      "        [-0.5960],\n",
      "        [-0.6228]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([4, 512])\n",
      "tensor([[ 0.0027,  0.1247,  0.6845,  ...,  0.2222,  0.3790,  0.0798],\n",
      "        [-0.5664, -0.8630, -0.5983,  ..., -0.0732,  0.0292, -1.0817],\n",
      "        [ 0.1397,  0.5587, -0.2522,  ..., -1.0445,  0.6003,  0.7344],\n",
      "        [ 0.4070, -0.2195,  0.7548,  ...,  0.2692,  0.7560,  0.6452]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 512])\n",
      "tensor([[ 2.3202,  3.7521,  1.9218,  1.1778, -1.4855,  2.1561, -3.1861,  6.6767,\n",
      "          2.1723, -1.2212, -0.2601,  4.2831,  0.2498,  0.5608, -2.2401,  3.8822,\n",
      "          0.8645,  2.1086,  1.0121, -0.7782, -1.3883,  2.5099, -0.1526, -1.3449,\n",
      "         -3.4523,  3.2604, -1.1456, -3.0491, -1.7937,  5.7019,  0.7258, -1.4789,\n",
      "         -0.0271,  1.1386, -3.9155,  3.3305,  6.8583,  7.3380,  2.0006, -0.5187,\n",
      "         -2.1127,  0.7362,  1.4117, -1.4453,  1.3606, -0.7661, -6.3832,  2.4708,\n",
      "         -2.8567,  0.9846,  0.4356, -3.9114,  3.6875, -2.6005,  3.1469, -1.4943,\n",
      "         -1.3082,  1.6649, -3.7403, -0.2329,  5.6422,  3.8884,  0.1450, -0.3767,\n",
      "          0.0312,  5.6604,  1.0761, -5.1655,  1.7245, -1.0269,  1.0176, -0.1517,\n",
      "         -1.8618, -1.0897, -0.7306,  1.7067,  3.3541, -1.3223,  2.0801, -4.3931,\n",
      "          0.1734, -4.2023,  0.8229, -1.9726, -1.2251,  3.1068,  1.6094, -0.3661,\n",
      "         -3.1105, -4.6491, -3.2475, -2.5808, -4.2323, -4.7069,  1.4290, -2.8611,\n",
      "          2.6413, -1.8033,  2.5595, -3.2508,  3.9211,  0.1355, -0.9552, -1.4006,\n",
      "          0.5552,  3.9265, -3.0400,  1.3740,  3.9941,  0.0915,  2.7051,  1.3943,\n",
      "         -3.2113,  3.2130, -5.2749,  2.8480, -2.6073, -3.5132, -0.3594,  1.9372,\n",
      "         -7.2293,  3.3668, -0.4765, -2.3981, -1.0523,  0.7083, -2.7800, -0.0788,\n",
      "          0.0759,  2.9164, -0.5861,  2.5354, -3.1915, -0.7609,  0.9742, -0.0690,\n",
      "         -2.1339,  1.3388, -4.0987,  2.7470, -0.2038, -2.5718,  2.4193, -1.0219,\n",
      "         -4.1891,  0.7511, -2.6851, -4.2503, -2.8689,  1.6032,  5.2810,  0.5769,\n",
      "         -0.3791,  0.5274, -4.6542, -5.5406,  3.5576, -1.3818,  0.4138, -3.0158,\n",
      "          2.4449, -1.9719, -0.1540, -3.7474,  3.4575, -4.2105,  3.4922,  3.6922,\n",
      "         -1.2460,  0.6646, -2.6697,  1.0761, -0.1850, -0.5806,  1.7340,  4.9892,\n",
      "          3.1511, -0.7641,  0.9552, -2.6091, -2.3195, -3.5671,  4.8185,  0.7702,\n",
      "          1.7923, -0.9201,  0.8074, -1.6347,  0.4270, -0.2480, -2.1000,  0.6487,\n",
      "          1.5519, -0.7070,  1.8550,  4.7660, -1.4728, -1.5437, -1.5927, -4.5548,\n",
      "          5.6756, -2.3631,  4.3567, -1.3802, -3.4647, -0.5088,  1.8374, -0.7495,\n",
      "          0.4748,  0.2525, -1.3193, -0.6391,  4.4004, -4.1891,  0.3548,  2.6144,\n",
      "         -2.5100,  4.8653, -1.3653,  3.1724,  0.7163,  0.9311,  0.0156, -2.2723,\n",
      "         -0.0734,  4.0066,  5.7881,  3.7808, -3.9007,  3.3874,  0.8668,  4.8137,\n",
      "         -2.1309, -0.3074,  3.0311,  3.8505,  3.2801, -1.5643,  0.3895,  4.4367,\n",
      "         -2.0507,  3.9648, -5.4711, -0.4867, -4.3182, -2.4759, -1.8627,  0.1491,\n",
      "         -1.6076,  2.4017, -0.4583, -1.4558, -3.4878,  1.3190, -2.2470,  3.5521,\n",
      "         -2.1606, -6.6182,  4.6678,  2.3794, -1.7488,  3.1118, -0.5026, -0.3149,\n",
      "         -6.2205,  3.5552, -0.2016,  3.4001,  1.8499,  0.8933, -2.2719,  1.6772,\n",
      "         -6.2248, -1.0977,  0.7951, -0.9740,  0.8539,  3.2222,  0.8291, -0.1299,\n",
      "         -0.3751, -1.1516,  1.8794,  0.5797, -3.3362, -3.5333,  2.2770, -0.0469,\n",
      "          1.9932, -2.6628, -5.0017,  2.0582,  1.8328,  0.4981, -1.4168,  6.6839,\n",
      "          0.0254,  1.0452, -3.8275,  0.8942, -2.5269, -2.3590,  3.2957,  1.0871,\n",
      "          3.1885, -0.5908, -2.6127,  0.0892, -1.6102,  0.5558, -0.4638,  2.8144,\n",
      "          1.1426, -5.6795,  0.7147,  6.3640, -0.4603,  3.3854, -0.6170,  0.7502,\n",
      "          2.8313,  1.7918,  2.8380,  0.5210, -3.5672,  0.7246, -0.7249, -3.4739,\n",
      "         -1.3374, -4.7316, -5.0705, -3.1876, -3.4272,  0.4867,  0.9020,  2.0755,\n",
      "         -0.8222, -2.4679, -6.7665,  0.7154, -1.7249,  0.4646, -2.3551, -2.3389,\n",
      "         -4.2697,  2.6925, -0.5037,  3.0843,  3.5257, -1.6153,  3.1253, -2.2129,\n",
      "          0.0802, -0.3800, -2.8644, -5.9571, -1.0151,  1.9899,  3.0511, -0.8870,\n",
      "         -0.0212, -0.1765, -3.4649,  5.4528,  2.5126, -1.7309,  4.3085,  1.1394,\n",
      "          4.7120, -1.5853,  1.8364,  3.1999,  1.1819, -0.4845,  0.9617, -3.3799,\n",
      "          0.4301,  2.4618, -0.4002,  5.1907,  3.4355, -3.2431,  1.9561, -2.1183,\n",
      "         -2.2329, -0.1878, -1.0117, -1.9784, -0.6961,  5.9586, -1.8979,  2.1180,\n",
      "         -4.6099,  5.3297, -0.4948, -0.6372, -7.8034,  2.5038, -0.7358, -3.6335,\n",
      "          3.3124, -0.6542,  1.7959, -0.5658,  1.9284,  0.0443, -4.3556,  5.0389,\n",
      "          3.1267, -3.3530,  2.7255, -2.1293,  1.6293, -1.4507, -1.8130, -1.9086,\n",
      "          4.1205,  2.7306, -0.5533,  0.7113, -3.9916, -0.6945,  2.4175, -2.5008,\n",
      "         -0.8414, -4.3170,  1.6501, -0.1123, -3.5035, -5.2228,  2.3469, -4.6329,\n",
      "         -1.0450,  0.8293, -2.0987, -1.7053,  2.2813,  2.2321, -0.1453,  5.2352,\n",
      "         -0.2025, -1.0540, -3.3670,  1.6928,  1.6892, -5.2344, -3.9398, -1.0755,\n",
      "          3.3013, -6.2665,  3.8583, -4.4508,  0.2695, -1.4191,  2.7788, -5.3649,\n",
      "          0.0335,  1.5634, -2.5936, -0.5959,  6.6256, -0.6402, -0.5737, -1.7201,\n",
      "         -1.7022, -2.9382,  2.9056, -2.3589,  2.3024, -3.1480,  1.4750, -0.9357,\n",
      "         -2.4581, -0.2159,  0.4851,  3.0923,  4.9380, -0.2729,  0.9026,  0.8499,\n",
      "         -1.0954,  2.3499,  2.7234,  1.9944, -4.7799,  1.7410, -0.1024, -1.3471,\n",
      "          0.4517,  1.0634, -3.5317,  1.7865,  2.4373,  0.3627,  6.3339,  3.3678,\n",
      "          2.5954,  1.9524,  2.7606, -0.4062, -2.5948,  0.5031,  4.1773, -7.3664,\n",
      "          2.8451,  3.1758, -3.7048,  1.2618,  0.2592,  0.6144, -1.2800,  4.1731]],\n",
      "       device='cuda:0', grad_fn=<MmBackward0>)\n",
      "tensor([[ 1.7143e-02, -4.8008e-03,  1.2542e-02,  2.2025e-02,  2.8882e-02,\n",
      "          1.8568e-02, -2.8732e-02,  1.5700e-02,  9.7479e-03, -2.9827e-02,\n",
      "          2.0754e-02, -3.6667e-02, -1.5246e-02, -1.4673e-02, -3.4143e-02,\n",
      "          1.8870e-02, -3.3328e-02,  1.1338e-02,  2.4312e-03, -4.0811e-03,\n",
      "         -1.4395e-02, -4.1016e-02, -4.3679e-02, -4.2365e-02, -2.3039e-02,\n",
      "          1.7028e-02,  9.3678e-03, -2.9893e-02, -2.7834e-02, -3.3187e-02,\n",
      "         -4.3348e-02,  2.3753e-03, -2.2288e-03,  3.7972e-02,  1.2616e-02,\n",
      "          3.8895e-02,  2.7593e-02,  1.4104e-02, -3.0829e-02,  1.9050e-02,\n",
      "          5.9025e-03,  7.7837e-03,  2.6884e-02,  4.1996e-02, -1.4450e-02,\n",
      "         -2.2397e-02, -5.9393e-03, -2.3913e-05,  3.6137e-02,  3.0431e-02,\n",
      "          4.0670e-02,  3.6752e-02, -3.9020e-02,  1.5637e-02, -2.7268e-02,\n",
      "         -1.2595e-02,  2.1426e-02,  9.0879e-05,  2.4869e-02, -4.1285e-02,\n",
      "         -3.9356e-02,  6.0615e-03,  1.8779e-03,  1.7123e-02,  1.0488e-02,\n",
      "         -3.3661e-03,  4.7980e-03, -4.1895e-02,  3.6435e-02,  2.4002e-02,\n",
      "          3.5289e-02,  3.4804e-02,  3.2696e-02, -1.3456e-03, -3.0258e-02,\n",
      "          5.4317e-03, -2.7693e-02, -3.1796e-02,  2.7278e-02, -2.3428e-02,\n",
      "         -1.7254e-02,  2.9608e-02,  1.5286e-02, -2.8931e-02, -1.4977e-02,\n",
      "          1.2329e-02,  9.9679e-03,  3.6776e-02,  5.8517e-03,  4.4856e-03,\n",
      "          1.1811e-02,  3.9531e-02,  1.8712e-02, -1.7365e-05,  2.6036e-02,\n",
      "          3.1895e-02,  2.5295e-02,  3.0273e-02,  4.1518e-02,  2.9619e-02,\n",
      "          7.9831e-03, -2.9561e-02, -8.5771e-03, -7.8231e-03, -3.3948e-02,\n",
      "          1.9572e-02,  4.0240e-02,  2.1530e-02,  3.0995e-02,  3.3108e-02,\n",
      "          5.3514e-03,  1.0953e-02, -1.3087e-02,  7.1965e-03,  8.8625e-03,\n",
      "         -2.6977e-02, -3.8092e-02,  3.7539e-02,  5.3151e-03,  1.1483e-02,\n",
      "         -3.3649e-03,  2.6912e-02,  1.5048e-02,  1.2880e-02,  2.6680e-02,\n",
      "          3.0863e-02,  3.3613e-02,  2.7062e-02,  4.0745e-02, -2.4930e-03,\n",
      "         -1.5409e-02,  2.1165e-02,  7.8844e-03,  8.3010e-03, -1.1422e-03,\n",
      "         -3.9921e-02,  4.1881e-03, -2.9853e-02,  1.5862e-02, -2.0140e-02,\n",
      "         -2.8434e-02,  3.9181e-02, -9.3265e-03, -1.3445e-03, -1.4142e-02,\n",
      "          1.5670e-03,  3.7501e-02,  3.8107e-02,  4.5386e-03, -5.9532e-03,\n",
      "          8.2170e-03,  3.7458e-02, -1.7530e-02,  3.3571e-02, -1.7323e-02,\n",
      "          6.3740e-04, -3.1916e-02, -4.1159e-02, -2.4103e-02, -3.0367e-02,\n",
      "          1.6460e-02, -2.5722e-02, -3.8696e-02,  8.8091e-03, -1.3947e-02,\n",
      "         -3.5258e-02,  3.1360e-02, -4.4272e-03, -3.9442e-02, -1.2709e-02,\n",
      "          1.2455e-02, -3.4951e-02, -2.6085e-02,  3.3874e-02, -3.0498e-02,\n",
      "         -2.8690e-02, -3.4675e-02,  4.3055e-02,  1.4655e-02, -3.9081e-03,\n",
      "         -3.2349e-02,  5.9025e-04, -3.2804e-02, -2.5039e-03,  4.1177e-02,\n",
      "          2.9130e-02,  2.4130e-02,  2.2309e-02,  2.9359e-02,  3.2981e-02,\n",
      "         -2.0086e-02,  3.2176e-02,  4.0411e-02,  2.6343e-02,  2.4450e-02,\n",
      "         -1.0426e-02,  2.9152e-02,  2.8588e-02, -4.8803e-03, -6.4112e-03,\n",
      "         -1.1253e-02,  2.8246e-02,  3.1732e-02,  2.3610e-02, -4.3166e-02,\n",
      "         -2.8819e-02,  3.1303e-02, -6.3709e-03, -3.5317e-02, -3.9971e-02,\n",
      "         -4.2584e-02,  1.6160e-02, -6.1430e-03,  4.2404e-02, -3.8547e-03,\n",
      "         -2.8233e-03, -3.9575e-02, -5.0029e-03, -3.9601e-02, -1.5022e-02,\n",
      "          3.4279e-02, -3.6830e-02,  2.9105e-02,  9.3017e-03, -3.1541e-02,\n",
      "         -3.3436e-02, -1.8493e-02,  1.2734e-02, -5.9732e-03, -1.0219e-02,\n",
      "          2.4501e-02,  2.0593e-03,  4.6684e-04, -3.7978e-02, -3.3512e-02,\n",
      "          4.0635e-02, -3.4573e-02, -3.8983e-02, -1.4969e-02, -1.2839e-02,\n",
      "         -1.1664e-02, -1.9651e-02, -3.3197e-02,  1.1866e-02,  3.9901e-02,\n",
      "          2.9781e-02, -4.0848e-02,  7.5262e-04,  9.9190e-03,  2.1180e-02,\n",
      "         -3.9393e-02, -4.8172e-03, -2.4060e-02,  1.5091e-02, -8.4752e-03,\n",
      "          2.7169e-02,  2.5076e-04, -3.6256e-02, -2.3785e-02, -4.0653e-02,\n",
      "         -3.1949e-02, -3.8159e-02,  4.3427e-02, -3.5188e-02, -2.5039e-02,\n",
      "         -1.3696e-02,  2.5103e-02,  8.3490e-03,  1.2443e-02,  1.5144e-02,\n",
      "          4.9168e-03, -3.1629e-02,  4.3267e-03,  6.6938e-03, -1.5077e-02,\n",
      "          3.7286e-02,  1.7427e-03, -3.6780e-02,  3.4056e-02,  3.4018e-02,\n",
      "         -1.5154e-02, -1.7995e-02, -1.6400e-02,  1.3118e-02,  2.0007e-02,\n",
      "         -1.1411e-02, -1.0369e-02, -8.0412e-03, -4.5789e-03, -2.8092e-02,\n",
      "         -1.1609e-02, -1.6563e-02, -4.2097e-02, -1.4906e-02, -1.8203e-02,\n",
      "          3.0573e-02, -2.4039e-02, -4.1078e-02,  6.5312e-04,  3.1293e-02,\n",
      "         -4.3136e-02,  3.2283e-02,  2.1260e-02, -2.1474e-02, -6.3562e-03,\n",
      "          1.1224e-02,  3.3739e-02,  4.3517e-02, -2.4970e-03, -4.2046e-02,\n",
      "          2.1198e-02, -2.8050e-02, -4.4662e-03,  4.3465e-02,  3.2508e-02,\n",
      "          1.7689e-03,  2.7826e-02, -1.7193e-02, -3.3569e-02, -1.4050e-02,\n",
      "          2.4110e-02, -3.8975e-02, -2.6146e-02, -6.5121e-03, -3.8692e-02,\n",
      "         -1.7287e-02, -2.2409e-02, -2.3010e-02, -3.3212e-02,  3.0569e-02,\n",
      "         -2.8225e-03, -3.1657e-02,  1.7243e-03, -1.0659e-02, -5.4001e-03,\n",
      "          7.9600e-03,  2.9426e-02,  1.5383e-02,  5.6250e-03,  4.3423e-02,\n",
      "         -6.8248e-03,  2.6164e-02,  7.9573e-03, -2.6748e-02,  1.2547e-02,\n",
      "         -5.9601e-03,  3.2798e-03,  3.9224e-02,  1.8425e-02,  7.4102e-03,\n",
      "         -1.5572e-02, -2.1950e-02, -2.3362e-02, -3.4824e-02,  8.3451e-03,\n",
      "         -6.9866e-03,  1.1875e-02, -3.0210e-02, -9.6349e-03, -8.4229e-04,\n",
      "         -2.9128e-02, -2.5504e-02, -3.7553e-02, -2.7728e-02, -5.8348e-03,\n",
      "          2.1707e-02,  3.2112e-02, -3.0273e-02, -3.1172e-02, -1.3451e-02,\n",
      "          8.1136e-03,  3.8987e-02,  7.4736e-03, -4.6450e-03,  4.0225e-02,\n",
      "         -8.8219e-03,  4.1232e-02, -1.5160e-02,  6.7792e-03,  3.9447e-02,\n",
      "          1.7789e-02,  2.5526e-02,  1.8578e-02,  1.2228e-02, -7.3306e-03,\n",
      "         -4.0117e-02,  3.8037e-02,  1.5079e-02,  7.3825e-03, -2.6144e-02,\n",
      "          1.7139e-02,  2.2047e-02,  1.8727e-02, -2.6153e-02, -9.8429e-03,\n",
      "          3.7312e-02, -1.5792e-02, -4.1770e-02,  1.7977e-03, -3.7472e-02,\n",
      "          5.8751e-04,  5.8627e-03,  3.8295e-02, -3.3624e-02, -1.9909e-02,\n",
      "         -2.6630e-03, -3.7542e-02, -1.5417e-03, -1.5010e-02,  3.0157e-03,\n",
      "         -1.9697e-02, -3.3875e-02, -2.4889e-02,  9.6520e-03, -1.7678e-02,\n",
      "         -7.3464e-03, -3.4555e-02, -2.9809e-02, -6.1817e-03,  2.7300e-02,\n",
      "         -4.3243e-02, -3.1099e-02,  1.1867e-02, -1.6684e-02, -5.3890e-05,\n",
      "         -2.4414e-02,  2.5968e-02,  3.9038e-02,  3.0146e-02, -3.3846e-02,\n",
      "          1.9037e-02,  1.9714e-02, -4.3761e-02, -1.1756e-02,  1.2097e-02,\n",
      "         -3.1893e-02, -4.2764e-02, -2.3784e-02, -1.1656e-02, -6.2409e-03,\n",
      "         -1.7526e-02,  1.3487e-02,  3.9920e-02, -3.5019e-02,  3.7840e-02,\n",
      "         -3.8855e-02, -5.1863e-03,  4.0255e-02, -3.0750e-02, -2.2179e-02,\n",
      "          1.4517e-02,  3.4002e-02,  3.7098e-02, -3.8098e-02,  2.2658e-02,\n",
      "         -2.5232e-02,  2.9611e-02, -1.7546e-02, -1.2742e-02,  2.0501e-02,\n",
      "         -1.9989e-02, -2.2746e-02, -4.1670e-02, -5.0501e-03, -1.1240e-02,\n",
      "          4.0702e-02,  2.7664e-02,  2.8416e-02,  9.5393e-04,  3.4508e-04,\n",
      "         -2.7185e-02, -1.2148e-02,  4.0239e-02,  2.1241e-03, -7.6657e-03,\n",
      "         -4.3718e-03, -1.7629e-02, -2.0631e-04,  3.1228e-02, -2.3924e-02,\n",
      "          4.0987e-02,  8.9472e-03, -3.4829e-03, -3.6761e-02, -1.4699e-03,\n",
      "         -3.7296e-02, -2.8943e-03, -3.3899e-03,  3.6608e-02,  3.4958e-02,\n",
      "          4.2940e-02,  1.1229e-02,  9.9369e-03,  5.3086e-04, -2.7646e-02,\n",
      "         -1.8458e-02,  1.8723e-02,  1.4443e-02, -9.7189e-03, -2.1879e-03,\n",
      "          3.4343e-02, -1.3096e-02, -1.9260e-02, -4.2515e-02,  3.6733e-02,\n",
      "          2.8999e-02, -1.3370e-02, -3.5628e-02,  2.6084e-02, -3.4787e-02,\n",
      "         -1.0392e-03, -2.7032e-02]], device='cuda:0')\n",
      "tensor([[-2.3031e+00, -3.7569e+00, -1.9093e+00, -1.1558e+00,  1.5144e+00,\n",
      "         -2.1376e+00,  3.1573e+00, -6.6610e+00, -2.1626e+00,  1.1914e+00,\n",
      "          2.8083e-01, -4.3198e+00, -2.6501e-01, -5.7550e-01,  2.2060e+00,\n",
      "         -3.8633e+00, -8.9779e-01, -2.0973e+00, -1.0097e+00,  7.7415e-01,\n",
      "          1.3739e+00, -2.5509e+00,  1.0893e-01,  1.3025e+00,  3.4293e+00,\n",
      "         -3.2434e+00,  1.1550e+00,  3.0192e+00,  1.7659e+00, -5.7351e+00,\n",
      "         -7.6913e-01,  1.4812e+00,  2.4920e-02, -1.1006e+00,  3.9281e+00,\n",
      "         -3.2916e+00, -6.8308e+00, -7.3239e+00, -2.0314e+00,  5.3772e-01,\n",
      "          2.1186e+00, -7.2837e-01, -1.3848e+00,  1.4872e+00, -1.3750e+00,\n",
      "          7.4367e-01,  6.3773e+00, -2.4708e+00,  2.8928e+00, -9.5415e-01,\n",
      "         -3.9491e-01,  3.9481e+00, -3.7265e+00,  2.6161e+00, -3.1742e+00,\n",
      "          1.4817e+00,  1.3296e+00, -1.6648e+00,  3.7652e+00,  1.9157e-01,\n",
      "         -5.6816e+00, -3.8823e+00, -1.4313e-01,  3.9382e-01, -2.0690e-02,\n",
      "         -5.6638e+00, -1.0713e+00,  5.1236e+00, -1.6881e+00,  1.0509e+00,\n",
      "         -9.8233e-01,  1.8645e-01,  1.8945e+00,  1.0883e+00,  7.0031e-01,\n",
      "         -1.7013e+00, -3.3818e+00,  1.2905e+00, -2.0528e+00,  4.3697e+00,\n",
      "         -1.9063e-01,  4.2319e+00, -8.0757e-01,  1.9437e+00,  1.2101e+00,\n",
      "         -3.0945e+00, -1.5994e+00,  4.0292e-01,  3.1164e+00,  4.6536e+00,\n",
      "          3.2594e+00,  2.6203e+00,  4.2510e+00,  4.7069e+00, -1.4029e+00,\n",
      "          2.8930e+00, -2.6160e+00,  1.8336e+00, -2.5180e+00,  3.2804e+00,\n",
      "         -3.9131e+00, -1.6511e-01,  9.4661e-01,  1.3927e+00, -5.8917e-01,\n",
      "         -3.9069e+00,  3.0803e+00, -1.3525e+00, -3.9631e+00, -5.8386e-02,\n",
      "         -2.6997e+00, -1.3833e+00,  3.1982e+00, -3.2058e+00,  5.2838e+00,\n",
      "         -2.8750e+00,  2.5692e+00,  3.5508e+00,  3.6476e-01, -1.9257e+00,\n",
      "          7.2259e+00, -3.3399e+00,  4.9158e-01,  2.4110e+00,  1.0789e+00,\n",
      "         -6.7739e-01,  2.8137e+00,  1.0583e-01, -3.5134e-02, -2.9189e+00,\n",
      "          5.7069e-01, -2.5142e+00,  3.1994e+00,  7.6919e-01, -9.7537e-01,\n",
      "          2.9031e-02,  2.1381e+00, -1.3686e+00,  4.1146e+00, -2.7671e+00,\n",
      "          1.7538e-01,  2.6110e+00, -2.4287e+00,  1.0206e+00,  4.1750e+00,\n",
      "         -7.4956e-01,  2.7226e+00,  4.2884e+00,  2.8734e+00, -1.6092e+00,\n",
      "         -5.2728e+00, -5.3946e-01,  3.6161e-01, -4.9384e-01,  4.6369e+00,\n",
      "          5.5412e+00, -3.5895e+00,  1.3406e+00, -4.3792e-01,  2.9855e+00,\n",
      "         -2.4284e+00,  1.9462e+00,  1.1530e-01,  3.7563e+00, -3.4715e+00,\n",
      "          4.1752e+00, -3.4608e+00, -3.6967e+00,  1.2065e+00, -6.7734e-01,\n",
      "          2.6822e+00, -1.1111e+00,  1.5890e-01,  6.1449e-01, -1.7645e+00,\n",
      "         -5.0179e+00, -3.1858e+00,  8.0715e-01, -9.4051e-01,  2.6052e+00,\n",
      "          2.2872e+00,  3.5677e+00, -4.8513e+00, -7.7267e-01, -1.7511e+00,\n",
      "          9.4918e-01, -7.8331e-01,  1.6570e+00, -3.9760e-01,  2.8098e-01,\n",
      "          2.0799e+00, -6.1651e-01, -1.5115e+00,  7.3336e-01, -1.8306e+00,\n",
      "         -4.7764e+00,  1.5020e+00,  1.5723e+00,  1.5879e+00,  4.5484e+00,\n",
      "         -5.6868e+00,  2.3914e+00, -4.3250e+00,  1.4038e+00,  3.4216e+00,\n",
      "          4.7998e-01, -1.8060e+00,  7.4316e-01, -5.1016e-01, -2.9249e-01,\n",
      "          1.2767e+00,  6.5528e-01, -4.4065e+00,  4.2315e+00, -3.5866e-01,\n",
      "         -2.6173e+00,  2.4704e+00, -4.8703e+00,  1.3257e+00, -3.1875e+00,\n",
      "         -6.8197e-01, -9.6794e-01,  1.3464e-02,  2.2816e+00,  4.1877e-02,\n",
      "         -4.0400e+00, -5.8066e+00, -3.7681e+00,  3.8948e+00, -3.3976e+00,\n",
      "         -8.4231e-01, -4.8117e+00,  2.1314e+00,  2.6944e-01, -3.0646e+00,\n",
      "         -3.8099e+00, -3.3147e+00,  1.5253e+00, -4.0449e-01, -4.4496e+00,\n",
      "          2.0391e+00, -3.9844e+00,  5.4379e+00,  4.9860e-01,  4.3581e+00,\n",
      "          2.5057e+00,  1.8219e+00, -1.4833e-01,  1.6175e+00, -2.3805e+00,\n",
      "          4.1886e-01,  1.4510e+00,  3.4638e+00, -1.3039e+00,  2.2385e+00,\n",
      "         -3.5249e+00,  2.1608e+00,  6.5820e+00, -4.6916e+00, -2.4200e+00,\n",
      "          1.7168e+00, -3.1500e+00,  5.4599e-01,  2.7975e-01,  6.1955e+00,\n",
      "         -3.5689e+00,  2.2668e-01, -3.3918e+00, -1.8375e+00, -8.7817e-01,\n",
      "          2.2768e+00, -1.7089e+00,  6.2291e+00,  1.1044e+00, -8.1016e-01,\n",
      "          1.0113e+00, -8.5216e-01, -3.2590e+00, -7.9505e-01,  1.6390e-01,\n",
      "          3.5999e-01,  1.1336e+00, -1.8958e+00, -5.6655e-01,  3.3562e+00,\n",
      "          3.5219e+00, -2.2874e+00,  3.8828e-02, -1.9978e+00,  2.6347e+00,\n",
      "          4.9901e+00, -2.0748e+00, -1.8749e+00, -5.1303e-01,  1.3986e+00,\n",
      "         -6.6534e+00, -4.9396e-02, -1.0863e+00,  3.8282e+00, -8.6293e-01,\n",
      "          2.4837e+00,  2.3913e+00, -3.2745e+00, -1.1086e+00, -3.1949e+00,\n",
      "          6.0199e-01,  2.6464e+00, -4.5696e-02,  1.6077e+00, -5.9789e-01,\n",
      "          4.8501e-01, -2.8424e+00, -1.1471e+00,  5.7230e+00, -6.8214e-01,\n",
      "         -6.3622e+00,  4.8816e-01, -3.4025e+00,  5.8339e-01, -7.6423e-01,\n",
      "         -2.8072e+00, -1.8308e+00, -2.8642e+00, -5.2752e-01,  3.5285e+00,\n",
      "         -7.4184e-01,  7.0245e-01,  3.4509e+00,  1.3042e+00,  4.7622e+00,\n",
      "          5.0677e+00,  3.1559e+00,  3.4289e+00, -4.9739e-01, -9.0735e-01,\n",
      "         -2.0676e+00,  8.5162e-01,  2.4832e+00,  6.7721e+00, -6.7196e-01,\n",
      "          1.7180e+00, -4.3840e-01,  2.3630e+00,  2.3121e+00,  4.2822e+00,\n",
      "         -2.6984e+00,  5.0694e-01, -3.0450e+00, -3.5073e+00,  1.6227e+00,\n",
      "         -3.1408e+00,  2.1910e+00, -1.0359e-01,  3.4514e-01,  2.8727e+00,\n",
      "          5.9501e+00,  1.0270e+00, -2.0201e+00, -3.0608e+00,  8.8617e-01,\n",
      "         -7.9334e-03,  1.5095e-01,  3.4274e+00, -5.4805e+00, -2.5185e+00,\n",
      "          1.7526e+00, -4.2764e+00, -1.1697e+00, -4.7432e+00,  1.5719e+00,\n",
      "         -1.8283e+00, -3.1609e+00, -1.1745e+00,  4.7987e-01, -9.2145e-01,\n",
      "          3.3710e+00, -3.8891e-01, -2.4770e+00,  4.0699e-01, -5.1513e+00,\n",
      "         -3.4177e+00,  3.2686e+00, -1.9375e+00,  2.1306e+00,  2.2255e+00,\n",
      "          1.4769e-01,  1.0498e+00,  1.9935e+00,  7.0345e-01, -5.9847e+00,\n",
      "          1.9150e+00, -2.0959e+00,  4.6287e+00, -5.3559e+00,  4.8492e-01,\n",
      "          6.7454e-01,  7.7876e+00, -2.5456e+00,  7.3759e-01,  3.5961e+00,\n",
      "         -3.3119e+00,  6.6007e-01, -1.7576e+00,  5.3216e-01, -1.9483e+00,\n",
      "         -4.6949e-02,  4.3180e+00, -5.0404e+00, -3.1417e+00,  3.3560e+00,\n",
      "         -2.7452e+00,  2.0954e+00, -1.6542e+00,  1.4604e+00,  1.7953e+00,\n",
      "          1.9013e+00, -4.1551e+00, -2.7604e+00,  5.4716e-01, -6.8403e-01,\n",
      "          3.9483e+00,  6.6335e-01, -2.4057e+00,  2.4841e+00,  8.4136e-01,\n",
      "          4.2926e+00, -1.6241e+00,  1.5130e-01,  3.5336e+00,  5.1890e+00,\n",
      "         -2.3278e+00,  4.6526e+00,  1.0012e+00, -8.4104e-01,  2.1108e+00,\n",
      "          1.6734e+00, -2.3240e+00, -2.2559e+00,  1.3360e-01, -5.2414e+00,\n",
      "          1.8500e-01,  1.0675e+00,  3.4070e+00, -1.7279e+00, -1.6514e+00,\n",
      "          5.1955e+00,  3.9347e+00,  1.1157e+00, -3.3320e+00,  6.2443e+00,\n",
      "         -3.8438e+00,  4.4848e+00, -2.3239e-01,  1.3810e+00, -2.7562e+00,\n",
      "          5.3396e+00, -3.8726e-03, -1.5809e+00,  2.5808e+00,  6.1642e-01,\n",
      "         -6.6456e+00,  6.1743e-01,  5.3206e-01,  1.7151e+00,  1.6910e+00,\n",
      "          2.9789e+00, -2.8780e+00,  2.3873e+00, -2.3014e+00,  3.1483e+00,\n",
      "         -1.5021e+00,  9.2358e-01,  2.4984e+00,  2.1803e-01, -4.9274e-01,\n",
      "         -3.0967e+00, -4.9556e+00,  2.7271e-01, -8.7142e-01, -8.7379e-01,\n",
      "          1.1364e+00, -2.3410e+00, -2.7269e+00, -2.0312e+00,  4.7785e+00,\n",
      "         -1.7783e+00,  9.9531e-02,  1.3437e+00, -4.1508e-01, -1.0284e+00,\n",
      "          3.5747e+00, -1.7753e+00, -2.4274e+00, -3.6222e-01, -6.3615e+00,\n",
      "         -3.3863e+00, -2.5767e+00, -1.9380e+00, -2.7704e+00,  4.0397e-01,\n",
      "          2.6291e+00, -5.1624e-01, -4.1965e+00,  7.3239e+00, -2.8083e+00,\n",
      "         -3.1468e+00,  3.6915e+00, -1.2975e+00, -2.3316e-01, -6.4919e-01,\n",
      "          1.2789e+00, -4.2002e+00]], device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "DL_Dz2= 2 * (z2 - y) / batch_size # DL/Dz2  (BWD-activation: layer2), in case of MSE\n",
    "print(DL_Dz2) # [4,1] [batch_size, output_size=1] DL/Dz2\n",
    "Dz2_Dw2 = fp32_model.z1.clone().detach() #Dz2/Dw2\n",
    "print(Dz2_Dw2.shape) #[4,8] [batch_size, hidden_size] \n",
    "print(Dz2_Dw2)\n",
    "#DL_Dw2 = DL_Dz2.T * Dz2_Dw2\n",
    "DL_Dw2 = torch.matmul(DL_Dz2.T, Dz2_Dw2) #[1,4] * [4,8] batch_size, hidden_size\n",
    "print(DL_Dw2.shape) # [1,8] [output_size=1, hidden_size]\n",
    "print(DL_Dw2)\n",
    "print(w2_weight) # [hidden_size=8 , output_size=1]\n",
    "print(w2_weight - lr * DL_Dw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2dbab75-ab40-44ef-a2eb-3a73f57026ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1225, -0.0400, -0.0139,  ...,  0.0628, -0.0690,  0.0354],\n",
       "        [ 0.0255, -0.0160,  0.0338,  ..., -0.0159,  0.0065, -0.0553],\n",
       "        [-0.0489, -0.0066, -0.0412,  ..., -0.0173, -0.0575,  0.0076],\n",
       "        ...,\n",
       "        [ 0.2714,  0.1371,  0.0183,  ..., -0.0575,  0.1026, -0.0588],\n",
       "        [ 0.0063,  0.0368,  0.0248,  ...,  0.0102,  0.0309, -0.0113],\n",
       "        [ 0.1724,  0.0849, -0.0402,  ..., -0.0939,  0.0681, -0.0668]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp32_model.w1.weight # w1 = [hidden_size, hidden_size] [8,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0166765e-a2a7-456b-89a9-5c3accbc5658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8296],\n",
      "        [-4.6953],\n",
      "        [-0.5960],\n",
      "        [-0.6228]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([1, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([4, 512])\n",
      "torch.Size([512, 512])\n",
      "tensor([[-0.1225, -0.0400, -0.0139,  ...,  0.0628, -0.0690,  0.0354],\n",
      "        [ 0.0255, -0.0160,  0.0338,  ..., -0.0159,  0.0065, -0.0553],\n",
      "        [-0.0489, -0.0066, -0.0412,  ..., -0.0173, -0.0575,  0.0076],\n",
      "        ...,\n",
      "        [ 0.2714,  0.1371,  0.0183,  ..., -0.0575,  0.1026, -0.0588],\n",
      "        [ 0.0063,  0.0368,  0.0248,  ...,  0.0102,  0.0309, -0.0113],\n",
      "        [ 0.1724,  0.0849, -0.0402,  ..., -0.0939,  0.0681, -0.0668]],\n",
      "       device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "DL_Dz2= 2 * (z2 - y) / batch_size # DL/Dz2  (BWD-activation: layer2), in case of MSE\n",
    "print(DL_Dz2) # [4,1] [batch_size=4, output_size=1]\n",
    "print(w2_weight.shape) # [1,8]\n",
    "temp = torch.matmul(DL_Dz2, w2_weight) #DL/Dz2 * w2\n",
    "print(temp.shape) # [4,8]\n",
    "print(x.shape) # [4,8]\n",
    "DL_Dw1 = torch.matmul(temp.T, x) # [8,4] * [4,8] = [8,8]\n",
    "print(DL_Dw1.shape) #[8,8]\n",
    "print(w1_weight - lr * DL_Dw1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20942146-5f0b-4c2c-b02a-5c0e9255a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(512, 512, bias=False)\n",
    "        self.w2 = nn.Linear(512, 1, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = self.w1(x)\n",
    "        z2 = self.w2(z1)\n",
    "        return z2\n",
    "\n",
    "from torch.optim import SGD\n",
    "\n",
    "fp32_model= Net().to(\"cuda\")\n",
    "optimizer = SGD(fp32_model.parameters(), lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3f77d7-ecc4-461b-b36f-81139c9eb1b5",
   "metadata": {},
   "source": [
    "### Float2Half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a79d4a4a-92ec-4cbc-9378-36f18a8d0c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp16_model = Net().half().to(\"cuda\")\n",
    "fp16_model.load_state_dict(fp32_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ffd276-1902-46c2-86de-83abd542298b",
   "metadata": {},
   "source": [
    "### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3877d42-2c6d-44d6-9df1-1f6778f2758f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logits type = torch.float16'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# example input sizes\n",
    "batch_size, hidden_size = 4, 512\n",
    "\n",
    "# create dummy data (bsz=4, hid=256)\n",
    "x = torch.randn(batch_size,hidden_size, dtype=torch.half, device=\"cuda\") \n",
    "\n",
    "# do forward\n",
    "z2 = fp16_model(x)\n",
    "\n",
    "# check dtypr of output logits\n",
    "f\"logits type = {z2.dtype}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afdde465-eb37-4d50-87ec-b561e8c21f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loss type = torch.float16'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# craete dummy data (bsz=4)\n",
    "y = torch.tensor([[1.9], [9.5], [0.9], [1.2]], dtype=torch.half, device=\"cuda\")\n",
    "\n",
    "# compute mean square error loss\n",
    "L = torch.nn.functional.mse_loss(z2, y)\n",
    "\n",
    "# check dtype of loss\n",
    "f\"loss type = {L.dtype}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd8bbdd-8569-41d0-87cc-93f842e88377",
   "metadata": {},
   "source": [
    "### Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0dfa9df7-48b2-419c-85c5-73256959455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss scaling\n",
    "L *= 1024\n",
    "\n",
    "# do backward\n",
    "L.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e3d467-45c6-4d1d-864b-f34cb185fe10",
   "metadata": {},
   "source": [
    "### Update Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e749deb2-6b96-4595-b3f1-c3d05e6d147f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: Parameter containing:\n",
      "tensor([[ 0.0108,  0.0091,  0.0023,  ..., -0.0054,  0.0047, -0.0132],\n",
      "        [-0.0140, -0.0182,  0.0112,  ...,  0.0318,  0.0181, -0.0185],\n",
      "        [-0.0333,  0.0409,  0.0015,  ..., -0.0210, -0.0003, -0.0340],\n",
      "        ...,\n",
      "        [ 0.0270,  0.0102,  0.0430,  ..., -0.0134, -0.0132, -0.0054],\n",
      "        [-0.0426, -0.0003, -0.0204,  ...,  0.0396, -0.0296,  0.0336],\n",
      "        [ 0.0124, -0.0283,  0.0418,  ..., -0.0426, -0.0158,  0.0190]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "\n",
      "after: Parameter containing:\n",
      "tensor([[ 0.0108,  0.0091,  0.0023,  ..., -0.0054,  0.0047, -0.0132],\n",
      "        [-0.0140, -0.0182,  0.0112,  ...,  0.0318,  0.0181, -0.0185],\n",
      "        [-0.0333,  0.0409,  0.0015,  ..., -0.0210, -0.0003, -0.0340],\n",
      "        ...,\n",
      "        [ 0.0270,  0.0102,  0.0430,  ..., -0.0134, -0.0132, -0.0054],\n",
      "        [-0.0426, -0.0003, -0.0204,  ...,  0.0396, -0.0296,  0.0336],\n",
      "        [ 0.0124, -0.0283,  0.0418,  ..., -0.0426, -0.0158,  0.0190]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'before: {fp32_model.w1.weight}\\n')\n",
    "optimizer.step()\n",
    "print(f'after: {fp32_model.w1.weight}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65aa4065-1520-444a-b6c3-99c1b57b9711",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f'before: {fp16_model.w1.weight}\\n')\n",
    "#optimizer.step()\n",
    "#print(f'after: {fp16_model.w1.weight}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "559310f0-09bc-4636-923d-9502f1f17f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy gradient to FP32 model\n",
    "fp32_model.w1.weight.grad = fp16_model.w1.weight.grad.float()\n",
    "fp32_model.w2.weight.grad = fp16_model.w2.weight.grad.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92b6ac53-8d55-4732-b125-0b588078c1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: Parameter containing:\n",
      "tensor([[ 0.0108,  0.0091,  0.0023,  ..., -0.0054,  0.0047, -0.0132],\n",
      "        [-0.0140, -0.0182,  0.0112,  ...,  0.0318,  0.0181, -0.0185],\n",
      "        [-0.0333,  0.0409,  0.0015,  ..., -0.0210, -0.0003, -0.0340],\n",
      "        ...,\n",
      "        [ 0.0270,  0.0102,  0.0430,  ..., -0.0134, -0.0132, -0.0054],\n",
      "        [-0.0426, -0.0003, -0.0204,  ...,  0.0396, -0.0296,  0.0336],\n",
      "        [ 0.0124, -0.0283,  0.0418,  ..., -0.0426, -0.0158,  0.0190]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "\n",
      "after: Parameter containing:\n",
      "tensor([[-1.3729,  1.3303, -1.7740,  ..., -0.2675,  0.0519, -0.2462],\n",
      "        [-0.4777,  0.4250, -0.5844,  ..., -0.0561,  0.0340, -0.0966],\n",
      "        [ 0.6248, -0.5875,  0.8459,  ...,  0.1036, -0.0227,  0.0767],\n",
      "        ...,\n",
      "        [ 1.2351, -1.1442,  1.5943,  ...,  0.2154, -0.0543,  0.1980],\n",
      "        [-0.0147, -0.0270,  0.0153,  ...,  0.0449, -0.0306,  0.0383],\n",
      "        [ 0.6768, -0.6630,  0.8949,  ...,  0.0833, -0.0385,  0.1309]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'before: {fp32_model.w1.weight}\\n')\n",
    "optimizer.step()\n",
    "print(f'after: {fp32_model.w1.weight}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ac8890c-f6a8-4a06-958d-25eb613a593b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = 425.021952 MiB\n",
      "step memory allocate: 823.834M\n",
      "step memory allocate: 823.834M\n",
      "CPU times: user 439 ms, sys: 24.3 ms, total: 463 ms\n",
      "Wall time: 462 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda import max_memory_allocated\n",
    "\n",
    "# example input sizes\n",
    "batch_size, hidden_size = 4, 10000\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_size, 1, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = self.w1(x)\n",
    "        z2 = self.w2(z1)\n",
    "        return z2\n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "torch.cuda.init()\n",
    "\n",
    "fp32_model= Net().to(\"cuda\")\n",
    "optimizer = SGD(fp32_model.parameters(), lr=1e-2)\n",
    "#optimizer = SGD(fp32_model.parameters(), lr=1e-2, momentum=0.9)\n",
    "#optimizer = Adam(fp32_model.parameters(), lr=1e-2)\n",
    "print(f\"GPU = {torch.cuda.max_memory_allocated(0) / 1e6} MiB\")\n",
    "\n",
    "\n",
    "# create dummy data (bsz=4, hid=256)\n",
    "x = torch.randn(batch_size,hidden_size, device=\"cuda\") \n",
    "y = torch.tensor([[1.0], [1.0], [1.0], [1.0]], device=\"cuda\")\n",
    "\n",
    "#torch.cuda.init()\n",
    "for _ in range(2):\n",
    "    optimizer.zero_grad()\n",
    "    z2 = fp32_model(x)\n",
    "    loss = torch.nn.functional.mse_loss(z2, y)       \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    memory = max_memory_allocated()\n",
    "    print(f'step memory allocate: {memory / 1e6:.3f}M')\n",
    "    #torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c70561bd-cedd-4b4e-a62c-e73a42b2ea16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0035,  0.0036, -0.0069,  ..., -0.0051,  0.0005, -0.0023],\n",
       "         [ 0.0095, -0.0065, -0.0096,  ...,  0.0044, -0.0077,  0.0064],\n",
       "         [-0.0084,  0.0059,  0.0059,  ...,  0.0005,  0.0005, -0.0094],\n",
       "         ...,\n",
       "         [ 0.0104,  0.0063,  0.0008,  ..., -0.0070, -0.0085,  0.0130],\n",
       "         [ 0.0031,  0.0068, -0.0001,  ..., -0.0007, -0.0047, -0.0048],\n",
       "         [-0.0065,  0.0017,  0.0044,  ..., -0.0092,  0.0024,  0.0034]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.1063, -0.0443,  0.2530,  ..., -0.1839,  0.2202,  0.1995]],\n",
       "        device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups[0]['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2110f8a9-ee6c-4519-a99d-3b75fedefc94",
   "metadata": {},
   "source": [
    "### float32 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d13e679-ad05-41ff-9e32-f4132c5a7d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = 10823.873024 MiB\n",
      "loss: 6.92418098449707\n",
      "before weight: Parameter containing:\n",
      "tensor([[-2.6025e-03, -7.1219e-04,  4.2557e-03,  ..., -2.6104e-03,\n",
      "         -8.3125e-05, -4.4543e-03],\n",
      "        [-3.6769e-03, -2.2897e-03,  3.6415e-03,  ...,  2.3379e-03,\n",
      "          2.1910e-03,  1.1797e-03],\n",
      "        [-1.4701e-03,  2.5678e-03,  3.1793e-03,  ..., -2.3515e-03,\n",
      "         -4.3107e-03, -4.1770e-03],\n",
      "        ...,\n",
      "        [ 1.1568e-03,  6.1297e-04, -2.2651e-03,  ..., -7.7470e-04,\n",
      "          1.5991e-03,  4.3360e-03],\n",
      "        [-3.1571e-03,  1.1334e-03, -2.0015e-03,  ..., -3.6614e-03,\n",
      "          1.4878e-03, -2.1269e-03],\n",
      "        [ 2.2894e-04,  1.0092e-04,  3.2273e-03,  ..., -7.4672e-04,\n",
      "          1.9045e-03, -2.1200e-03]], device='cuda:0', requires_grad=True)\n",
      "\n",
      "after weight: Parameter containing:\n",
      "tensor([[-2.6021e-03, -7.1179e-04,  4.2561e-03,  ..., -2.6100e-03,\n",
      "         -8.2722e-05, -4.4539e-03],\n",
      "        [-3.6769e-03, -2.2896e-03,  3.6415e-03,  ...,  2.3380e-03,\n",
      "          2.1911e-03,  1.1797e-03],\n",
      "        [-1.4695e-03,  2.5684e-03,  3.1798e-03,  ..., -2.3510e-03,\n",
      "         -4.3102e-03, -4.1764e-03],\n",
      "        ...,\n",
      "        [ 1.1560e-03,  6.1223e-04, -2.2659e-03,  ..., -7.7543e-04,\n",
      "          1.5983e-03,  4.3352e-03],\n",
      "        [-3.1582e-03,  1.1323e-03, -2.0026e-03,  ..., -3.6624e-03,\n",
      "          1.4867e-03, -2.1280e-03],\n",
      "        [ 2.2854e-04,  1.0052e-04,  3.2269e-03,  ..., -7.4711e-04,\n",
      "          1.9041e-03, -2.1203e-03]], device='cuda:0', requires_grad=True)\n",
      "\n",
      "step memory allocate: 30824.917M\n",
      "loss: 37.812984466552734\n",
      "before weight: Parameter containing:\n",
      "tensor([[-2.6021e-03, -7.1179e-04,  4.2561e-03,  ..., -2.6100e-03,\n",
      "         -8.2722e-05, -4.4539e-03],\n",
      "        [-3.6769e-03, -2.2896e-03,  3.6415e-03,  ...,  2.3380e-03,\n",
      "          2.1911e-03,  1.1797e-03],\n",
      "        [-1.4695e-03,  2.5684e-03,  3.1798e-03,  ..., -2.3510e-03,\n",
      "         -4.3102e-03, -4.1764e-03],\n",
      "        ...,\n",
      "        [ 1.1560e-03,  6.1223e-04, -2.2659e-03,  ..., -7.7543e-04,\n",
      "          1.5983e-03,  4.3352e-03],\n",
      "        [-3.1582e-03,  1.1323e-03, -2.0026e-03,  ..., -3.6624e-03,\n",
      "          1.4867e-03, -2.1280e-03],\n",
      "        [ 2.2854e-04,  1.0052e-04,  3.2269e-03,  ..., -7.4711e-04,\n",
      "          1.9041e-03, -2.1203e-03]], device='cuda:0', requires_grad=True)\n",
      "\n",
      "after weight: Parameter containing:\n",
      "tensor([[-2.6028e-03, -7.1247e-04,  4.2554e-03,  ..., -2.6107e-03,\n",
      "         -8.3398e-05, -4.4546e-03],\n",
      "        [-3.6769e-03, -2.2896e-03,  3.6416e-03,  ...,  2.3380e-03,\n",
      "          2.1911e-03,  1.1797e-03],\n",
      "        [-1.4703e-03,  2.5675e-03,  3.1790e-03,  ..., -2.3518e-03,\n",
      "         -4.3110e-03, -4.1773e-03],\n",
      "        ...,\n",
      "        [ 1.1570e-03,  6.1318e-04, -2.2649e-03,  ..., -7.7448e-04,\n",
      "          1.5993e-03,  4.3362e-03],\n",
      "        [-3.1566e-03,  1.1339e-03, -2.0010e-03,  ..., -3.6609e-03,\n",
      "          1.4883e-03, -2.1265e-03],\n",
      "        [ 2.2910e-04,  1.0107e-04,  3.2275e-03,  ..., -7.4656e-04,\n",
      "          1.9047e-03, -2.1198e-03]], device='cuda:0', requires_grad=True)\n",
      "\n",
      "step memory allocate: 30825.714M\n",
      "loss: 41.49366760253906\n",
      "before weight: Parameter containing:\n",
      "tensor([[-2.6028e-03, -7.1247e-04,  4.2554e-03,  ..., -2.6107e-03,\n",
      "         -8.3398e-05, -4.4546e-03],\n",
      "        [-3.6769e-03, -2.2896e-03,  3.6416e-03,  ...,  2.3380e-03,\n",
      "          2.1911e-03,  1.1797e-03],\n",
      "        [-1.4703e-03,  2.5675e-03,  3.1790e-03,  ..., -2.3518e-03,\n",
      "         -4.3110e-03, -4.1773e-03],\n",
      "        ...,\n",
      "        [ 1.1570e-03,  6.1318e-04, -2.2649e-03,  ..., -7.7448e-04,\n",
      "          1.5993e-03,  4.3362e-03],\n",
      "        [-3.1566e-03,  1.1339e-03, -2.0010e-03,  ..., -3.6609e-03,\n",
      "          1.4883e-03, -2.1265e-03],\n",
      "        [ 2.2910e-04,  1.0107e-04,  3.2275e-03,  ..., -7.4656e-04,\n",
      "          1.9047e-03, -2.1198e-03]], device='cuda:0', requires_grad=True)\n",
      "\n",
      "after weight: Parameter containing:\n",
      "tensor([[-2.6024e-03, -7.1214e-04,  4.2557e-03,  ..., -2.6104e-03,\n",
      "         -8.3071e-05, -4.4543e-03],\n",
      "        [-3.6767e-03, -2.2895e-03,  3.6417e-03,  ...,  2.3381e-03,\n",
      "          2.1912e-03,  1.1799e-03],\n",
      "        [-1.4698e-03,  2.5681e-03,  3.1796e-03,  ..., -2.3513e-03,\n",
      "         -4.3105e-03, -4.1767e-03],\n",
      "        ...,\n",
      "        [ 1.1560e-03,  6.1220e-04, -2.2659e-03,  ..., -7.7547e-04,\n",
      "          1.5983e-03,  4.3352e-03],\n",
      "        [-3.1578e-03,  1.1327e-03, -2.0022e-03,  ..., -3.6621e-03,\n",
      "          1.4871e-03, -2.1277e-03],\n",
      "        [ 2.2862e-04,  1.0060e-04,  3.2270e-03,  ..., -7.4703e-04,\n",
      "          1.9042e-03, -2.1203e-03]], device='cuda:0', requires_grad=True)\n",
      "\n",
      "step memory allocate: 30825.714M\n",
      "loss: 11.338289260864258\n",
      "before weight: Parameter containing:\n",
      "tensor([[-2.6024e-03, -7.1214e-04,  4.2557e-03,  ..., -2.6104e-03,\n",
      "         -8.3071e-05, -4.4543e-03],\n",
      "        [-3.6767e-03, -2.2895e-03,  3.6417e-03,  ...,  2.3381e-03,\n",
      "          2.1912e-03,  1.1799e-03],\n",
      "        [-1.4698e-03,  2.5681e-03,  3.1796e-03,  ..., -2.3513e-03,\n",
      "         -4.3105e-03, -4.1767e-03],\n",
      "        ...,\n",
      "        [ 1.1560e-03,  6.1220e-04, -2.2659e-03,  ..., -7.7547e-04,\n",
      "          1.5983e-03,  4.3352e-03],\n",
      "        [-3.1578e-03,  1.1327e-03, -2.0022e-03,  ..., -3.6621e-03,\n",
      "          1.4871e-03, -2.1277e-03],\n",
      "        [ 2.2862e-04,  1.0060e-04,  3.2270e-03,  ..., -7.4703e-04,\n",
      "          1.9042e-03, -2.1203e-03]], device='cuda:0', requires_grad=True)\n",
      "\n",
      "after weight: Parameter containing:\n",
      "tensor([[-2.6027e-03, -7.1239e-04,  4.2555e-03,  ..., -2.6106e-03,\n",
      "         -8.3320e-05, -4.4545e-03],\n",
      "        [-3.6766e-03, -2.2893e-03,  3.6418e-03,  ...,  2.3383e-03,\n",
      "          2.1913e-03,  1.1800e-03],\n",
      "        [-1.4700e-03,  2.5679e-03,  3.1793e-03,  ..., -2.3515e-03,\n",
      "         -4.3107e-03, -4.1769e-03],\n",
      "        ...,\n",
      "        [ 1.1560e-03,  6.1220e-04, -2.2659e-03,  ..., -7.7547e-04,\n",
      "          1.5983e-03,  4.3352e-03],\n",
      "        [-3.1575e-03,  1.1329e-03, -2.0019e-03,  ..., -3.6618e-03,\n",
      "          1.4874e-03, -2.1274e-03],\n",
      "        [ 2.2869e-04,  1.0066e-04,  3.2271e-03,  ..., -7.4697e-04,\n",
      "          1.9043e-03, -2.1202e-03]], device='cuda:0', requires_grad=True)\n",
      "\n",
      "step memory allocate: 30825.714M\n",
      "loss: 1.6681572198867798\n",
      "before weight: Parameter containing:\n",
      "tensor([[-2.6027e-03, -7.1239e-04,  4.2555e-03,  ..., -2.6106e-03,\n",
      "         -8.3320e-05, -4.4545e-03],\n",
      "        [-3.6766e-03, -2.2893e-03,  3.6418e-03,  ...,  2.3383e-03,\n",
      "          2.1913e-03,  1.1800e-03],\n",
      "        [-1.4700e-03,  2.5679e-03,  3.1793e-03,  ..., -2.3515e-03,\n",
      "         -4.3107e-03, -4.1769e-03],\n",
      "        ...,\n",
      "        [ 1.1560e-03,  6.1220e-04, -2.2659e-03,  ..., -7.7547e-04,\n",
      "          1.5983e-03,  4.3352e-03],\n",
      "        [-3.1575e-03,  1.1329e-03, -2.0019e-03,  ..., -3.6618e-03,\n",
      "          1.4874e-03, -2.1274e-03],\n",
      "        [ 2.2869e-04,  1.0066e-04,  3.2271e-03,  ..., -7.4697e-04,\n",
      "          1.9043e-03, -2.1202e-03]], device='cuda:0', requires_grad=True)\n",
      "\n",
      "after weight: Parameter containing:\n",
      "tensor([[-2.6031e-03, -7.1281e-04,  4.2550e-03,  ..., -2.6110e-03,\n",
      "         -8.3746e-05, -4.4550e-03],\n",
      "        [-3.6765e-03, -2.2892e-03,  3.6419e-03,  ...,  2.3384e-03,\n",
      "          2.1914e-03,  1.1801e-03],\n",
      "        [-1.4705e-03,  2.5674e-03,  3.1789e-03,  ..., -2.3519e-03,\n",
      "         -4.3111e-03, -4.1774e-03],\n",
      "        ...,\n",
      "        [ 1.1563e-03,  6.1254e-04, -2.2656e-03,  ..., -7.7513e-04,\n",
      "          1.5986e-03,  4.3355e-03],\n",
      "        [-3.1567e-03,  1.1337e-03, -2.0011e-03,  ..., -3.6610e-03,\n",
      "          1.4881e-03, -2.1266e-03],\n",
      "        [ 2.2893e-04,  1.0091e-04,  3.2273e-03,  ..., -7.4673e-04,\n",
      "          1.9045e-03, -2.1200e-03]], device='cuda:0', requires_grad=True)\n",
      "\n",
      "step memory allocate: 30825.714M\n",
      "CPU times: user 11 s, sys: 655 ms, total: 11.7 s\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda import max_memory_allocated\n",
    "\n",
    "# example input sizes\n",
    "batch_size, hidden_size = 4, 50000\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_size, 1, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = self.w1(x)\n",
    "        z2 = self.w2(z1)\n",
    "        return z2\n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "torch.cuda.init()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "fp32_model= Net().to(\"cuda\")\n",
    "#optimizer = SGD(fp32_model.parameters(), lr=1e-2)\n",
    "#optimizer = SGD(fp32_model.parameters(), lr=2e-4, momentum=0.9)\n",
    "optimizer = SGD(fp32_model.parameters(), lr=5e-5, momentum=0.9)\n",
    "#optimizer = Adam(fp32_model.parameters(), lr=1e-2)\n",
    "print(f\"GPU = {torch.cuda.max_memory_allocated(0) / 1e6} MiB\")\n",
    "\n",
    "\n",
    "# create dummy data (bsz=4, hid=256)\n",
    "#x = torch.randn(batch_size, hidden_size, device=\"cuda\") \n",
    "x = torch.ones(batch_size, hidden_size, device=\"cuda\") \n",
    "y = torch.tensor([[3.0], [3.0], [3.0], [3.0]], device=\"cuda\")\n",
    "\n",
    "torch.cuda.init()\n",
    "for _ in range(5):\n",
    "    #x = torch.ones(batch_size, hidden_size, device=\"cuda\")\n",
    "    optimizer.zero_grad()\n",
    "    z2 = fp32_model(x)\n",
    "    loss = torch.nn.functional.mse_loss(z2, y)\n",
    "    #loss = z2.sum()\n",
    "    print(f\"loss: {loss}\")\n",
    "    loss.backward()\n",
    "    print(f'before weight: {fp32_model.w1.weight}\\n')\n",
    "    #print(f'before grad: {fp32_model.w1.weight.grad}\\n')\n",
    "    #print(torch.max(fp32_model.w1.weight))\n",
    "    optimizer.step()\n",
    "    print(f'after weight: {fp32_model.w1.weight}\\n')\n",
    "    #print(torch.max(fp32_model.w1.weight))\n",
    "    memory = max_memory_allocated()\n",
    "    print(f'step memory allocate: {memory / 1e6:.3f}M')\n",
    "    #torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d176cc6-3819-4a67-85a4-a050dfd705c9",
   "metadata": {},
   "source": [
    "### float16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e14fbbdb-0d39-49cd-9662-7d82377f4d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = 35825.01376 MiB\n",
      "loss: 7.703125\n",
      "before weight: Parameter containing:\n",
      "tensor([[-2.6031e-03, -7.1281e-04,  4.2550e-03,  ..., -2.6110e-03,\n",
      "         -8.3746e-05, -4.4550e-03],\n",
      "        [-3.6765e-03, -2.2892e-03,  3.6419e-03,  ...,  2.3384e-03,\n",
      "          2.1914e-03,  1.1801e-03],\n",
      "        [-1.4705e-03,  2.5674e-03,  3.1789e-03,  ..., -2.3519e-03,\n",
      "         -4.3111e-03, -4.1774e-03],\n",
      "        ...,\n",
      "        [ 1.1563e-03,  6.1254e-04, -2.2656e-03,  ..., -7.7513e-04,\n",
      "          1.5986e-03,  4.3355e-03],\n",
      "        [-3.1567e-03,  1.1337e-03, -2.0011e-03,  ..., -3.6610e-03,\n",
      "          1.4881e-03, -2.1266e-03],\n",
      "        [ 2.2893e-04,  1.0091e-04,  3.2273e-03,  ..., -7.4673e-04,\n",
      "          1.9045e-03, -2.1200e-03]], device='cuda:0', requires_grad=True)\n",
      "\n",
      "after weight: Parameter containing:\n",
      "tensor([[-2.6031e-03, -7.1281e-04,  4.2550e-03,  ..., -2.6110e-03,\n",
      "         -8.3746e-05, -4.4550e-03],\n",
      "        [-3.6765e-03, -2.2892e-03,  3.6419e-03,  ...,  2.3384e-03,\n",
      "          2.1914e-03,  1.1801e-03],\n",
      "        [-1.4705e-03,  2.5674e-03,  3.1789e-03,  ..., -2.3519e-03,\n",
      "         -4.3111e-03, -4.1774e-03],\n",
      "        ...,\n",
      "        [ 1.1563e-03,  6.1254e-04, -2.2656e-03,  ..., -7.7513e-04,\n",
      "          1.5986e-03,  4.3355e-03],\n",
      "        [-3.1567e-03,  1.1337e-03, -2.0011e-03,  ..., -3.6610e-03,\n",
      "          1.4881e-03, -2.1266e-03],\n",
      "        [ 2.2893e-04,  1.0091e-04,  3.2273e-03,  ..., -7.4673e-04,\n",
      "          1.9045e-03, -2.1200e-03]], device='cuda:0', requires_grad=True)\n",
      "\n",
      "step memory allocate: 35825.014M\n",
      "loss: 2.95703125\n",
      "before weight: Parameter containing:\n",
      "tensor([[-2.6031e-03, -7.1281e-04,  4.2550e-03,  ..., -2.6110e-03,\n",
      "         -8.3746e-05, -4.4550e-03],\n",
      "        [-3.6765e-03, -2.2892e-03,  3.6419e-03,  ...,  2.3384e-03,\n",
      "          2.1914e-03,  1.1801e-03],\n",
      "        [-1.4705e-03,  2.5674e-03,  3.1789e-03,  ..., -2.3519e-03,\n",
      "         -4.3111e-03, -4.1774e-03],\n",
      "        ...,\n",
      "        [ 1.1563e-03,  6.1254e-04, -2.2656e-03,  ..., -7.7513e-04,\n",
      "          1.5986e-03,  4.3355e-03],\n",
      "        [-3.1567e-03,  1.1337e-03, -2.0011e-03,  ..., -3.6610e-03,\n",
      "          1.4881e-03, -2.1266e-03],\n",
      "        [ 2.2893e-04,  1.0091e-04,  3.2273e-03,  ..., -7.4673e-04,\n",
      "          1.9045e-03, -2.1200e-03]], device='cuda:0', requires_grad=True)\n",
      "\n",
      "after weight: Parameter containing:\n",
      "tensor([[-2.6031e-03, -7.1281e-04,  4.2550e-03,  ..., -2.6110e-03,\n",
      "         -8.3746e-05, -4.4550e-03],\n",
      "        [-3.6765e-03, -2.2892e-03,  3.6419e-03,  ...,  2.3384e-03,\n",
      "          2.1914e-03,  1.1801e-03],\n",
      "        [-1.4705e-03,  2.5674e-03,  3.1789e-03,  ..., -2.3519e-03,\n",
      "         -4.3111e-03, -4.1774e-03],\n",
      "        ...,\n",
      "        [ 1.1563e-03,  6.1254e-04, -2.2656e-03,  ..., -7.7513e-04,\n",
      "          1.5986e-03,  4.3355e-03],\n",
      "        [-3.1567e-03,  1.1337e-03, -2.0011e-03,  ..., -3.6610e-03,\n",
      "          1.4881e-03, -2.1266e-03],\n",
      "        [ 2.2893e-04,  1.0091e-04,  3.2273e-03,  ..., -7.4673e-04,\n",
      "          1.9045e-03, -2.1200e-03]], device='cuda:0', requires_grad=True)\n",
      "\n",
      "step memory allocate: 35825.015M\n",
      "loss: 0.0001373291015625\n",
      "before weight: Parameter containing:\n",
      "tensor([[-2.6031e-03, -7.1281e-04,  4.2550e-03,  ..., -2.6110e-03,\n",
      "         -8.3746e-05, -4.4550e-03],\n",
      "        [-3.6765e-03, -2.2892e-03,  3.6419e-03,  ...,  2.3384e-03,\n",
      "          2.1914e-03,  1.1801e-03],\n",
      "        [-1.4705e-03,  2.5674e-03,  3.1789e-03,  ..., -2.3519e-03,\n",
      "         -4.3111e-03, -4.1774e-03],\n",
      "        ...,\n",
      "        [ 1.1563e-03,  6.1254e-04, -2.2656e-03,  ..., -7.7513e-04,\n",
      "          1.5986e-03,  4.3355e-03],\n",
      "        [-3.1567e-03,  1.1337e-03, -2.0011e-03,  ..., -3.6610e-03,\n",
      "          1.4881e-03, -2.1266e-03],\n",
      "        [ 2.2893e-04,  1.0091e-04,  3.2273e-03,  ..., -7.4673e-04,\n",
      "          1.9045e-03, -2.1200e-03]], device='cuda:0', requires_grad=True)\n",
      "\n",
      "after weight: Parameter containing:\n",
      "tensor([[-2.6031e-03, -7.1281e-04,  4.2550e-03,  ..., -2.6110e-03,\n",
      "         -8.3746e-05, -4.4550e-03],\n",
      "        [-3.6765e-03, -2.2892e-03,  3.6419e-03,  ...,  2.3384e-03,\n",
      "          2.1914e-03,  1.1801e-03],\n",
      "        [-1.4705e-03,  2.5674e-03,  3.1789e-03,  ..., -2.3519e-03,\n",
      "         -4.3111e-03, -4.1774e-03],\n",
      "        ...,\n",
      "        [ 1.1563e-03,  6.1254e-04, -2.2656e-03,  ..., -7.7513e-04,\n",
      "          1.5986e-03,  4.3355e-03],\n",
      "        [-3.1567e-03,  1.1337e-03, -2.0011e-03,  ..., -3.6610e-03,\n",
      "          1.4881e-03, -2.1266e-03],\n",
      "        [ 2.2893e-04,  1.0091e-04,  3.2273e-03,  ..., -7.4673e-04,\n",
      "          1.9045e-03, -2.1200e-03]], device='cuda:0', requires_grad=True)\n",
      "\n",
      "step memory allocate: 35825.015M\n",
      "loss: 2.41796875\n",
      "before weight: Parameter containing:\n",
      "tensor([[-2.6031e-03, -7.1281e-04,  4.2550e-03,  ..., -2.6110e-03,\n",
      "         -8.3746e-05, -4.4550e-03],\n",
      "        [-3.6765e-03, -2.2892e-03,  3.6419e-03,  ...,  2.3384e-03,\n",
      "          2.1914e-03,  1.1801e-03],\n",
      "        [-1.4705e-03,  2.5674e-03,  3.1789e-03,  ..., -2.3519e-03,\n",
      "         -4.3111e-03, -4.1774e-03],\n",
      "        ...,\n",
      "        [ 1.1563e-03,  6.1254e-04, -2.2656e-03,  ..., -7.7513e-04,\n",
      "          1.5986e-03,  4.3355e-03],\n",
      "        [-3.1567e-03,  1.1337e-03, -2.0011e-03,  ..., -3.6610e-03,\n",
      "          1.4881e-03, -2.1266e-03],\n",
      "        [ 2.2893e-04,  1.0091e-04,  3.2273e-03,  ..., -7.4673e-04,\n",
      "          1.9045e-03, -2.1200e-03]], device='cuda:0', requires_grad=True)\n",
      "\n",
      "after weight: Parameter containing:\n",
      "tensor([[-2.6031e-03, -7.1281e-04,  4.2550e-03,  ..., -2.6110e-03,\n",
      "         -8.3746e-05, -4.4550e-03],\n",
      "        [-3.6765e-03, -2.2892e-03,  3.6419e-03,  ...,  2.3384e-03,\n",
      "          2.1914e-03,  1.1801e-03],\n",
      "        [-1.4705e-03,  2.5674e-03,  3.1789e-03,  ..., -2.3519e-03,\n",
      "         -4.3111e-03, -4.1774e-03],\n",
      "        ...,\n",
      "        [ 1.1563e-03,  6.1254e-04, -2.2656e-03,  ..., -7.7513e-04,\n",
      "          1.5986e-03,  4.3355e-03],\n",
      "        [-3.1567e-03,  1.1337e-03, -2.0011e-03,  ..., -3.6610e-03,\n",
      "          1.4881e-03, -2.1266e-03],\n",
      "        [ 2.2893e-04,  1.0091e-04,  3.2273e-03,  ..., -7.4673e-04,\n",
      "          1.9045e-03, -2.1200e-03]], device='cuda:0', requires_grad=True)\n",
      "\n",
      "step memory allocate: 35825.015M\n",
      "loss: 5.02734375\n",
      "before weight: Parameter containing:\n",
      "tensor([[-2.6031e-03, -7.1281e-04,  4.2550e-03,  ..., -2.6110e-03,\n",
      "         -8.3746e-05, -4.4550e-03],\n",
      "        [-3.6765e-03, -2.2892e-03,  3.6419e-03,  ...,  2.3384e-03,\n",
      "          2.1914e-03,  1.1801e-03],\n",
      "        [-1.4705e-03,  2.5674e-03,  3.1789e-03,  ..., -2.3519e-03,\n",
      "         -4.3111e-03, -4.1774e-03],\n",
      "        ...,\n",
      "        [ 1.1563e-03,  6.1254e-04, -2.2656e-03,  ..., -7.7513e-04,\n",
      "          1.5986e-03,  4.3355e-03],\n",
      "        [-3.1567e-03,  1.1337e-03, -2.0011e-03,  ..., -3.6610e-03,\n",
      "          1.4881e-03, -2.1266e-03],\n",
      "        [ 2.2893e-04,  1.0091e-04,  3.2273e-03,  ..., -7.4673e-04,\n",
      "          1.9045e-03, -2.1200e-03]], device='cuda:0', requires_grad=True)\n",
      "\n",
      "after weight: Parameter containing:\n",
      "tensor([[-2.6031e-03, -7.1281e-04,  4.2550e-03,  ..., -2.6110e-03,\n",
      "         -8.3746e-05, -4.4550e-03],\n",
      "        [-3.6765e-03, -2.2892e-03,  3.6419e-03,  ...,  2.3384e-03,\n",
      "          2.1914e-03,  1.1801e-03],\n",
      "        [-1.4705e-03,  2.5674e-03,  3.1789e-03,  ..., -2.3519e-03,\n",
      "         -4.3111e-03, -4.1774e-03],\n",
      "        ...,\n",
      "        [ 1.1563e-03,  6.1254e-04, -2.2656e-03,  ..., -7.7513e-04,\n",
      "          1.5986e-03,  4.3355e-03],\n",
      "        [-3.1567e-03,  1.1337e-03, -2.0011e-03,  ..., -3.6610e-03,\n",
      "          1.4881e-03, -2.1266e-03],\n",
      "        [ 2.2893e-04,  1.0091e-04,  3.2273e-03,  ..., -7.4673e-04,\n",
      "          1.9045e-03, -2.1200e-03]], device='cuda:0', requires_grad=True)\n",
      "\n",
      "step memory allocate: 35825.015M\n",
      "CPU times: user 13.2 s, sys: 1.92 s, total: 15.1 s\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda import max_memory_allocated\n",
    "\n",
    "# example input sizes\n",
    "batch_size, hidden_size = 4, 50000\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_size, 1, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = self.w1(x)\n",
    "        z2 = self.w2(z1)\n",
    "        return z2\n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "torch.cuda.init()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "fp16_model= Net().half().to(\"cuda\")\n",
    "#optimizer = SGD(fp16_model.parameters(), lr=1e-2)\n",
    "optimizer = SGD(fp16_model.parameters(), lr=1e-5, momentum=0.9)\n",
    "#optimizer = Adam(fp16_model.parameters(), lr=1e-2)\n",
    "print(f\"GPU = {torch.cuda.max_memory_allocated(0) / 1e6} MiB\")\n",
    "\n",
    "\n",
    "# create dummy data (bsz=4, hid=256)\n",
    "#x = torch.randn(batch_size, hidden_size, device=\"cuda\") \n",
    "x = torch.ones(batch_size, hidden_size,dtype=torch.half, device=\"cuda\") \n",
    "y = torch.tensor([[3.0], [3.0], [3.0], [3.0]], dtype=torch.half, device=\"cuda\")\n",
    "\n",
    "torch.cuda.init()\n",
    "for _ in range(5):\n",
    "    #x = torch.ones(batch_size, hidden_size, device=\"cuda\")\n",
    "    optimizer.zero_grad()\n",
    "    z2 = fp16_model(x)\n",
    "    loss = torch.nn.functional.mse_loss(z2, y)\n",
    "    #loss = z2.sum()\n",
    "    print(f\"loss: {loss}\")\n",
    "    loss.backward()\n",
    "    print(f'before weight: {fp32_model.w1.weight}\\n')\n",
    "    #print(f'before grad: {fp32_model.w1.weight.grad}\\n')\n",
    "    #print(torch.max(fp32_model.w1.weight))\n",
    "    optimizer.step()\n",
    "    print(f'after weight: {fp32_model.w1.weight}\\n')\n",
    "    #print(torch.max(fp32_model.w1.weight))\n",
    "    memory = max_memory_allocated()\n",
    "    print(f'step memory allocate: {memory / 1e6:.3f}M')\n",
    "    #torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333b62e7-159f-4861-b1ba-30fd24496f5d",
   "metadata": {},
   "source": [
    "### Bfloat16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffdaf0be-08dd-4f3c-a242-68b86b928869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = 40824.714752 MiB\n",
      "loss: 10.5625\n",
      "step memory allocate: 45824.815M\n",
      "loss: 0.15234375\n",
      "step memory allocate: 45825.216M\n",
      "loss: 10.375\n",
      "step memory allocate: 45825.216M\n",
      "loss: 4.65625\n",
      "step memory allocate: 45825.216M\n",
      "loss: 1.4296875\n",
      "step memory allocate: 45825.216M\n",
      "loss: 8.25\n",
      "step memory allocate: 45825.216M\n",
      "loss: 1.390625\n",
      "step memory allocate: 45825.216M\n",
      "loss: 2.75\n",
      "step memory allocate: 45825.216M\n",
      "loss: 5.625\n",
      "step memory allocate: 45825.216M\n",
      "loss: 0.140625\n",
      "step memory allocate: 45825.216M\n",
      "loss: 3.453125\n",
      "step memory allocate: 45825.216M\n",
      "loss: 3.421875\n",
      "step memory allocate: 45825.216M\n",
      "loss: 0.0791015625\n",
      "step memory allocate: 45825.216M\n",
      "loss: 3.515625\n",
      "step memory allocate: 45825.216M\n",
      "loss: 1.5625\n",
      "step memory allocate: 45825.216M\n",
      "loss: 0.5625\n",
      "step memory allocate: 45825.216M\n",
      "loss: 2.875\n",
      "step memory allocate: 45825.216M\n",
      "loss: 0.4296875\n",
      "step memory allocate: 45825.216M\n",
      "loss: 1.0\n",
      "step memory allocate: 45825.216M\n",
      "loss: 1.890625\n",
      "step memory allocate: 45825.216M\n",
      "CPU times: user 14 s, sys: 543 ms, total: 14.6 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda import max_memory_allocated\n",
    "\n",
    "# example input sizes\n",
    "batch_size, hidden_size = 4, 50000\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(hidden_size, hidden_size, bias=False, dtype=torch.bfloat16)\n",
    "        self.w2 = nn.Linear(hidden_size, 1, bias=False, dtype=torch.bfloat16)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = self.w1(x)\n",
    "        z2 = self.w2(z1)\n",
    "        return z2\n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "torch.cuda.init()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bp16_model= Net().to(\"cuda\")\n",
    "#optimizer = SGD(bp16_model.parameters(), lr=1e-2)\n",
    "optimizer = SGD(bp16_model.parameters(), lr=1e-5, momentum=0.9)\n",
    "#optimizer = Adam(bp16_model.parameters(), lr=1e-2)\n",
    "print(f\"GPU = {torch.cuda.max_memory_allocated(0) / 1e6} MiB\")\n",
    "\n",
    "\n",
    "# create dummy data (bsz=4, hid=256)\n",
    "#x = torch.randn(batch_size, hidden_size, device=\"cuda\") \n",
    "x = torch.ones(batch_size, hidden_size,dtype=torch.bfloat16, device=\"cuda\") \n",
    "y = torch.tensor([[3.0], [3.0], [3.0], [3.0]], dtype=torch.bfloat16, device=\"cuda\")\n",
    "\n",
    "torch.cuda.init()\n",
    "for _ in range(20):\n",
    "    #x = torch.ones(batch_size, hidden_size, device=\"cuda\")\n",
    "    optimizer.zero_grad()\n",
    "    z2 = bp16_model(x)\n",
    "    loss = torch.nn.functional.mse_loss(z2, y)\n",
    "    #loss = z2.sum()\n",
    "    print(f\"loss: {loss}\")\n",
    "    loss.backward()\n",
    "    #print(f'before weight: {fp32_model.w1.weight}\\n')\n",
    "    #print(f'before grad: {fp32_model.w1.weight.grad}\\n')\n",
    "    #print(torch.max(fp32_model.w1.weight))\n",
    "    optimizer.step()\n",
    "    #print(f'after weight: {fp32_model.w1.weight}\\n')\n",
    "    #print(torch.max(fp32_model.w1.weight))\n",
    "    memory = max_memory_allocated()\n",
    "    print(f'step memory allocate: {memory / 1e6:.3f}M')\n",
    "    #torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c062b9f0-ea2e-4ef8-8397-80d7cb36a3cb",
   "metadata": {},
   "source": [
    "### Mixed Precision: Autocast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8718c5d5-92c8-41df-8c81-c48c962f6fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = 55825.015296 MiB\n",
      "loss: 9.86275863647461\n",
      "step memory allocate: 55825.415M\n",
      "loss: 1.0837135314941406\n",
      "step memory allocate: 60825.416M\n",
      "loss: 2.3807525634765625\n",
      "step memory allocate: 60825.416M\n",
      "loss: 8.064712524414062\n",
      "step memory allocate: 60825.416M\n",
      "loss: 4.41656494140625\n",
      "step memory allocate: 60825.416M\n",
      "loss: 0.00152587890625\n",
      "step memory allocate: 60825.416M\n",
      "loss: 3.3886194229125977\n",
      "step memory allocate: 60825.416M\n",
      "loss: 5.322843551635742\n",
      "step memory allocate: 60825.416M\n",
      "loss: 1.42877197265625\n",
      "step memory allocate: 60825.416M\n",
      "loss: 0.3571929931640625\n",
      "step memory allocate: 60825.416M\n",
      "loss: 3.28515625\n",
      "step memory allocate: 60825.416M\n",
      "loss: 2.9139556884765625\n",
      "step memory allocate: 60825.416M\n",
      "loss: 0.2308502197265625\n",
      "step memory allocate: 60825.416M\n",
      "loss: 0.8825721740722656\n",
      "step memory allocate: 60825.416M\n",
      "loss: 2.5431528091430664\n",
      "step memory allocate: 60825.416M\n",
      "loss: 1.2876930236816406\n",
      "step memory allocate: 60825.416M\n",
      "loss: 0.00054931640625\n",
      "step memory allocate: 60825.416M\n",
      "loss: 1.1041412353515625\n",
      "step memory allocate: 60825.416M\n",
      "loss: 1.6516265869140625\n",
      "step memory allocate: 60825.416M\n",
      "loss: 0.4255523681640625\n",
      "step memory allocate: 60825.416M\n",
      "CPU times: user 11.6 s, sys: 1.29 s, total: 12.9 s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda import max_memory_allocated\n",
    "\n",
    "from torch.cuda.amp import autocast \n",
    "\n",
    "# example input sizes\n",
    "batch_size, hidden_size = 4, 50000\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_size, 1, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = self.w1(x)\n",
    "        z2 = self.w2(z1)\n",
    "        return z2\n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "torch.cuda.init()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "fp32_model= Net().to(\"cuda\")\n",
    "#optimizer = SGD(fp32_model.parameters(), lr=1e-2)\n",
    "#optimizer = SGD(fp32_model.parameters(), lr=1e-4, momentum=0.9) # diverge\n",
    "optimizer = SGD(fp32_model.parameters(), lr=1e-5, momentum=0.9)\n",
    "#optimizer = Adam(fp32_model.parameters(), lr=1e-2)\n",
    "print(f\"GPU = {torch.cuda.max_memory_allocated(0) / 1e6} MiB\")\n",
    "\n",
    "# create dummy data (bsz=4, hid=256)\n",
    "#x = torch.randn(batch_size, hidden_size, device=\"cuda\") \n",
    "x = torch.ones(batch_size, hidden_size, device=\"cuda\") \n",
    "y = torch.tensor([[3.0], [3.0], [3.0], [3.0]], device=\"cuda\")\n",
    "\n",
    "torch.cuda.init()\n",
    "for _ in range(20):\n",
    "  with autocast():\n",
    "  #with autocast(cache_enabled=False):\n",
    "    #x = torch.ones(batch_size, hidden_size, device=\"cuda\")\n",
    "    optimizer.zero_grad()\n",
    "    z2 = fp32_model(x)\n",
    "    loss = torch.nn.functional.mse_loss(z2, y)\n",
    "    #loss = z2.sum()\n",
    "    print(f\"loss: {loss}\")\n",
    "    loss.backward()\n",
    "    #print(f'before weight: {fp32_model.w1.weight}\\n')\n",
    "    #print(f'before grad: {fp32_model.w1.weight.grad}\\n')\n",
    "    #print(torch.max(fp32_model.w1.weight))\n",
    "    optimizer.step()\n",
    "    #print(f'after weight: {fp32_model.w1.weight}\\n')\n",
    "    #print(torch.max(fp32_model.w1.weight))\n",
    "    memory = max_memory_allocated()\n",
    "    print(f'step memory allocate: {memory / 1e6:.3f}M')\n",
    "    #torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e1efcf-b5ef-4406-a7c1-2481ca193eb2",
   "metadata": {},
   "source": [
    "### Autocast + torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eb1bb185-c44a-4698-bc4a-ae1b001272d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = 60825.515008 MiB\n",
      "loss: 14.198324203491211\n",
      "step memory allocate: 60825.515M\n",
      "loss: 14.198324203491211\n",
      "step memory allocate: 60825.515M\n",
      "loss: 14.198324203491211\n",
      "step memory allocate: 60825.515M\n",
      "loss: 14.198324203491211\n",
      "step memory allocate: 60825.515M\n",
      "loss: 14.198324203491211\n",
      "step memory allocate: 60825.515M\n",
      "loss: 14.198324203491211\n",
      "step memory allocate: 60825.515M\n",
      "loss: 1.5478858947753906\n",
      "step memory allocate: 60825.515M\n",
      "loss: 3.457275390625\n",
      "step memory allocate: 60825.515M\n",
      "loss: 11.6025390625\n",
      "step memory allocate: 60825.515M\n",
      "loss: 6.328369140625\n",
      "step memory allocate: 60825.515M\n",
      "loss: 0.001102447509765625\n",
      "step memory allocate: 60825.515M\n",
      "loss: 4.914188385009766\n",
      "step memory allocate: 60825.515M\n",
      "loss: 7.647331237792969\n",
      "step memory allocate: 60825.515M\n",
      "loss: 2.0272865295410156\n",
      "step memory allocate: 60825.515M\n",
      "loss: 0.5250587463378906\n",
      "step memory allocate: 60825.515M\n",
      "loss: 4.7340240478515625\n",
      "step memory allocate: 60825.515M\n",
      "loss: 4.1737213134765625\n",
      "step memory allocate: 60825.515M\n",
      "loss: 0.3319740295410156\n",
      "step memory allocate: 60825.515M\n",
      "loss: 1.2590417861938477\n",
      "step memory allocate: 60825.515M\n",
      "loss: 3.6486968994140625\n",
      "step memory allocate: 60825.515M\n",
      "CPU times: user 11.7 s, sys: 1.35 s, total: 13 s\n",
      "Wall time: 13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda import max_memory_allocated\n",
    "\n",
    "from torch.cuda.amp import autocast \n",
    "\n",
    "# example input sizes\n",
    "batch_size, hidden_size = 4, 50000\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_size, 1, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = self.w1(x)\n",
    "        z2 = self.w2(z1)\n",
    "        return z2\n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "torch.cuda.init()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "fp32_model= Net().to(\"cuda\")\n",
    "#optimizer = SGD(fp32_model.parameters(), lr=1e-2)\n",
    "#optimizer = SGD(fp32_model.parameters(), lr=1e-4, momentum=0.9) #diverge\n",
    "optimizer = SGD(fp32_model.parameters(), lr=1e-5, momentum=0.9)\n",
    "#optimizer = Adam(fp32_model.parameters(), lr=1e-2)\n",
    "print(f\"GPU = {torch.cuda.max_memory_allocated(0) / 1e6} MiB\")\n",
    "\n",
    "# create dummy data (bsz=4, hid=256)\n",
    "#x = torch.randn(batch_size, hidden_size, device=\"cuda\") \n",
    "x = torch.ones(batch_size, hidden_size, device=\"cuda\") \n",
    "y = torch.tensor([[3.0], [3.0], [3.0], [3.0]], device=\"cuda\")\n",
    "\n",
    "torch.cuda.init()\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for _ in range(20):\n",
    "    optimizer.zero_grad()\n",
    "    with autocast():\n",
    "        z2 = fp32_model(x)\n",
    "        loss = torch.nn.functional.mse_loss(z2, y)\n",
    "        \n",
    "    print(f\"loss: {loss}\")\n",
    "    scaler.scale(loss).backward()\n",
    "    #print(f'before weight: {fp32_model.w1.weight}\\n')\n",
    "    #print(f'before grad: {fp32_model.w1.weight.grad}\\n')\n",
    "    #print(torch.max(fp32_model.w1.weight))\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    #print(f'after weight: {fp32_model.w1.weight}\\n')\n",
    "    #print(torch.max(fp32_model.w1.weight))\n",
    "    memory = max_memory_allocated()\n",
    "    print(f'step memory allocate: {memory / 1e6:.3f}M')\n",
    "    #torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f533fa92-5782-4801-b308-273eec433185",
   "metadata": {},
   "source": [
    "### NVIDIA AMP (Apex Mixed Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "59db040f-49cb-4dc0-bbd6-70b39a4d8ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = 60825.516032 MiB\n",
      "Selected optimization level O3:  Pure FP16 training.\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : False\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : False\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "loss: 8.989714622497559\n",
      "before weight: Parameter containing:\n",
      "tensor([[-9.0981e-04,  3.0441e-03, -3.2597e-03,  ...,  1.1253e-03,\n",
      "         -2.5101e-03, -1.2150e-03],\n",
      "        [-1.5602e-03, -6.0678e-05,  3.2139e-03,  ..., -1.6451e-03,\n",
      "         -3.7518e-03,  2.5578e-03],\n",
      "        [-1.6747e-03, -4.8459e-05, -2.9564e-03,  ..., -3.3092e-03,\n",
      "          4.4136e-03,  2.4395e-03],\n",
      "        ...,\n",
      "        [ 2.4204e-03,  1.4067e-03,  2.3651e-04,  ..., -3.6182e-03,\n",
      "         -1.0786e-03, -5.3358e-04],\n",
      "        [-4.2114e-03,  2.1019e-03,  4.3182e-03,  ..., -2.2354e-03,\n",
      "          4.4403e-03,  1.0519e-03],\n",
      "        [-2.9874e-04, -2.4815e-03, -4.1542e-03,  ..., -1.5650e-03,\n",
      "          4.3640e-03,  2.1992e-03]], device='cuda:0', dtype=torch.float16,\n",
      "       requires_grad=True)\n",
      "\n",
      "after weight: Parameter containing:\n",
      "tensor([[-9.0981e-04,  3.0441e-03, -3.2597e-03,  ...,  1.1253e-03,\n",
      "         -2.5101e-03, -1.2150e-03],\n",
      "        [-1.5602e-03, -6.0558e-05,  3.2139e-03,  ..., -1.6451e-03,\n",
      "         -3.7518e-03,  2.5578e-03],\n",
      "        [-1.6747e-03, -4.8459e-05, -2.9564e-03,  ..., -3.3092e-03,\n",
      "          4.4136e-03,  2.4395e-03],\n",
      "        ...,\n",
      "        [ 2.4204e-03,  1.4067e-03,  2.3663e-04,  ..., -3.6182e-03,\n",
      "         -1.0786e-03, -5.3358e-04],\n",
      "        [-4.2114e-03,  2.1019e-03,  4.3182e-03,  ..., -2.2354e-03,\n",
      "          4.4403e-03,  1.0519e-03],\n",
      "        [-2.9898e-04, -2.4815e-03, -4.1542e-03,  ..., -1.5650e-03,\n",
      "          4.3640e-03,  2.1992e-03]], device='cuda:0', dtype=torch.float16,\n",
      "       requires_grad=True)\n",
      "\n",
      "step memory allocate: 60825.516M\n",
      "loss: 3.2957849502563477\n",
      "before weight: Parameter containing:\n",
      "tensor([[-9.0981e-04,  3.0441e-03, -3.2597e-03,  ...,  1.1253e-03,\n",
      "         -2.5101e-03, -1.2150e-03],\n",
      "        [-1.5602e-03, -6.0558e-05,  3.2139e-03,  ..., -1.6451e-03,\n",
      "         -3.7518e-03,  2.5578e-03],\n",
      "        [-1.6747e-03, -4.8459e-05, -2.9564e-03,  ..., -3.3092e-03,\n",
      "          4.4136e-03,  2.4395e-03],\n",
      "        ...,\n",
      "        [ 2.4204e-03,  1.4067e-03,  2.3663e-04,  ..., -3.6182e-03,\n",
      "         -1.0786e-03, -5.3358e-04],\n",
      "        [-4.2114e-03,  2.1019e-03,  4.3182e-03,  ..., -2.2354e-03,\n",
      "          4.4403e-03,  1.0519e-03],\n",
      "        [-2.9898e-04, -2.4815e-03, -4.1542e-03,  ..., -1.5650e-03,\n",
      "          4.3640e-03,  2.1992e-03]], device='cuda:0', dtype=torch.float16,\n",
      "       requires_grad=True)\n",
      "\n",
      "after weight: Parameter containing:\n",
      "tensor([[-9.0981e-04,  3.0441e-03, -3.2597e-03,  ...,  1.1253e-03,\n",
      "         -2.5101e-03, -1.2150e-03],\n",
      "        [-1.5602e-03, -6.0380e-05,  3.2139e-03,  ..., -1.6451e-03,\n",
      "         -3.7518e-03,  2.5578e-03],\n",
      "        [-1.6747e-03, -4.8459e-05, -2.9564e-03,  ..., -3.3092e-03,\n",
      "          4.4136e-03,  2.4395e-03],\n",
      "        ...,\n",
      "        [ 2.4204e-03,  1.4067e-03,  2.3687e-04,  ..., -3.6182e-03,\n",
      "         -1.0786e-03, -5.3358e-04],\n",
      "        [-4.2114e-03,  2.1019e-03,  4.3182e-03,  ..., -2.2354e-03,\n",
      "          4.4403e-03,  1.0519e-03],\n",
      "        [-2.9922e-04, -2.4815e-03, -4.1542e-03,  ..., -1.5650e-03,\n",
      "          4.3640e-03,  2.1992e-03]], device='cuda:0', dtype=torch.float16,\n",
      "       requires_grad=True)\n",
      "\n",
      "step memory allocate: 60825.516M\n",
      "loss: 0.0034332275390625\n",
      "before weight: Parameter containing:\n",
      "tensor([[-9.0981e-04,  3.0441e-03, -3.2597e-03,  ...,  1.1253e-03,\n",
      "         -2.5101e-03, -1.2150e-03],\n",
      "        [-1.5602e-03, -6.0380e-05,  3.2139e-03,  ..., -1.6451e-03,\n",
      "         -3.7518e-03,  2.5578e-03],\n",
      "        [-1.6747e-03, -4.8459e-05, -2.9564e-03,  ..., -3.3092e-03,\n",
      "          4.4136e-03,  2.4395e-03],\n",
      "        ...,\n",
      "        [ 2.4204e-03,  1.4067e-03,  2.3687e-04,  ..., -3.6182e-03,\n",
      "         -1.0786e-03, -5.3358e-04],\n",
      "        [-4.2114e-03,  2.1019e-03,  4.3182e-03,  ..., -2.2354e-03,\n",
      "          4.4403e-03,  1.0519e-03],\n",
      "        [-2.9922e-04, -2.4815e-03, -4.1542e-03,  ..., -1.5650e-03,\n",
      "          4.3640e-03,  2.1992e-03]], device='cuda:0', dtype=torch.float16,\n",
      "       requires_grad=True)\n",
      "\n",
      "after weight: Parameter containing:\n",
      "tensor([[-9.0981e-04,  3.0441e-03, -3.2597e-03,  ...,  1.1253e-03,\n",
      "         -2.5101e-03, -1.2150e-03],\n",
      "        [-1.5602e-03, -6.0260e-05,  3.2139e-03,  ..., -1.6451e-03,\n",
      "         -3.7518e-03,  2.5578e-03],\n",
      "        [-1.6747e-03, -4.8459e-05, -2.9564e-03,  ..., -3.3092e-03,\n",
      "          4.4136e-03,  2.4395e-03],\n",
      "        ...,\n",
      "        [ 2.4204e-03,  1.4067e-03,  2.3711e-04,  ..., -3.6182e-03,\n",
      "         -1.0786e-03, -5.3358e-04],\n",
      "        [-4.2114e-03,  2.1019e-03,  4.3182e-03,  ..., -2.2354e-03,\n",
      "          4.4403e-03,  1.0519e-03],\n",
      "        [-2.9945e-04, -2.4815e-03, -4.1542e-03,  ..., -1.5650e-03,\n",
      "          4.3640e-03,  2.1992e-03]], device='cuda:0', dtype=torch.float16,\n",
      "       requires_grad=True)\n",
      "\n",
      "step memory allocate: 60825.516M\n",
      "loss: 2.9139556884765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/dp/lib/python3.10/site-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
      "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
      "/scratch/qualis/miniconda3/envs/dp/lib/python3.10/site-packages/apex/amp/_process_optimizer.py:344: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449201336/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  optimizer._amp_stash.dummy_overflow_buf = torch.cuda.IntTensor([0]);\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before weight: Parameter containing:\n",
      "tensor([[-9.0981e-04,  3.0441e-03, -3.2597e-03,  ...,  1.1253e-03,\n",
      "         -2.5101e-03, -1.2150e-03],\n",
      "        [-1.5602e-03, -6.0260e-05,  3.2139e-03,  ..., -1.6451e-03,\n",
      "         -3.7518e-03,  2.5578e-03],\n",
      "        [-1.6747e-03, -4.8459e-05, -2.9564e-03,  ..., -3.3092e-03,\n",
      "          4.4136e-03,  2.4395e-03],\n",
      "        ...,\n",
      "        [ 2.4204e-03,  1.4067e-03,  2.3711e-04,  ..., -3.6182e-03,\n",
      "         -1.0786e-03, -5.3358e-04],\n",
      "        [-4.2114e-03,  2.1019e-03,  4.3182e-03,  ..., -2.2354e-03,\n",
      "          4.4403e-03,  1.0519e-03],\n",
      "        [-2.9945e-04, -2.4815e-03, -4.1542e-03,  ..., -1.5650e-03,\n",
      "          4.3640e-03,  2.1992e-03]], device='cuda:0', dtype=torch.float16,\n",
      "       requires_grad=True)\n",
      "\n",
      "after weight: Parameter containing:\n",
      "tensor([[-9.0981e-04,  3.0441e-03, -3.2597e-03,  ...,  1.1253e-03,\n",
      "         -2.5101e-03, -1.2150e-03],\n",
      "        [-1.5602e-03, -6.0201e-05,  3.2139e-03,  ..., -1.6451e-03,\n",
      "         -3.7518e-03,  2.5578e-03],\n",
      "        [-1.6747e-03, -4.8459e-05, -2.9564e-03,  ..., -3.3092e-03,\n",
      "          4.4136e-03,  2.4395e-03],\n",
      "        ...,\n",
      "        [ 2.4204e-03,  1.4067e-03,  2.3723e-04,  ..., -3.6182e-03,\n",
      "         -1.0786e-03, -5.3358e-04],\n",
      "        [-4.2114e-03,  2.1019e-03,  4.3182e-03,  ..., -2.2354e-03,\n",
      "          4.4403e-03,  1.0519e-03],\n",
      "        [-2.9969e-04, -2.4815e-03, -4.1542e-03,  ..., -1.5650e-03,\n",
      "          4.3640e-03,  2.1992e-03]], device='cuda:0', dtype=torch.float16,\n",
      "       requires_grad=True)\n",
      "\n",
      "step memory allocate: 60825.516M\n",
      "loss: 5.8465728759765625\n",
      "before weight: Parameter containing:\n",
      "tensor([[-9.0981e-04,  3.0441e-03, -3.2597e-03,  ...,  1.1253e-03,\n",
      "         -2.5101e-03, -1.2150e-03],\n",
      "        [-1.5602e-03, -6.0201e-05,  3.2139e-03,  ..., -1.6451e-03,\n",
      "         -3.7518e-03,  2.5578e-03],\n",
      "        [-1.6747e-03, -4.8459e-05, -2.9564e-03,  ..., -3.3092e-03,\n",
      "          4.4136e-03,  2.4395e-03],\n",
      "        ...,\n",
      "        [ 2.4204e-03,  1.4067e-03,  2.3723e-04,  ..., -3.6182e-03,\n",
      "         -1.0786e-03, -5.3358e-04],\n",
      "        [-4.2114e-03,  2.1019e-03,  4.3182e-03,  ..., -2.2354e-03,\n",
      "          4.4403e-03,  1.0519e-03],\n",
      "        [-2.9969e-04, -2.4815e-03, -4.1542e-03,  ..., -1.5650e-03,\n",
      "          4.3640e-03,  2.1992e-03]], device='cuda:0', dtype=torch.float16,\n",
      "       requires_grad=True)\n",
      "\n",
      "after weight: Parameter containing:\n",
      "tensor([[-9.0981e-04,  3.0441e-03, -3.2597e-03,  ...,  1.1253e-03,\n",
      "         -2.5101e-03, -1.2150e-03],\n",
      "        [-1.5602e-03, -6.0260e-05,  3.2139e-03,  ..., -1.6451e-03,\n",
      "         -3.7518e-03,  2.5578e-03],\n",
      "        [-1.6747e-03, -4.8459e-05, -2.9564e-03,  ..., -3.3092e-03,\n",
      "          4.4136e-03,  2.4395e-03],\n",
      "        ...,\n",
      "        [ 2.4204e-03,  1.4067e-03,  2.3723e-04,  ..., -3.6182e-03,\n",
      "         -1.0786e-03, -5.3358e-04],\n",
      "        [-4.2114e-03,  2.1019e-03,  4.3182e-03,  ..., -2.2354e-03,\n",
      "          4.4403e-03,  1.0519e-03],\n",
      "        [-2.9969e-04, -2.4815e-03, -4.1542e-03,  ..., -1.5650e-03,\n",
      "          4.3640e-03,  2.1992e-03]], device='cuda:0', dtype=torch.float16,\n",
      "       requires_grad=True)\n",
      "\n",
      "step memory allocate: 60825.516M\n",
      "CPU times: user 11 s, sys: 688 ms, total: 11.7 s\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda import max_memory_allocated\n",
    "\n",
    "#from torch.cuda.amp import autocast \n",
    "\n",
    "from apex import amp\n",
    "\n",
    "# example input sizes\n",
    "batch_size, hidden_size = 4, 50000\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_size, 1, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = self.w1(x)\n",
    "        z2 = self.w2(z1)\n",
    "        return z2\n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "torch.cuda.init()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "fp32_model= Net().to(\"cuda\")\n",
    "#optimizer = SGD(fp32_model.parameters(), lr=1e-2)\n",
    "#optimizer = SGD(fp32_model.parameters(), lr=1e-4, momentum=0.9) #diverge\n",
    "optimizer = SGD(fp32_model.parameters(), lr=1e-5, momentum=0.9)\n",
    "#optimizer = Adam(fp32_model.parameters(), lr=1e-2)\n",
    "print(f\"GPU = {torch.cuda.max_memory_allocated(0) / 1e6} MiB\")\n",
    "\n",
    "# Allow Amp to perform casts as required by the opt_level\n",
    "fp32_model, optimizer = amp.initialize(fp32_model, optimizer, opt_level=\"O3\")\n",
    "\n",
    "# create dummy data (bsz=4, hid=256)\n",
    "#x = torch.randn(batch_size, hidden_size, device=\"cuda\") \n",
    "x = torch.ones(batch_size, hidden_size, device=\"cuda\") \n",
    "y = torch.tensor([[3.0], [3.0], [3.0], [3.0]], device=\"cuda\")\n",
    "\n",
    "torch.cuda.init()\n",
    "\n",
    "#scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for _ in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    z2 = fp32_model(x)\n",
    "    loss = torch.nn.functional.mse_loss(z2, y)\n",
    "        \n",
    "    print(f\"loss: {loss}\")\n",
    "    #scaler.scale(loss).backward()\n",
    "    # loss.backward() becomes:\n",
    "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "    print(f'before weight: {fp32_model.w1.weight}\\n')\n",
    "    #print(f'before grad: {fp32_model.w1.weight.grad}\\n')\n",
    "    #print(torch.max(fp32_model.w1.weight))\n",
    "    #scaler.step(optimizer)\n",
    "    #scaler.update()\n",
    "    optimizer.step()\n",
    "    print(f'after weight: {fp32_model.w1.weight}\\n')\n",
    "    #print(torch.max(fp32_model.w1.weight))\n",
    "    memory = max_memory_allocated()\n",
    "    print(f'step memory allocate: {memory / 1e6:.3f}M')\n",
    "    #torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dc1c09ae-2be6-4946-97db-a88f77370273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = 60825.516032 MiB\n",
      "GPU = 60825.516032 MiB\n",
      "loss: 10.28125\n",
      "before weight: Parameter containing:\n",
      "tensor([[ 0.0028,  0.0036, -0.0021,  ...,  0.0016,  0.0030, -0.0012],\n",
      "        [ 0.0024,  0.0002, -0.0032,  ...,  0.0003, -0.0013, -0.0005],\n",
      "        [ 0.0004, -0.0026,  0.0036,  ..., -0.0022,  0.0041, -0.0003],\n",
      "        ...,\n",
      "        [ 0.0012,  0.0032,  0.0012,  ..., -0.0024,  0.0034,  0.0028],\n",
      "        [ 0.0031, -0.0026, -0.0041,  ..., -0.0026, -0.0035, -0.0039],\n",
      "        [ 0.0035, -0.0010, -0.0007,  ...,  0.0025, -0.0026,  0.0012]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "\n",
      "after weight: Parameter containing:\n",
      "tensor([[ 0.0028,  0.0036, -0.0021,  ...,  0.0016,  0.0030, -0.0012],\n",
      "        [ 0.0024,  0.0002, -0.0032,  ...,  0.0003, -0.0013, -0.0005],\n",
      "        [ 0.0004, -0.0026,  0.0036,  ..., -0.0022,  0.0041, -0.0003],\n",
      "        ...,\n",
      "        [ 0.0012,  0.0032,  0.0012,  ..., -0.0024,  0.0034,  0.0028],\n",
      "        [ 0.0031, -0.0026, -0.0041,  ..., -0.0026, -0.0035, -0.0039],\n",
      "        [ 0.0035, -0.0010, -0.0007,  ...,  0.0025, -0.0026,  0.0012]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "\n",
      "step memory allocate: 65825.222M\n",
      "loss: 1.1396484375\n",
      "before weight: Parameter containing:\n",
      "tensor([[ 0.0028,  0.0036, -0.0021,  ...,  0.0016,  0.0030, -0.0012],\n",
      "        [ 0.0024,  0.0002, -0.0032,  ...,  0.0003, -0.0013, -0.0005],\n",
      "        [ 0.0004, -0.0026,  0.0036,  ..., -0.0022,  0.0041, -0.0003],\n",
      "        ...,\n",
      "        [ 0.0012,  0.0032,  0.0012,  ..., -0.0024,  0.0034,  0.0028],\n",
      "        [ 0.0031, -0.0026, -0.0041,  ..., -0.0026, -0.0035, -0.0039],\n",
      "        [ 0.0035, -0.0010, -0.0007,  ...,  0.0025, -0.0026,  0.0012]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "\n",
      "after weight: Parameter containing:\n",
      "tensor([[ 0.0028,  0.0036, -0.0021,  ...,  0.0016,  0.0030, -0.0012],\n",
      "        [ 0.0024,  0.0002, -0.0032,  ...,  0.0003, -0.0013, -0.0005],\n",
      "        [ 0.0004, -0.0026,  0.0036,  ..., -0.0022,  0.0041, -0.0003],\n",
      "        ...,\n",
      "        [ 0.0012,  0.0032,  0.0012,  ..., -0.0024,  0.0034,  0.0028],\n",
      "        [ 0.0031, -0.0026, -0.0041,  ..., -0.0026, -0.0035, -0.0039],\n",
      "        [ 0.0035, -0.0010, -0.0007,  ...,  0.0025, -0.0026,  0.0012]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "\n",
      "step memory allocate: 65825.222M\n",
      "loss: 13.7734375\n",
      "before weight: Parameter containing:\n",
      "tensor([[ 0.0028,  0.0036, -0.0021,  ...,  0.0016,  0.0030, -0.0012],\n",
      "        [ 0.0024,  0.0002, -0.0032,  ...,  0.0003, -0.0013, -0.0005],\n",
      "        [ 0.0004, -0.0026,  0.0036,  ..., -0.0022,  0.0041, -0.0003],\n",
      "        ...,\n",
      "        [ 0.0012,  0.0032,  0.0012,  ..., -0.0024,  0.0034,  0.0028],\n",
      "        [ 0.0031, -0.0026, -0.0041,  ..., -0.0026, -0.0035, -0.0039],\n",
      "        [ 0.0035, -0.0010, -0.0007,  ...,  0.0025, -0.0026,  0.0012]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "\n",
      "after weight: Parameter containing:\n",
      "tensor([[ 0.0028,  0.0036, -0.0021,  ...,  0.0016,  0.0030, -0.0012],\n",
      "        [ 0.0024,  0.0002, -0.0032,  ...,  0.0003, -0.0013, -0.0005],\n",
      "        [ 0.0004, -0.0026,  0.0036,  ..., -0.0022,  0.0041, -0.0003],\n",
      "        ...,\n",
      "        [ 0.0012,  0.0032,  0.0012,  ..., -0.0024,  0.0033,  0.0028],\n",
      "        [ 0.0031, -0.0026, -0.0041,  ..., -0.0026, -0.0035, -0.0039],\n",
      "        [ 0.0035, -0.0010, -0.0007,  ...,  0.0025, -0.0026,  0.0012]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "\n",
      "step memory allocate: 65825.222M\n",
      "loss: 70.25\n",
      "before weight: Parameter containing:\n",
      "tensor([[ 0.0028,  0.0036, -0.0021,  ...,  0.0016,  0.0030, -0.0012],\n",
      "        [ 0.0024,  0.0002, -0.0032,  ...,  0.0003, -0.0013, -0.0005],\n",
      "        [ 0.0004, -0.0026,  0.0036,  ..., -0.0022,  0.0041, -0.0003],\n",
      "        ...,\n",
      "        [ 0.0012,  0.0032,  0.0012,  ..., -0.0024,  0.0033,  0.0028],\n",
      "        [ 0.0031, -0.0026, -0.0041,  ..., -0.0026, -0.0035, -0.0039],\n",
      "        [ 0.0035, -0.0010, -0.0007,  ...,  0.0025, -0.0026,  0.0012]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "\n",
      "after weight: Parameter containing:\n",
      "tensor([[ 0.0028,  0.0036, -0.0021,  ...,  0.0016,  0.0030, -0.0012],\n",
      "        [ 0.0024,  0.0002, -0.0032,  ...,  0.0003, -0.0013, -0.0005],\n",
      "        [ 0.0004, -0.0026,  0.0036,  ..., -0.0022,  0.0041, -0.0003],\n",
      "        ...,\n",
      "        [ 0.0012,  0.0032,  0.0012,  ..., -0.0024,  0.0033,  0.0028],\n",
      "        [ 0.0031, -0.0026, -0.0041,  ..., -0.0026, -0.0035, -0.0039],\n",
      "        [ 0.0035, -0.0010, -0.0007,  ...,  0.0025, -0.0026,  0.0012]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "\n",
      "step memory allocate: 65825.222M\n",
      "loss: 53.8125\n",
      "before weight: Parameter containing:\n",
      "tensor([[ 0.0028,  0.0036, -0.0021,  ...,  0.0016,  0.0030, -0.0012],\n",
      "        [ 0.0024,  0.0002, -0.0032,  ...,  0.0003, -0.0013, -0.0005],\n",
      "        [ 0.0004, -0.0026,  0.0036,  ..., -0.0022,  0.0041, -0.0003],\n",
      "        ...,\n",
      "        [ 0.0012,  0.0032,  0.0012,  ..., -0.0024,  0.0033,  0.0028],\n",
      "        [ 0.0031, -0.0026, -0.0041,  ..., -0.0026, -0.0035, -0.0039],\n",
      "        [ 0.0035, -0.0010, -0.0007,  ...,  0.0025, -0.0026,  0.0012]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "\n",
      "after weight: Parameter containing:\n",
      "tensor([[ 0.0028,  0.0036, -0.0021,  ...,  0.0016,  0.0030, -0.0012],\n",
      "        [ 0.0024,  0.0002, -0.0032,  ...,  0.0003, -0.0013, -0.0005],\n",
      "        [ 0.0004, -0.0026,  0.0036,  ..., -0.0022,  0.0041, -0.0003],\n",
      "        ...,\n",
      "        [ 0.0012,  0.0032,  0.0012,  ..., -0.0024,  0.0034,  0.0028],\n",
      "        [ 0.0031, -0.0026, -0.0041,  ..., -0.0026, -0.0035, -0.0039],\n",
      "        [ 0.0035, -0.0010, -0.0007,  ...,  0.0025, -0.0026,  0.0012]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "\n",
      "step memory allocate: 65825.222M\n",
      "CPU times: user 24.2 s, sys: 2.57 s, total: 26.8 s\n",
      "Wall time: 23.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda import max_memory_allocated\n",
    "\n",
    "from torch.cuda.amp import autocast \n",
    "\n",
    "# example input sizes\n",
    "batch_size, hidden_size = 4, 50000\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_size, 1, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = self.w1(x)\n",
    "        z2 = self.w2(z1)\n",
    "        return z2\n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "torch.cuda.init()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "fp32_model= Net().to(\"cuda\")\n",
    "print(f\"GPU = {torch.cuda.max_memory_allocated(0) / 1e6} MiB\")\n",
    "\n",
    "#optimizer = SGD(fp32_model.parameters(), lr=1e-2)\n",
    "#optimizer = SGD(fp32_model.parameters(), lr=1e-4, momentum=0.9) #diverge\n",
    "optimizer = SGD(fp32_model.parameters(), lr=1e-5, momentum=0.9)\n",
    "#optimizer = Adam(fp32_model.parameters(), lr=1e-2)\n",
    "\n",
    "fp16_model = Net().half().to(\"cuda\")\n",
    "fp16_model.load_state_dict(fp32_model.state_dict())\n",
    "print(f\"GPU = {torch.cuda.max_memory_allocated(0) / 1e6} MiB\")\n",
    "\n",
    "# create dummy data (bsz=4, hid=256)\n",
    "#x = torch.randn(batch_size, hidden_size, device=\"cuda\") \n",
    "x = torch.ones(batch_size, hidden_size, dtype=torch.half, device=\"cuda\") \n",
    "y = torch.tensor([[3.0], [3.0], [3.0], [3.0]], dtype=torch.half, device=\"cuda\")\n",
    "\n",
    "torch.cuda.init()\n",
    "\n",
    "for _ in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    z2 = fp16_model(x)\n",
    "    loss = torch.nn.functional.mse_loss(z2, y)    \n",
    "    #loss *= 1024\n",
    "    print(f\"loss: {loss}\")\n",
    "    loss.backward()\n",
    "    # copy gradient to FP32 model\n",
    "    fp32_model.w1.weight.grad = fp16_model.w1.weight.grad.float()\n",
    "    fp32_model.w2.weight.grad = fp16_model.w2.weight.grad.float()\n",
    "    print(f'before weight: {fp32_model.w1.weight}\\n')\n",
    "    #print(f'before grad: {fp32_model.w1.weight.grad}\\n')\n",
    "    #print(torch.max(fp32_model.w1.weight))\n",
    "    optimizer.step()\n",
    "    print(f'after weight: {fp32_model.w1.weight}\\n')\n",
    "    #print(torch.max(fp32_model.w1.weight))\n",
    "    memory = max_memory_allocated()\n",
    "    print(f'step memory allocate: {memory / 1e6:.3f}M')\n",
    "    #torch.cuda.reset_max_memory_allocated()\n",
    "    fp16_model.load_state_dict(fp32_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a14843b7-ca00-468a-a79a-26963b77b85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/large-scale-lm-tutorials/src\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff658586-8437-4f57-96b0-8f3bbc53841e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"middle\">\n",
       "<video width=\"80%\" controls>\n",
       "      <source src=\"../images/zero_video.mp4\" type=\"video/mp4\">\n",
       "</video></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zero-3 Video \n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<div align=\"middle\">\n",
    "<video width=\"80%\" controls>\n",
    "      <source src=\"../images/zero_video.mp4\" type=\"video/mp4\">\n",
    "</video></div>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73dd419-3a94-49cd-a587-38e5235a2668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp",
   "language": "python",
   "name": "dp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
